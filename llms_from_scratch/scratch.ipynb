{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": 1,
         "metadata": {},
         "outputs": [],
         "source": [
            "import tiktoken\n",
            "import re"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 2,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "20479\n"
               ]
            }
         ],
         "source": [
            "with open(\"the-verdict.txt\", \"r\") as f:\n",
            "    lines = f.read()\n",
            "\n",
            "print(len(lines))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 3,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "['I', ' ', 'HAD', ' ', 'always', ' ', 'thought', ' ', 'Jack', ' ']\n",
                  "4690\n"
               ]
            }
         ],
         "source": [
            "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', lines)\n",
            "print(preprocessed[:10])\n",
            "preprocessed = [item for item in preprocessed if item.strip()]\n",
            "print(len(preprocessed))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 4,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "['would', 'wouldn', 'year', 'years', 'yellow', 'yet', 'you', 'younger', 'your', 'yourself']\n",
                  "1130\n",
                  "! 0\n",
                  "\" 1\n",
                  "' 2\n",
                  "( 3\n",
                  ") 4\n",
                  ", 5\n",
                  "-- 6\n",
                  ". 7\n",
                  ": 8\n",
                  "; 9\n",
                  "? 10\n"
               ]
            }
         ],
         "source": [
            "all_words = sorted(set(preprocessed))\n",
            "print(all_words[-10:])\n",
            "vocab_size = len(all_words)\n",
            "print(vocab_size)\n",
            "vocab = {token: idx for idx, token in enumerate(all_words)}\n",
            "for i, word in enumerate(vocab):\n",
            "    if i > 10:\n",
            "        break\n",
            "    print(word, vocab[word])"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 5,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "[989, 1077, 7, 7]\n",
                  "their was..\n"
               ]
            }
         ],
         "source": [
            "class SimpleTokenizerV1:\n",
            "    def __init__(self, vocab):\n",
            "        self.token_to_int = vocab\n",
            "        self.int_to_token = {idx: token for token, idx in vocab.items()}\n",
            "    \n",
            "    def encode(self, text):\n",
            "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
            "        preprocessed = [item for item in preprocessed if item.strip()]\n",
            "        return [self.token_to_int[token] for token in preprocessed]\n",
            "    \n",
            "    def decode(self, integers):\n",
            "        text = \" \".join([self.int_to_token[idx] for idx in integers])\n",
            "        # replaces spaces before punctuation marks for format sentences correctly\n",
            "        text = re.sub(r'\\s+([,.:;?_!\"()\\'])', r'\\1', text)\n",
            "        return text\n",
            "    \n",
            "tokenizer = SimpleTokenizerV1(vocab)\n",
            "test = \"their  was..\"\n",
            "print(tokenizer.encode(test))\n",
            "print(tokenizer.decode(tokenizer.encode(test)))\n",
            "\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 6,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "('younger', 1127)\n",
                  "('your', 1128)\n",
                  "('yourself', 1129)\n",
                  "('<|unk|>', 1130)\n",
                  "('<|endoftext|>', 1131)\n",
                  "Hi, do you like tea? <|endoftext|> In the sunlit terraces\n"
               ]
            }
         ],
         "source": [
            "all_words.extend([\"<|unk|>\", \"<|endoftext|>\"])\n",
            "all_words\n",
            "vocab = {token: idx for idx, token in enumerate(all_words)}\n",
            "max = 10\n",
            "for i, item in enumerate(list(vocab.items())[-5:]):\n",
            "    if i > max:\n",
            "        break\n",
            "    print(item)\n",
            "text1 = \"Hi, do you like tea?\"\n",
            "text2 = \"In the sunlit terraces\"\n",
            "text = \" <|endoftext|> \".join((text1, text2))\n",
            "\n",
            "print(text)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 7,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "[1130, 5, 355, 1126, 628, 975, 10, 1131, 55, 988, 956, 984]\n",
                  "<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces\n",
                  "<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces\n"
               ]
            }
         ],
         "source": [
            "class SimpleTokenizerV2:\n",
            "    def __init__(self, vocab):\n",
            "        self.token_to_int = vocab\n",
            "        self.int_to_token = {idx: token for  token, idx in vocab.items()}\n",
            "    \n",
            "    def encode(self, text):\n",
            "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
            "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
            "        preprocessed = [item if item in self.token_to_int else \"<|unk|>\" for item in preprocessed]\n",
            "        return [self.token_to_int[token] for token in preprocessed]\n",
            "\n",
            "    def decode(self, integers):\n",
            "        text = \" \".join([self.int_to_token[idx] for idx in integers])\n",
            "        # replaces spaces before punctuation marks for format sentences correctly\n",
            "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
            "        return text\n",
            "\n",
            "tokenizer = SimpleTokenizerV2(vocab)\n",
            "encoded = tokenizer.encode(text)\n",
            "# tokenizer.int_to_token.keys()\n",
            "print(encoded)\n",
            "decoded = tokenizer.decode(encoded)\n",
            "print(decoded)\n",
            "print(tokenizer.decode(tokenizer.encode(text)))\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 8,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n",
                  "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
               ]
            }
         ],
         "source": [
            "bpe_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
            "text = (\n",
            "\"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
            "\"of someunknownPlace.\"\n",
            ")\n",
            "encoded = bpe_tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
            "print(encoded)\n",
            "strings = bpe_tokenizer.decode(encoded)\n",
            "print(strings)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 9,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "[33901, 86, 343, 86, 220, 959]\n",
                  "Akwirw ier\n"
               ]
            }
         ],
         "source": [
            "new_word = \"Akwirw ier\"\n",
            "\n",
            "encoded = bpe_tokenizer.encode(new_word)\n",
            "print(encoded)\n",
            "decoded = bpe_tokenizer.decode(encoded)\n",
            "print(decoded)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 10,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "['I', ' ', 'HAD', ' ', 'always', ' ', 'thought', ' ', 'Jack', ' ']\n",
                  "20479\n",
                  "['year', 'years', 'yellow', 'yet', 'you', 'younger', 'your', 'yourself', '<|unk|>', '<|endoftext|>']\n",
                  "[40, 367, 2885, 1464, 1807, 3619, 402, 271, 10899, 2138]\n"
               ]
            }
         ],
         "source": [
            "with open(\"the-verdict.txt\", \"r\") as f:\n",
            "    lines = f.read()\n",
            "\n",
            "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', lines)\n",
            "print(preprocessed[:10])\n",
            "preprocessed = [item for item in preprocessed if item.strip()]\n",
            "print(len(lines))\n",
            "all_words = sorted(set(preprocessed))\n",
            "all_words.extend([\"<|unk|>\", \"<|endoftext|>\"])\n",
            "print(all_words[-10:])\n",
            "vocab_size = len(all_words)\n",
            "enc_text = bpe_tokenizer.encode(lines)\n",
            "vocab_size = len(enc_text)\n",
            "print(enc_text[:10])\n",
            "\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 11,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "[290, 4920]\n",
                  "[4920, 2241]\n",
                  " and established -->  established himself\n"
               ]
            }
         ],
         "source": [
            "block_size = 2\n",
            "enc_sample = enc_text[50:]\n",
            "for i in range(block_size):\n",
            "    x = enc_sample[i:block_size]\n",
            "    y = enc_sample[i+1:i+block_size+1]\n",
            "    print(x)\n",
            "    print(y)\n",
            "    print(bpe_tokenizer.decode(x), \"-->\", bpe_tokenizer.decode(y))\n",
            "    break\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 12,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "(tensor([  44,  149, 1003,   57]), tensor([ 149, 1003,   57,   38]))"
                  ]
               },
               "execution_count": 12,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "import torch\n",
            "from torch.utils.data import DataLoader, Dataset\n",
            "\n",
            "class GPTDatasetV1(Dataset):\n",
            "    def __init__(self, text, tokenizer, max_length, stride):\n",
            "        self.input_ids = []\n",
            "        self.target_ids = []\n",
            "        self.encoded = tokenizer.encode(text)\n",
            "        #max_length = block_size or context_length, so need to substract max_length from range as that will be size of the sliced array \n",
            "        for i in range(0, len(self.encoded) - max_length, stride):\n",
            "            input_chunk = self.encoded[i:i+max_length]\n",
            "            targets = self.encoded[i+1:i+max_length+1]\n",
            "            self.input_ids.append(torch.tensor(input_chunk))\n",
            "            self.target_ids.append(torch.tensor(targets))\n",
            "    \n",
            "    def __len__(self):\n",
            "        return len(self.input_ids)\n",
            "    \n",
            "    def __getitem__(self, idx):\n",
            "        return (self.input_ids[idx], self.target_ids[idx])\n",
            "\n",
            "\n",
            "dataset = GPTDatasetV1(lines, tokenizer, 4, 1)\n",
            "dataset[1]\n",
            "\n",
            "\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 19,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "torch.Size([4, 4])"
                  ]
               },
               "execution_count": 19,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "\n",
            "def create_dataloader_v1(text, batch_size, max_length, stride, shuffle=True, drop_last=True, num_workers=0):\n",
            "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
            "    dataset = GPTDatasetV1(text, tokenizer=tokenizer, max_length=max_length, stride=stride)\n",
            "    dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers, drop_last=drop_last, shuffle=shuffle)\n",
            "    return dataloader\n",
            "\n",
            "dataloader = create_dataloader_v1(lines, batch_size=4, max_length=4, stride=1, shuffle=False)\n",
            "\n",
            "first_batch = next(iter(dataloader))\n",
            "\n",
            "first_batch[0].shape"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 18,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "[tensor([[   40,   367,  2885,  1464],\n",
                     "         [ 1464,  1807,  3619,   402],\n",
                     "         [  402,   271, 10899,  2138],\n",
                     "         [ 2138,   257,  7026, 15632]]),\n",
                     " tensor([[  367,  2885,  1464,  1807],\n",
                     "         [ 1807,  3619,   402,   271],\n",
                     "         [  271, 10899,  2138,   257],\n",
                     "         [  257,  7026, 15632,   438]])]"
                  ]
               },
               "execution_count": 18,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "\n",
            "dataloader = create_dataloader_v1(lines, batch_size=4, max_length=4, stride=3, shuffle=False)\n",
            "\n",
            "first_batch = next(iter(dataloader))\n",
            "\n",
            "first_batch"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Chapter 3\n",
            "Prior to transformers, RNNs were a popular architecture for NLP tasks such as machine translation. These networks consist of an encoder and decoder network. The encoder processes a sequence of tokens, where the first token is passed to the first hidden layer, the second is passed to the second layer state AS WELL AS the hidden state of the first token. The last hidden state of the encoder is passed to the decoder network, therefore it relies solely on the encapsulated context from the last hidden state of the encoder. It does not have direct access to previous tokens/hidden states (e.g words in a sentence), but only the last generated hidden state from the encoder. Although the the task of the encoder is to generate a sufficiently rich representation of the entire embedded sentence, this is remains limited to short sequence lengths and highlights the main limitation of the classical RNN architecture.\n",
            "\n",
            "#### Bahdanau attention (2014)\n",
            "\n",
            "Modified the encoder-decoder RNN so the decoder can selectively access different parts of the input sequence at each decoding step. "
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Scaled dot product attention\n",
            "\n",
            "The following implements self-attention where the dot products are scaled by the dimension of the compute key matrix encoding representations of the tokens after apply the key weights matrix. The reason for normalised and scaling these values is to improve training performance. For large architectures that use a large embedding dimension (e.g 1000 for GPT models), large dot products can result in very small gradients during backpropagation due to the softmax function. If one value dominates output from the dot product, the softmax normalisation will accordingly scale all the remaining values to very small values. Scaling by the sqrt of the embedding dim prevents these very large values from dominating the normalisation.\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 51,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "tensor([[ 1.0869,  0.9647,  0.9141],\n",
                  "        [-0.7345, -0.9722,  2.0583],\n",
                  "        [ 0.0583,  0.7741,  0.2938],\n",
                  "        [ 0.2305, -0.5297,  0.2802]], grad_fn=<SelectBackward0>)\n",
                  "input shape torch.Size([4, 4, 3])\n",
                  "torch.Size([2])\n",
                  "torch.Size([4, 2])\n",
                  "tensor(1.0278, grad_fn=<AddBackward0>)\n",
                  "tensor(0.5412, grad_fn=<AddBackward0>)\n",
                  "tensor(0.5412, grad_fn=<DotBackward0>)\n",
                  "attention_scores_2 shape: torch.Size([4])\n",
                  "values shape: torch.Size([4, 2])\n"
               ]
            },
            {
               "data": {
                  "text/plain": [
                     "tensor([-0.6578,  0.2621], grad_fn=<SqueezeBackward4>)"
                  ]
               },
               "execution_count": 51,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "\n",
            "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
            "vocab_size = tokenizer.n_vocab\n",
            "output_dim = 3  # for illustration\n",
            "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
            "input_ids, target_ids = first_batch\n",
            "inputs = token_embedding_layer(input_ids)\n",
            "\n",
            "print(inputs[1])\n",
            "print(\"input shape\", inputs.shape)\n",
            "\n",
            "x2 = inputs[1][1]\n",
            "d_in = x2.shape[0]\n",
            "d_out = 2\n",
            "\n",
            "torch.manual_seed(0)\n",
            "W_query = torch.nn.Parameter(torch.randn(d_in, d_out))\n",
            "K_query = torch.nn.Parameter(torch.randn(d_in, d_out))\n",
            "V_query = torch.nn.Parameter(torch.randn(d_in, d_out))\n",
            "\n",
            "\n",
            "x2_query = x2 @ W_query # query vector for 2nd token in the first example of the batch\n",
            "keys = inputs[1] @ K_query # key matrix for all tokens in the first example of the batch\n",
            "values = inputs[1] @ V_query # value matrix for all tokens in the first example of the batch\n",
            "\n",
            "print(x2_query.shape) # query vector\n",
            "print(keys.shape) # \n",
            "\n",
            "# attention score\n",
            "tmp_score = 0\n",
            "x2_key = keys[1]\n",
            "for i in range(2):\n",
            "    tmp_score += x2_query[i] * x2_key[i]\n",
            "    print(tmp_score)\n",
            "attention_score_22 = x2_query.dot(keys[1])\n",
            "print(attention_score_22)\n",
            "\n",
            "attention_scores_2 = x2_query @ keys.T\n",
            "# softmax normalisation\n",
            "d_k = keys.shape[1] # embedding dimension of the keys\n",
            "# SCALED DOT PRODUCT ATTENTION\n",
            "attention_weights_2 = torch.nn.functional.softmax(attention_scores_2 / (d_k ** 0.5), dim=-1)\n",
            "attention_weights_2\n",
            "\n",
            "# context vector calculation\n",
            "print(f\"attention_scores_2 shape: {attention_scores_2.shape}\")\n",
            "print(f\"values shape: {values.shape}\")\n",
            "context_vector_2 = attention_weights_2 @ values\n",
            "context_vector_2 # represents relative importance of all the input tokens wrto the 2nd token in the first example of the batch\n",
            "\n",
            "context_vector_2\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "class SelfAttention_v1(torch.nn.Module):\n",
            "    def __init__(self, d_in, d_out):\n",
            "        super().__init__()\n",
            "        self.Q = torch.nn.Parameter(torch.randn(d_in, d_out))\n",
            "        self.K = torch.nn.Parameter(torch.randn(d_in, d_out))\n",
            "        self.V = torch.nn.Parameter(torch.randn(d_in, d_out))\n",
            "    \n",
            "    def forward(self, x):\n",
            "        query = x @ self.Q\n",
            "        keys = x @ self.K\n",
            "        values = x @ self.V\n",
            "        attention_scores = query @ keys.T\n",
            "        d_k = keys.shape[1]\n",
            "        # rescale the attention scores because dot products can be large when embedding dimension is large\n",
            "        # i.e the sum is taken over the embedding dimension, so many values result in a large dot product\n",
            "        # results in very small gradients\n",
            "        attention_weights = torch.nn.functional.softmax(attention_scores / (d_k ** 0.5), dim=-1)\n",
            "        context_vector = attention_weights @ values\n",
            "        return context_vector\n",
            "\n",
            "class SelfAttention_v2(torch.nn.Module):\n",
            "    def __init__(self, d_in, d_out, qkv_bias: bool = False):\n",
            "        super().__init__()\n",
            "        # proper initialization of weights in torch nn.Linear modules\n",
            "        self.Q = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
            "        self.K = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
            "        self.V = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
            "    \n",
            "    def forward(self, x):\n",
            "        query = self.Q(x)\n",
            "        keys = self.K(x)\n",
            "        values = self.V(x)\n",
            "        attention_scores = query @ keys.T\n",
            "        d_k = keys.shape[1]\n",
            "        attention_weights = torch.nn.functional.softmax(attention_scores / (d_k ** 0.5), dim=-1)\n",
            "        context_vector = attention_weights @ values\n",
            "        return context_vector\n",
            "\n",
            "    \n",
            "\n",
            "\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 40,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "torch.Size([4, 4, 3])"
                  ]
               },
               "execution_count": 40,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "inputs.shape"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 45,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "d_in 3, d_out 2\n",
                  "torch.Size([4, 3])\n",
                  "sa_v1\n",
                  "tensor([-1.7741,  0.1581], grad_fn=<SelectBackward0>)\n",
                  "sa_v2\n",
                  "tensor([0.1687, 0.1249], grad_fn=<SelectBackward0>)\n",
                  "torch.Size([3, 2])\n",
                  "torch.Size([2, 3])\n",
                  "updated sa_v1\n",
                  "tensor([0.1687, 0.1249], grad_fn=<SelectBackward0>)\n"
               ]
            }
         ],
         "source": [
            "# compare SelfAttentionV1 and V2\n",
            "d_in = inputs.shape[-1] # embedding dim\n",
            "d_out = 2\n",
            "print(f\"d_in {d_in}, d_out {d_out}\")\n",
            "sa_v1 = SelfAttention_v1(d_in=d_in, d_out=d_out)\n",
            "sa_v2 = SelfAttention_v2(d_in=d_in, d_out=d_out)\n",
            "\n",
            "x2 = inputs[1]\n",
            "print(x2.shape)\n",
            "sa_v1_mat = sa_v1(inputs[1])\n",
            "sa_v2_mat = sa_v2(inputs[1])\n",
            "print(\"sa_v1\")\n",
            "print(sa_v1_mat[0])\n",
            "print(\"sa_v2\")\n",
            "print(sa_v2_mat[0])\n",
            "\n",
            "print(sa_v1.Q.shape)\n",
            "print(sa_v2.Q.weight.shape)\n",
            "\n",
            "sa_v1.Q = torch.nn.Parameter(sa_v2.Q.weight.T)\n",
            "sa_v1.V = torch.nn.Parameter(sa_v2.V.weight.T)\n",
            "sa_v1.K = torch.nn.Parameter(sa_v2.K.weight.T)\n",
            "print(\"updated sa_v1\")\n",
            "sa_v1_mat_up = sa_v1(inputs[1])\n",
            "print(sa_v1_mat_up[0])\n",
            "\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 55,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "torch.Size([4, 4, 2]) torch.Size([4, 4, 2])\n",
                  "context length 4\n",
                  "tensor([[0., 1., 1., 1.],\n",
                  "        [0., 0., 1., 1.],\n",
                  "        [0., 0., 0., 1.],\n",
                  "        [0., 0., 0., 0.]])\n",
                  "tensor([[False,  True,  True,  True],\n",
                  "        [False, False,  True,  True],\n",
                  "        [False, False, False,  True],\n",
                  "        [False, False, False, False]])\n",
                  "tensor([[[-0.0322,    -inf,    -inf,    -inf],\n",
                  "         [ 0.2211,  1.9618,    -inf,    -inf],\n",
                  "         [ 0.2161, -0.2522, -2.0162,    -inf],\n",
                  "         [ 0.0581,  0.6341,  0.4317,  0.0918]],\n",
                  "\n",
                  "        [[ 1.9618,    -inf,    -inf,    -inf],\n",
                  "         [-0.2522, -2.0162,    -inf,    -inf],\n",
                  "         [ 0.6341,  0.4317,  0.0918,    -inf],\n",
                  "         [ 0.0359, -0.3077,  0.1054, -0.1262]],\n",
                  "\n",
                  "        [[-2.0162,    -inf,    -inf,    -inf],\n",
                  "         [ 0.4317,  0.0918,    -inf,    -inf],\n",
                  "         [-0.3077,  0.1054, -0.1262,    -inf],\n",
                  "         [-1.1827, -0.3320, -0.5014,  2.4920]],\n",
                  "\n",
                  "        [[ 0.0918,    -inf,    -inf,    -inf],\n",
                  "         [ 0.1054, -0.1262,    -inf,    -inf],\n",
                  "         [-0.3320, -0.5014,  2.4920,    -inf],\n",
                  "         [-0.1557,  0.1262, -0.0338,  0.2315]]], grad_fn=<MaskedFillBackward0>)\n"
               ]
            },
            {
               "data": {
                  "text/plain": [
                     "tensor([[[1.0000, 0.0000, 0.0000, 0.0000],\n",
                     "         [0.2260, 0.7740, 0.0000, 0.0000],\n",
                     "         [0.5197, 0.3732, 0.1072, 0.0000],\n",
                     "         [0.2071, 0.3112, 0.2697, 0.2121]],\n",
                     "\n",
                     "        [[1.0000, 0.0000, 0.0000, 0.0000],\n",
                     "         [0.7768, 0.2232, 0.0000, 0.0000],\n",
                     "         [0.3924, 0.3401, 0.2674, 0.0000],\n",
                     "         [0.2684, 0.2105, 0.2819, 0.2393]],\n",
                     "\n",
                     "        [[1.0000, 0.0000, 0.0000, 0.0000],\n",
                     "         [0.5598, 0.4402, 0.0000, 0.0000],\n",
                     "         [0.2877, 0.3853, 0.3271, 0.0000],\n",
                     "         [0.0559, 0.1020, 0.0905, 0.7515]],\n",
                     "\n",
                     "        [[1.0000, 0.0000, 0.0000, 0.0000],\n",
                     "         [0.5409, 0.4591, 0.0000, 0.0000],\n",
                     "         [0.1081, 0.0959, 0.7961, 0.0000],\n",
                     "         [0.2162, 0.2639, 0.2356, 0.2843]]], grad_fn=<SoftmaxBackward0>)"
                  ]
               },
               "execution_count": 55,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "# masked self-attention (causal self-attention)\n",
            "# ensures that no current position is influenced by future positions (e.g for generative tasks)\n",
            "queries = sa_v2.Q(inputs)\n",
            "keys = sa_v2.K(inputs)\n",
            "print(queries.shape, keys.shape)\n",
            "attention_scores = queries @ keys.transpose(1,2)\n",
            "# normalisation of scores to they form a probability distribution summing to 1 - this is applied along the embedding dim \n",
            "attention_weights = torch.nn.functional.softmax(attention_scores / keys.shape[-1] ** 0.5, dim=-1)\n",
            "\n",
            "context_length = inputs.shape[1]\n",
            "print(f\"context length {context_length}\")\n",
            "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
            "print(mask)\n",
            "print(mask.bool())\n",
            "# negative infinity approaches 0 in softmax function (because e-inf approaches 0)\n",
            "attention_scores_masked = attention_scores.masked_fill_(mask.bool(), -torch.inf)\n",
            "print(attention_scores_masked)\n",
            "# normalised weights\n",
            "attention_weights = torch.nn.functional.softmax((attention_scores_masked) / keys.shape[-1] ** 0.5, dim=-1)\n",
            "attention_weights"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 59,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "tensor([[2., 2., 0., 2.],\n",
                  "        [2., 0., 0., 0.],\n",
                  "        [0., 2., 0., 2.],\n",
                  "        [2., 2., 2., 2.]])\n"
               ]
            }
         ],
         "source": [
            "# Dropout\n",
            "# only applied during training, not inference\n",
            "torch.manual_seed(123)\n",
            "dropout = torch.nn.Dropout(0.5)\n",
            "example = torch.ones((context_length, context_length))\n",
            "# to compensate for the reduction in active units, the un-zero'd values are scaled by (original_val / dropout_prob (0.5))\n",
            "# 1 / 0.5 = 2\n",
            "# this ensures the overall influence of the weight is consistent at both training and inference time\n",
            "print(dropout(example))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 67,
         "metadata": {},
         "outputs": [],
         "source": [
            "class CausalAttention(torch.nn.Module):\n",
            "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias: bool= False):\n",
            "        super().__init__()\n",
            "        self.d_out = d_out\n",
            "        self.dropout = torch.nn.Dropout(dropout)\n",
            "        self.Q = torch.nn.Linear(in_features=d_in, out_features=d_out)\n",
            "        self.K = torch.nn.Linear(in_features=d_in, out_features=d_out)\n",
            "        self.V = torch.nn.Linear(in_features=d_in, out_features=d_out)\n",
            "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
            "    \n",
            "    def forward(self, x):\n",
            "        batch, num_token, d_in = x.shape\n",
            "        keys = self.K(x)\n",
            "        queries = self.Q(x)\n",
            "        values = self.V(x)\n",
            "        attn_scores = queries @ keys.transpose(1, 2)\n",
            "        attn_scores.masked_fill_(self.mask.bool()[:num_token, :num_token], -torch.inf)\n",
            "        attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim=-1)\n",
            "        attn_weights = self.dropout(attn_weights)\n",
            "        context_vec = attn_weights @ values\n",
            "        return context_vec\n",
            "        \n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 68,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "torch.Size([4, 4, 2])"
                  ]
               },
               "execution_count": 68,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "context_length = inputs.shape[1]\n",
            "ca=CausalAttention(d_in, d_out, context_length, 0.0)\n",
            "context_vecs = ca(inputs)\n",
            "context_vecs.shape\n",
            "\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# multihead attention\n",
            "# apply multiple query key and value matrices (one for each head) in parallel\n",
            "\n",
            "class MultiHeadAttention(torch.nn.Module):\n",
            "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
            "        super().__init__()\n",
            "        assert (d_out % num_heads == 0), f\"d_out {d_out} must be divisible by num heads {num_heads}\"\n",
            "        self.d_out = d_out\n",
            "        self.num_heads = num_heads\n",
            "        self.head_dim = d_out // num_heads\n",
            "        self.Q = torch.nn.Linear(in_features=d_in, out_features=d_out)\n",
            "        self.K = torch.nn.Linear(in_features=d_in, out_features=d_out)\n",
            "        self.V = torch.nn.Linear(in_features=d_in, out_features=d_out)\n",
            "        self.out_proj = torch.nn.Linear(in_features=d_out, out_features=d_out)\n",
            "        self.dropout = torch.nn.Dropout(dropout)\n",
            "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
            "    \n",
            "    def forward(self, x):\n",
            "        batch, num_token, d_in = x.shape\n",
            "        keys = self.K(x)\n",
            "        queries = self.Q(x)\n",
            "        values = self.V(x)\n",
            "        keys = keys.view(batch, num_token, self.num_heads, self.head_dim)\n",
            "        queries = queries.view(batch, num_token, self.num_heads, self.head_dim)\n",
            "        values = values.view(batch, num_token, self.num_heads, self.head_dim)\n",
            "        keys = keys.transpose(1,2)\n",
            "        queries = queries.transpose(1,2)\n",
            "        values = values.transpose(1,2)\n",
            "        attn_scores = queries @ keys.transpose(2, 3)\n",
            "        mask_bool = self.mask.bool()[:num_token, :num_token]\n",
            "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
            "        attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim=-1)\n",
            "        attn_weights = self.dropout(attn_weights)\n",
            "        context_vec = attn_weights @ values\n",
            "        # contiguous makes a copy of the tensory with the final memory layout specified by the tensor shape\n",
            "        context_vec = context_vec.contiguous().view(batch, num_token, self.d_out)\n",
            "        # project layer\n",
            "        context_vec = self.out_proj(context_vec)\n",
            "        return context_vec\n",
            "        \n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 77,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "torch.Size([4, 4, 2])"
                  ]
               },
               "execution_count": 77,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "torch.manual_seed(123)\n",
            "\n",
            "batch_size, context_length, d_in = inputs.shape\n",
            "d_out = 2\n",
            "mha = MultiHeadAttention(d_in, d_out, context_length, 0.5, num_heads=2)\n",
            "out = mha(inputs)\n",
            "out.shape"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 80,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "torch.Size([2, 1024])\n"
               ]
            },
            {
               "data": {
                  "text/plain": [
                     "torch.Size([2, 1024, 768])"
                  ]
               },
               "execution_count": 80,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "# gpt-2 sized MHA\n",
            "\n",
            "embedding_dim = 768\n",
            "num_heads = 12\n",
            "context_length = 1024\n",
            "\n",
            "dataloader = create_dataloader_v1(lines, batch_size=2, max_length=context_length, stride=1, shuffle=False)\n",
            "first_batch = next(iter(dataloader))\n",
            "print(first_batch[0].shape)\n",
            "\n",
            "vocab_size = tokenizer.n_vocab\n",
            "token_embedding_layer = torch.nn.Embedding(vocab_size, embedding_dim)\n",
            "input_ids, target_ids = first_batch\n",
            "inputs = token_embedding_layer(input_ids)\n",
            "\n",
            "inputs[0].shape\n",
            "\n",
            "mha = MultiHeadAttention(d_in=768, d_out=768, context_length=context_length, dropout=0.2, num_heads=12)\n",
            "\n",
            "out = mha(inputs)\n",
            "\n",
            "out.shape"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "stanford",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.10.16"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
