{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": 1,
         "metadata": {},
         "outputs": [],
         "source": [
            "import tiktoken\n",
            "import re"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 2,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "20479\n"
               ]
            }
         ],
         "source": [
            "with open(\"the-verdict.txt\", \"r\") as f:\n",
            "    lines = f.read()\n",
            "\n",
            "print(len(lines))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 3,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "['I', ' ', 'HAD', ' ', 'always', ' ', 'thought', ' ', 'Jack', ' ']\n",
                  "4690\n"
               ]
            }
         ],
         "source": [
            "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', lines)\n",
            "print(preprocessed[:10])\n",
            "preprocessed = [item for item in preprocessed if item.strip()]\n",
            "print(len(preprocessed))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 4,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "['would', 'wouldn', 'year', 'years', 'yellow', 'yet', 'you', 'younger', 'your', 'yourself']\n",
                  "1130\n",
                  "! 0\n",
                  "\" 1\n",
                  "' 2\n",
                  "( 3\n",
                  ") 4\n",
                  ", 5\n",
                  "-- 6\n",
                  ". 7\n",
                  ": 8\n",
                  "; 9\n",
                  "? 10\n"
               ]
            }
         ],
         "source": [
            "all_words = sorted(set(preprocessed))\n",
            "print(all_words[-10:])\n",
            "vocab_size = len(all_words)\n",
            "print(vocab_size)\n",
            "vocab = {token: idx for idx, token in enumerate(all_words)}\n",
            "for i, word in enumerate(vocab):\n",
            "    if i > 10:\n",
            "        break\n",
            "    print(word, vocab[word])"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 5,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "[989, 1077, 7, 7]\n",
                  "their was..\n"
               ]
            }
         ],
         "source": [
            "class SimpleTokenizerV1:\n",
            "    def __init__(self, vocab):\n",
            "        self.token_to_int = vocab\n",
            "        self.int_to_token = {idx: token for token, idx in vocab.items()}\n",
            "    \n",
            "    def encode(self, text):\n",
            "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
            "        preprocessed = [item for item in preprocessed if item.strip()]\n",
            "        return [self.token_to_int[token] for token in preprocessed]\n",
            "    \n",
            "    def decode(self, integers):\n",
            "        text = \" \".join([self.int_to_token[idx] for idx in integers])\n",
            "        # replaces spaces before punctuation marks for format sentences correctly\n",
            "        text = re.sub(r'\\s+([,.:;?_!\"()\\'])', r'\\1', text)\n",
            "        return text\n",
            "    \n",
            "tokenizer = SimpleTokenizerV1(vocab)\n",
            "test = \"their  was..\"\n",
            "print(tokenizer.encode(test))\n",
            "print(tokenizer.decode(tokenizer.encode(test)))\n",
            "\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 6,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "('younger', 1127)\n",
                  "('your', 1128)\n",
                  "('yourself', 1129)\n",
                  "('<|unk|>', 1130)\n",
                  "('<|endoftext|>', 1131)\n",
                  "Hi, do you like tea? <|endoftext|> In the sunlit terraces\n"
               ]
            }
         ],
         "source": [
            "all_words.extend([\"<|unk|>\", \"<|endoftext|>\"])\n",
            "all_words\n",
            "vocab = {token: idx for idx, token in enumerate(all_words)}\n",
            "max = 10\n",
            "for i, item in enumerate(list(vocab.items())[-5:]):\n",
            "    if i > max:\n",
            "        break\n",
            "    print(item)\n",
            "text1 = \"Hi, do you like tea?\"\n",
            "text2 = \"In the sunlit terraces\"\n",
            "text = \" <|endoftext|> \".join((text1, text2))\n",
            "\n",
            "print(text)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 7,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "[1130, 5, 355, 1126, 628, 975, 10, 1131, 55, 988, 956, 984]\n",
                  "<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces\n",
                  "<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces\n"
               ]
            }
         ],
         "source": [
            "class SimpleTokenizerV2:\n",
            "    def __init__(self, vocab):\n",
            "        self.token_to_int = vocab\n",
            "        self.int_to_token = {idx: token for  token, idx in vocab.items()}\n",
            "    \n",
            "    def encode(self, text):\n",
            "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
            "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
            "        preprocessed = [item if item in self.token_to_int else \"<|unk|>\" for item in preprocessed]\n",
            "        return [self.token_to_int[token] for token in preprocessed]\n",
            "\n",
            "    def decode(self, integers):\n",
            "        text = \" \".join([self.int_to_token[idx] for idx in integers])\n",
            "        # replaces spaces before punctuation marks for format sentences correctly\n",
            "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
            "        return text\n",
            "\n",
            "tokenizer = SimpleTokenizerV2(vocab)\n",
            "encoded = tokenizer.encode(text)\n",
            "# tokenizer.int_to_token.keys()\n",
            "print(encoded)\n",
            "decoded = tokenizer.decode(encoded)\n",
            "print(decoded)\n",
            "print(tokenizer.decode(tokenizer.encode(text)))\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 8,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n",
                  "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
               ]
            }
         ],
         "source": [
            "bpe_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
            "text = (\n",
            "\"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
            "\"of someunknownPlace.\"\n",
            ")\n",
            "encoded = bpe_tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
            "print(encoded)\n",
            "strings = bpe_tokenizer.decode(encoded)\n",
            "print(strings)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 9,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "[33901, 86, 343, 86, 220, 959]\n",
                  "Akwirw ier\n"
               ]
            }
         ],
         "source": [
            "new_word = \"Akwirw ier\"\n",
            "\n",
            "encoded = bpe_tokenizer.encode(new_word)\n",
            "print(encoded)\n",
            "decoded = bpe_tokenizer.decode(encoded)\n",
            "print(decoded)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 10,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "['I', ' ', 'HAD', ' ', 'always', ' ', 'thought', ' ', 'Jack', ' ']\n",
                  "20479\n",
                  "['year', 'years', 'yellow', 'yet', 'you', 'younger', 'your', 'yourself', '<|unk|>', '<|endoftext|>']\n",
                  "[40, 367, 2885, 1464, 1807, 3619, 402, 271, 10899, 2138]\n"
               ]
            }
         ],
         "source": [
            "with open(\"the-verdict.txt\", \"r\") as f:\n",
            "    lines = f.read()\n",
            "\n",
            "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', lines)\n",
            "print(preprocessed[:10])\n",
            "preprocessed = [item for item in preprocessed if item.strip()]\n",
            "print(len(lines))\n",
            "all_words = sorted(set(preprocessed))\n",
            "all_words.extend([\"<|unk|>\", \"<|endoftext|>\"])\n",
            "print(all_words[-10:])\n",
            "vocab_size = len(all_words)\n",
            "enc_text = bpe_tokenizer.encode(lines)\n",
            "vocab_size = len(enc_text)\n",
            "print(enc_text[:10])\n",
            "\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 11,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "[290, 4920]\n",
                  "[4920, 2241]\n",
                  " and established -->  established himself\n"
               ]
            }
         ],
         "source": [
            "block_size = 2\n",
            "enc_sample = enc_text[50:]\n",
            "for i in range(block_size):\n",
            "    x = enc_sample[i:block_size]\n",
            "    y = enc_sample[i+1:i+block_size+1]\n",
            "    print(x)\n",
            "    print(y)\n",
            "    print(bpe_tokenizer.decode(x), \"-->\", bpe_tokenizer.decode(y))\n",
            "    break\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 12,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "(tensor([  44,  149, 1003,   57]), tensor([ 149, 1003,   57,   38]))"
                  ]
               },
               "execution_count": 12,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "import torch\n",
            "from torch.utils.data import DataLoader, Dataset\n",
            "\n",
            "class GPTDatasetV1(Dataset):\n",
            "    def __init__(self, text, tokenizer, max_length, stride):\n",
            "        self.input_ids = []\n",
            "        self.target_ids = []\n",
            "        self.encoded = tokenizer.encode(text)\n",
            "        #max_length = block_size or context_length, so need to substract max_length from range as that will be size of the sliced array \n",
            "        for i in range(0, len(self.encoded) - max_length, stride):\n",
            "            input_chunk = self.encoded[i:i+max_length]\n",
            "            targets = self.encoded[i+1:i+max_length+1]\n",
            "            self.input_ids.append(torch.tensor(input_chunk))\n",
            "            self.target_ids.append(torch.tensor(targets))\n",
            "    \n",
            "    def __len__(self):\n",
            "        return len(self.input_ids)\n",
            "    \n",
            "    def __getitem__(self, idx):\n",
            "        return (self.input_ids[idx], self.target_ids[idx])\n",
            "\n",
            "\n",
            "dataset = GPTDatasetV1(lines, tokenizer, 4, 1)\n",
            "dataset[1]\n",
            "\n",
            "\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 13,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "torch.Size([4, 4])"
                  ]
               },
               "execution_count": 13,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "\n",
            "def create_dataloader_v1(text, batch_size, max_length, stride, shuffle=True, drop_last=True, num_workers=0):\n",
            "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
            "    dataset = GPTDatasetV1(text, tokenizer=tokenizer, max_length=max_length, stride=stride)\n",
            "    dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers, drop_last=drop_last, shuffle=shuffle)\n",
            "    return dataloader\n",
            "\n",
            "dataloader = create_dataloader_v1(lines, batch_size=4, max_length=4, stride=1, shuffle=False)\n",
            "\n",
            "first_batch = next(iter(dataloader))\n",
            "\n",
            "first_batch[0].shape"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 14,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "[tensor([[   40,   367,  2885,  1464],\n",
                     "         [ 1464,  1807,  3619,   402],\n",
                     "         [  402,   271, 10899,  2138],\n",
                     "         [ 2138,   257,  7026, 15632]]),\n",
                     " tensor([[  367,  2885,  1464,  1807],\n",
                     "         [ 1807,  3619,   402,   271],\n",
                     "         [  271, 10899,  2138,   257],\n",
                     "         [  257,  7026, 15632,   438]])]"
                  ]
               },
               "execution_count": 14,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "\n",
            "dataloader = create_dataloader_v1(lines, batch_size=4, max_length=4, stride=3, shuffle=False)\n",
            "\n",
            "first_batch = next(iter(dataloader))\n",
            "\n",
            "first_batch"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Chapter 3\n",
            "Prior to transformers, RNNs were a popular architecture for NLP tasks such as machine translation. These networks consist of an encoder and decoder network. The encoder processes a sequence of tokens, where the first token is passed to the first hidden layer, the second is passed to the second layer state AS WELL AS the hidden state of the first token. The last hidden state of the encoder is passed to the decoder network, therefore it relies solely on the encapsulated context from the last hidden state of the encoder. It does not have direct access to previous tokens/hidden states (e.g words in a sentence), but only the last generated hidden state from the encoder. Although the the task of the encoder is to generate a sufficiently rich representation of the entire embedded sentence, this is remains limited to short sequence lengths and highlights the main limitation of the classical RNN architecture.\n",
            "\n",
            "#### Bahdanau attention (2014)\n",
            "\n",
            "Modified the encoder-decoder RNN so the decoder can selectively access different parts of the input sequence at each decoding step. "
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Scaled dot product attention\n",
            "\n",
            "The following implements self-attention where the dot products are scaled by the dimension of the compute key matrix encoding representations of the tokens after apply the key weights matrix. The reason for normalised and scaling these values is to improve training performance. For large architectures that use a large embedding dimension (e.g 1000 for GPT models), large dot products can result in very small gradients during backpropagation due to the softmax function. If one value dominates output from the dot product, the softmax normalisation will accordingly scale all the remaining values to very small values. Scaling by the sqrt of the embedding dim prevents these very large values from dominating the normalisation.\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 15,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "tensor([[ 0.0280, -1.5794, -1.2866],\n",
                  "        [ 2.2193,  0.5818,  0.6965],\n",
                  "        [ 0.0583,  0.7548, -1.3453],\n",
                  "        [ 0.6103,  0.5915,  0.2866]], grad_fn=<SelectBackward0>)\n",
                  "input shape torch.Size([4, 4, 3])\n",
                  "torch.Size([2])\n",
                  "torch.Size([4, 2])\n",
                  "tensor(0.0853, grad_fn=<AddBackward0>)\n",
                  "tensor(-2.1828, grad_fn=<AddBackward0>)\n",
                  "tensor(-2.1828, grad_fn=<DotBackward0>)\n",
                  "attention_scores_2 shape: torch.Size([4])\n",
                  "values shape: torch.Size([4, 2])\n"
               ]
            },
            {
               "data": {
                  "text/plain": [
                     "tensor([ 1.2619, -0.3508], grad_fn=<SqueezeBackward4>)"
                  ]
               },
               "execution_count": 15,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "\n",
            "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
            "vocab_size = tokenizer.n_vocab\n",
            "output_dim = 3  # for illustration\n",
            "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
            "input_ids, target_ids = first_batch\n",
            "inputs = token_embedding_layer(input_ids)\n",
            "\n",
            "print(inputs[1])\n",
            "print(\"input shape\", inputs.shape)\n",
            "\n",
            "x2 = inputs[1][1]\n",
            "d_in = x2.shape[0]\n",
            "d_out = 2\n",
            "\n",
            "torch.manual_seed(0)\n",
            "W_query = torch.nn.Parameter(torch.randn(d_in, d_out))\n",
            "K_query = torch.nn.Parameter(torch.randn(d_in, d_out))\n",
            "V_query = torch.nn.Parameter(torch.randn(d_in, d_out))\n",
            "\n",
            "\n",
            "x2_query = x2 @ W_query # query vector for 2nd token in the first example of the batch\n",
            "keys = inputs[1] @ K_query # key matrix for all tokens in the first example of the batch\n",
            "values = inputs[1] @ V_query # value matrix for all tokens in the first example of the batch\n",
            "\n",
            "print(x2_query.shape) # query vector\n",
            "print(keys.shape) # \n",
            "\n",
            "# attention score\n",
            "tmp_score = 0\n",
            "x2_key = keys[1]\n",
            "for i in range(2):\n",
            "    tmp_score += x2_query[i] * x2_key[i]\n",
            "    print(tmp_score)\n",
            "attention_score_22 = x2_query.dot(keys[1])\n",
            "print(attention_score_22)\n",
            "\n",
            "attention_scores_2 = x2_query @ keys.T\n",
            "# softmax normalisation\n",
            "d_k = keys.shape[1] # embedding dimension of the keys\n",
            "# SCALED DOT PRODUCT ATTENTION\n",
            "attention_weights_2 = torch.nn.functional.softmax(attention_scores_2 / (d_k ** 0.5), dim=-1)\n",
            "attention_weights_2\n",
            "\n",
            "# context vector calculation\n",
            "print(f\"attention_scores_2 shape: {attention_scores_2.shape}\")\n",
            "print(f\"values shape: {values.shape}\")\n",
            "context_vector_2 = attention_weights_2 @ values\n",
            "context_vector_2 # represents relative importance of all the input tokens wrto the 2nd token in the first example of the batch\n",
            "\n",
            "context_vector_2\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 16,
         "metadata": {},
         "outputs": [],
         "source": [
            "class SelfAttention_v1(torch.nn.Module):\n",
            "    def __init__(self, d_in, d_out):\n",
            "        super().__init__()\n",
            "        self.Q = torch.nn.Parameter(torch.randn(d_in, d_out))\n",
            "        self.K = torch.nn.Parameter(torch.randn(d_in, d_out))\n",
            "        self.V = torch.nn.Parameter(torch.randn(d_in, d_out))\n",
            "    \n",
            "    def forward(self, x):\n",
            "        query = x @ self.Q\n",
            "        keys = x @ self.K\n",
            "        values = x @ self.V\n",
            "        attention_scores = query @ keys.T\n",
            "        d_k = keys.shape[1]\n",
            "        # rescale the attention scores because dot products can be large when embedding dimension is large\n",
            "        # i.e the sum is taken over the embedding dimension, so many values result in a large dot product\n",
            "        # results in very small gradients\n",
            "        attention_weights = torch.nn.functional.softmax(attention_scores / (d_k ** 0.5), dim=-1)\n",
            "        context_vector = attention_weights @ values\n",
            "        return context_vector\n",
            "\n",
            "class SelfAttention_v2(torch.nn.Module):\n",
            "    def __init__(self, d_in, d_out, qkv_bias: bool = False):\n",
            "        super().__init__()\n",
            "        # proper initialization of weights in torch nn.Linear modules\n",
            "        self.Q = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
            "        self.K = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
            "        self.V = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
            "    \n",
            "    def forward(self, x):\n",
            "        query = self.Q(x)\n",
            "        keys = self.K(x)\n",
            "        values = self.V(x)\n",
            "        attention_scores = query @ keys.T\n",
            "        d_k = keys.shape[1]\n",
            "        attention_weights = torch.nn.functional.softmax(attention_scores / (d_k ** 0.5), dim=-1)\n",
            "        context_vector = attention_weights @ values\n",
            "        return context_vector\n",
            "\n",
            "    \n",
            "\n",
            "\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 17,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "torch.Size([4, 4, 3])"
                  ]
               },
               "execution_count": 17,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "inputs.shape"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 18,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "d_in 3, d_out 2\n",
                  "torch.Size([4, 3])\n",
                  "sa_v1\n",
                  "tensor([ 1.0938, -0.2683], grad_fn=<SelectBackward0>)\n",
                  "sa_v2\n",
                  "tensor([ 0.0730, -0.2277], grad_fn=<SelectBackward0>)\n",
                  "torch.Size([3, 2])\n",
                  "torch.Size([2, 3])\n",
                  "updated sa_v1\n",
                  "tensor([ 0.0730, -0.2277], grad_fn=<SelectBackward0>)\n"
               ]
            }
         ],
         "source": [
            "# compare SelfAttentionV1 and V2\n",
            "d_in = inputs.shape[-1] # embedding dim\n",
            "d_out = 2\n",
            "print(f\"d_in {d_in}, d_out {d_out}\")\n",
            "sa_v1 = SelfAttention_v1(d_in=d_in, d_out=d_out)\n",
            "sa_v2 = SelfAttention_v2(d_in=d_in, d_out=d_out)\n",
            "\n",
            "x2 = inputs[1]\n",
            "print(x2.shape)\n",
            "sa_v1_mat = sa_v1(inputs[1])\n",
            "sa_v2_mat = sa_v2(inputs[1])\n",
            "print(\"sa_v1\")\n",
            "print(sa_v1_mat[0])\n",
            "print(\"sa_v2\")\n",
            "print(sa_v2_mat[0])\n",
            "\n",
            "print(sa_v1.Q.shape)\n",
            "print(sa_v2.Q.weight.shape)\n",
            "\n",
            "sa_v1.Q = torch.nn.Parameter(sa_v2.Q.weight.T)\n",
            "sa_v1.V = torch.nn.Parameter(sa_v2.V.weight.T)\n",
            "sa_v1.K = torch.nn.Parameter(sa_v2.K.weight.T)\n",
            "print(\"updated sa_v1\")\n",
            "sa_v1_mat_up = sa_v1(inputs[1])\n",
            "print(sa_v1_mat_up[0])\n",
            "\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 19,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "torch.Size([4, 4, 2]) torch.Size([4, 4, 2])\n",
                  "context length 4\n",
                  "tensor([[0., 1., 1., 1.],\n",
                  "        [0., 0., 1., 1.],\n",
                  "        [0., 0., 0., 1.],\n",
                  "        [0., 0., 0., 0.]])\n",
                  "tensor([[False,  True,  True,  True],\n",
                  "        [False, False,  True,  True],\n",
                  "        [False, False, False,  True],\n",
                  "        [False, False, False, False]])\n",
                  "tensor([[[-0.8525,    -inf,    -inf,    -inf],\n",
                  "         [ 0.3941, -0.0829,    -inf,    -inf],\n",
                  "         [-1.2522,  0.3203, -0.1983,    -inf],\n",
                  "         [ 1.6815, -0.5202,  0.9057, -1.7346]],\n",
                  "\n",
                  "        [[-1.7346,    -inf,    -inf,    -inf],\n",
                  "         [ 1.4834,  0.3129,    -inf,    -inf],\n",
                  "         [-0.3322,  0.9269,  0.7625,    -inf],\n",
                  "         [ 0.7294, -0.0308,  0.5238, -0.0939]],\n",
                  "\n",
                  "        [[-0.0939,    -inf,    -inf,    -inf],\n",
                  "         [-0.1420,  0.6980,    -inf,    -inf],\n",
                  "         [ 0.0889, -0.2988,  0.0962,    -inf],\n",
                  "         [ 0.1063,  0.2906, -0.1457, -0.1490]],\n",
                  "\n",
                  "        [[-0.1490,    -inf,    -inf,    -inf],\n",
                  "         [ 0.1830,  0.0379,    -inf,    -inf],\n",
                  "         [ 0.2817, -0.4424, -0.5308,    -inf],\n",
                  "         [ 0.2276, -0.6726, -0.3217,  0.0171]]], grad_fn=<MaskedFillBackward0>)\n"
               ]
            },
            {
               "data": {
                  "text/plain": [
                     "tensor([[[1.0000, 0.0000, 0.0000, 0.0000],\n",
                     "         [0.5835, 0.4165, 0.0000, 0.0000],\n",
                     "         [0.1627, 0.4946, 0.3428, 0.0000],\n",
                     "         [0.5325, 0.1123, 0.3077, 0.0476]],\n",
                     "\n",
                     "        [[1.0000, 0.0000, 0.0000, 0.0000],\n",
                     "         [0.6959, 0.3041, 0.0000, 0.0000],\n",
                     "         [0.1784, 0.4346, 0.3869, 0.0000],\n",
                     "         [0.3325, 0.1942, 0.2875, 0.1858]],\n",
                     "\n",
                     "        [[1.0000, 0.0000, 0.0000, 0.0000],\n",
                     "         [0.3557, 0.6443, 0.0000, 0.0000],\n",
                     "         [0.3616, 0.2749, 0.3635, 0.0000],\n",
                     "         [0.2624, 0.2989, 0.2196, 0.2191]],\n",
                     "\n",
                     "        [[1.0000, 0.0000, 0.0000, 0.0000],\n",
                     "         [0.5256, 0.4744, 0.0000, 0.0000],\n",
                     "         [0.4625, 0.2772, 0.2604, 0.0000],\n",
                     "         [0.3258, 0.1724, 0.2210, 0.2808]]], grad_fn=<SoftmaxBackward0>)"
                  ]
               },
               "execution_count": 19,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "# masked self-attention (causal self-attention)\n",
            "# ensures that no current position is influenced by future positions (e.g for generative tasks)\n",
            "queries = sa_v2.Q(inputs)\n",
            "keys = sa_v2.K(inputs)\n",
            "print(queries.shape, keys.shape)\n",
            "attention_scores = queries @ keys.transpose(1,2)\n",
            "# normalisation of scores to they form a probability distribution summing to 1 - this is applied along the embedding dim \n",
            "attention_weights = torch.nn.functional.softmax(attention_scores / keys.shape[-1] ** 0.5, dim=-1)\n",
            "\n",
            "context_length = inputs.shape[1]\n",
            "print(f\"context length {context_length}\")\n",
            "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
            "print(mask)\n",
            "print(mask.bool())\n",
            "# negative infinity approaches 0 in softmax function (because e-inf approaches 0)\n",
            "attention_scores_masked = attention_scores.masked_fill_(mask.bool(), -torch.inf)\n",
            "print(attention_scores_masked)\n",
            "# normalised weights\n",
            "attention_weights = torch.nn.functional.softmax((attention_scores_masked) / keys.shape[-1] ** 0.5, dim=-1)\n",
            "attention_weights"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 20,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "tensor([[2., 2., 0., 2.],\n",
                  "        [2., 0., 0., 0.],\n",
                  "        [0., 2., 0., 2.],\n",
                  "        [2., 2., 2., 2.]])\n"
               ]
            }
         ],
         "source": [
            "# Dropout\n",
            "# only applied during training, not inference\n",
            "torch.manual_seed(123)\n",
            "dropout = torch.nn.Dropout(0.5)\n",
            "example = torch.ones((context_length, context_length))\n",
            "# to compensate for the reduction in active units, the un-zero'd values are scaled by (original_val / dropout_prob (0.5))\n",
            "# 1 / 0.5 = 2\n",
            "# this ensures the overall influence of the weight is consistent at both training and inference time\n",
            "print(dropout(example))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 21,
         "metadata": {},
         "outputs": [],
         "source": [
            "class CausalAttention(torch.nn.Module):\n",
            "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias: bool= False):\n",
            "        super().__init__()\n",
            "        self.d_out = d_out\n",
            "        self.dropout = torch.nn.Dropout(dropout)\n",
            "        self.Q = torch.nn.Linear(in_features=d_in, out_features=d_out)\n",
            "        self.K = torch.nn.Linear(in_features=d_in, out_features=d_out)\n",
            "        self.V = torch.nn.Linear(in_features=d_in, out_features=d_out)\n",
            "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
            "    \n",
            "    def forward(self, x):\n",
            "        batch, num_token, d_in = x.shape\n",
            "        keys = self.K(x)\n",
            "        queries = self.Q(x)\n",
            "        values = self.V(x)\n",
            "        attn_scores = queries @ keys.transpose(1, 2)\n",
            "        attn_scores.masked_fill_(self.mask.bool()[:num_token, :num_token], -torch.inf)\n",
            "        attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim=-1)\n",
            "        attn_weights = self.dropout(attn_weights)\n",
            "        context_vec = attn_weights @ values\n",
            "        return context_vec\n",
            "        \n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 22,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "torch.Size([4, 4, 2])"
                  ]
               },
               "execution_count": 22,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "context_length = inputs.shape[1]\n",
            "ca=CausalAttention(d_in, d_out, context_length, 0.0)\n",
            "context_vecs = ca(inputs)\n",
            "context_vecs.shape\n",
            "\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 23,
         "metadata": {},
         "outputs": [],
         "source": [
            "# multihead attention\n",
            "# apply multiple query key and value matrices (one for each head) in parallel\n",
            "\n",
            "class MultiHeadAttention(torch.nn.Module):\n",
            "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
            "        super().__init__()\n",
            "        assert (d_out % num_heads == 0), f\"d_out {d_out} must be divisible by num heads {num_heads}\"\n",
            "        self.d_out = d_out\n",
            "        self.num_heads = num_heads\n",
            "        self.head_dim = d_out // num_heads\n",
            "        self.Q = torch.nn.Linear(in_features=d_in, out_features=d_out)\n",
            "        self.K = torch.nn.Linear(in_features=d_in, out_features=d_out)\n",
            "        self.V = torch.nn.Linear(in_features=d_in, out_features=d_out)\n",
            "        self.out_proj = torch.nn.Linear(in_features=d_out, out_features=d_out)\n",
            "        self.dropout = torch.nn.Dropout(dropout)\n",
            "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
            "    \n",
            "    def forward(self, x):\n",
            "        batch, num_token, d_in = x.shape\n",
            "        keys = self.K(x)\n",
            "        queries = self.Q(x)\n",
            "        values = self.V(x)\n",
            "        keys = keys.view(batch, num_token, self.num_heads, self.head_dim)\n",
            "        queries = queries.view(batch, num_token, self.num_heads, self.head_dim)\n",
            "        values = values.view(batch, num_token, self.num_heads, self.head_dim)\n",
            "        keys = keys.transpose(1,2)\n",
            "        queries = queries.transpose(1,2)\n",
            "        values = values.transpose(1,2)\n",
            "        attn_scores = queries @ keys.transpose(2, 3)\n",
            "        mask_bool = self.mask.bool()[:num_token, :num_token]\n",
            "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
            "        attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim=-1)\n",
            "        attn_weights = self.dropout(attn_weights)\n",
            "        context_vec = attn_weights @ values\n",
            "        # contiguous makes a copy of the tensor with the final memory layout specified by the tensor shape\n",
            "        context_vec = context_vec.contiguous().view(batch, num_token, self.d_out)\n",
            "        # project layer\n",
            "        context_vec = self.out_proj(context_vec)\n",
            "        return context_vec\n",
            "        \n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 24,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "torch.Size([4, 4, 2])"
                  ]
               },
               "execution_count": 24,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "torch.manual_seed(123)\n",
            "\n",
            "batch_size, context_length, d_in = inputs.shape\n",
            "d_out = 2\n",
            "mha = MultiHeadAttention(d_in, d_out, context_length, 0.5, num_heads=2)\n",
            "out = mha(inputs)\n",
            "out.shape"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 25,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "torch.Size([2, 1024])\n"
               ]
            },
            {
               "data": {
                  "text/plain": [
                     "torch.Size([2, 1024, 768])"
                  ]
               },
               "execution_count": 25,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "# gpt-2 sized MHA\n",
            "\n",
            "embedding_dim = 768\n",
            "num_heads = 12\n",
            "context_length = 1024\n",
            "\n",
            "dataloader = create_dataloader_v1(lines, batch_size=2, max_length=context_length, stride=1, shuffle=False)\n",
            "first_batch = next(iter(dataloader))\n",
            "print(first_batch[0].shape)\n",
            "\n",
            "vocab_size = tokenizer.n_vocab\n",
            "token_embedding_layer = torch.nn.Embedding(vocab_size, embedding_dim)\n",
            "input_ids, target_ids = first_batch\n",
            "inputs = token_embedding_layer(input_ids)\n",
            "\n",
            "inputs[0].shape\n",
            "\n",
            "mha = MultiHeadAttention(d_in=768, d_out=768, context_length=context_length, dropout=0.2, num_heads=12)\n",
            "\n",
            "out = mha(inputs)\n",
            "\n",
            "out.shape"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 63,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "(tensor([[-1.3039e-08, -9.3132e-09, -6.2088e-10,  ..., -3.1044e-09,\n",
                     "          -7.4506e-09, -4.9671e-09],\n",
                     "         [-9.3132e-09, -6.2088e-10, -1.4901e-08,  ..., -7.4506e-09,\n",
                     "          -4.9671e-09,  2.4835e-09]], grad_fn=<MeanBackward1>),\n",
                     " tensor([[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
                     "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
                     "        grad_fn=<VarBackward0>))"
                  ]
               },
               "execution_count": 63,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "# GPT-2 implementation\n",
            "# vocab size is determined by the BPE tokenizer\n",
            "GPT_CONFIG_124M = {\"vocab_size\": 50257, \"context_length\": 1024, \"emb_dim\": 768, \"n_heads\": 12, \"n_layers\": 12, \"drop_rate\": 0.1, \"qkv_bias\": False}\n",
            "\n",
            "class LayerNorm(torch.nn.Module):\n",
            "    def __init__(self, emb_dim):\n",
            "        super().__init__()\n",
            "        self.eps = 1e-5\n",
            "        # Parameter = trainable parameter in torch\n",
            "        self.scale = torch.nn.Parameter(torch.ones(emb_dim))\n",
            "        self.shift = torch.nn.Parameter(torch.zeros(emb_dim))\n",
            "\n",
            "    def forward(self, x):\n",
            "        # dim = -1  --> embedding dimension\n",
            "        mean = x.mean(dim=-1, keepdim=True)\n",
            "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
            "        norm_x = (x-mean) / torch.sqrt(var + self.eps)\n",
            "        return self.scale * norm_x + self.shift\n",
            "\n",
            "first_batch\n",
            "embedding_dim = 768 \n",
            "ln = LayerNorm(embedding_dim)\n",
            "out_ln = ln(inputs)\n",
            "# expect mean = 0 and var = 1\n",
            "out_ln.mean(dim=-1), out_ln.var(dim=-1, unbiased=False)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 64,
         "metadata": {},
         "outputs": [],
         "source": [
            "# GELU activation function\n",
            "\n",
            "class GELU(torch.nn.Module):\n",
            "    def __init__(self):\n",
            "        super().__init__()\n",
            "    \n",
            "    def forward(self, x):\n",
            "        value = 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2.0 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3))))\n",
            "        return value\n",
            "\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 65,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxYAAAEiCAYAAABkykQ1AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAWidJREFUeJzt3Qd4FFUXBuCPdAgkEEihd0JPIHRUQEFARLAgohQVpBgQRFFAfhAbKiooHRvSpChFERFEEBUQktBCCZ0klBRKEtLL/s+5YWMSNpCQMrO73/s8Y3Yns7t3JjJ3bznnljIYDAYQEREREREVgk1hXkxERERERCTYsCAiIiIiokJjw4KIiIiIiAqNDQsiIiIiIio0NiyIiIiIiKjQ2LAgIiIiIqJCY8OCiIiIiIgKjQ0LIiIiIiIqNDYsiIiIiIio0NiwIDLh7bffRqlSpTT57CVLlqjPPn/+fIl/dlpaGt544w1Ur14dNjY26Nu3L/RIy2tERNatc+fOatPC888/j1q1amny2REREXjqqadQsWJFdf+dPXs29EjLa0RsWFilc+fOYfTo0WjQoAHKlCmjtsaNG8Pf3x+HDx82+QU7r+3KlSvqOPmCJ88/+eSTPD9X/qE/+uijJn8XEBCgXi9fGEtKQkKCOr+dO3dCCx988AE2bNgAPfnmm28wc+ZMVXl89913ePXVVzUtjx6vEZGlMjbYjZudnR2qVq2qvqhdvHjxtuPly3VedUPDhg1ve1+5z5tyt/pD9pd0R8KxY8dU/aBF58WlS5fUZx88eBB6IvXBb7/9hkmTJmHZsmXo0aOHZmXR6zUiwE7rAlDJ2rRpE/r3768qjOeeew4+Pj6qZ/rEiRNYt24dFixYoBoeNWvWzPE62V+2bNnb3q98+fIwV9KwmD59unqcu/dpypQpmDhxYrF/aZYv8LlHBQYNGoRnnnkGjo6OKGl//PGH+iIxa9Ys6IEerxGRpXvnnXdQu3ZtJCUlYe/evaph8PfffyM4OBhOTk45jq1WrRpmzJhx23u4urrCnEnDQuoHqRty935v3bq12L80y2fL5/r6+ub43ZdffomMjAxoQeqHPn364PXXX4fW9HqNiA0Lq3LmzBn1ZUwaDdu3b0flypVz/P6jjz7C/PnzVUMjN/lyV6lSJVgLaXjJpgVbW1u1aSEyMtIsGotaXiMiS9ezZ0+0atVKPR42bJi690v98NNPP+Hpp5++rQExcOBAWBMHBwfNPtve3l6zzzaX+kHLa0ScCmVVPv74Y8THx+Pbb7+9rVEh5Iv0K6+8oubX69W1a9dUb0mzZs3UCIqLi4uqBA8dOnTbsdLbJkOlMuVLetnknJ944gnVwJLhbXd3d3Wc9HoYh+/leFMxFk2bNkWXLl1u+wzpFZEefml4ZR+279Chg5qHWrp0afj5+eGHH37I8Tp5b/lbyHQj42fLdIM7xQ9Io69Jkyaql75KlSpq6tqNGzdyHCO9a1JW6W2T8so0Nymf/O3vxDgVYceOHTh69GhWmWSamGzGx6Zek336mpyD/F1k2oSMMshjuc7yN0tPT7/t2n3++efqbyl/HzlOhtaN0yX0do2IrNX999+vfsq9U89kKq/cI+rUqaPuKV5eXnjxxRdx9erV246Ve9TQoUPVfULuFzJCM2rUKKSkpKj7S79+/dRxco/Ifj/MHWMhcQdSdxpHv7MLCQlRr5s7d26+6y/5jNatW6vHL7zwQtZnG++zpuIH5D752muvqbpbzsXb21vVQwaDIcdx8j4yDVqml8o9UI6V++WWLVvueF2N91t5v3nz5mWV6U7xiKbu0cbp0DL61aZNG/U3kr/V0qVLb3u93Ldl6pW8RsopI2ODBw9GdHS0Lq8R/YcjFlY2DapevXpo27ZtgV8rN8Tc5GZa0r0XZ8+eVf/g5aYvFYHc1BctWoROnTqpL4pSSQj5Eis3MBmZkVGasWPHIi4uDtu2bVPD+V27dlXTu6Qiefzxx1WDQzRv3tzk58r0MbmBSkyJVFZGcoOUIVn5DCP5svzYY4+pqWZSSa1atUqVV65/r1691DEyP1V6AuXmOnz4cLWvbt26eZ63fLZUXFJuKbNUWFL+/fv3459//snRQ3P9+nX1BV3OSXoXpVHz5ptvqspMKjFT5Eu9lOn999/HzZs3s6Y2NGrUCMePHy/Q30iufffu3dX/Z3Lj/v333/Hpp5+q85OyG0mlLhWBlEmuhQSO//XXX2rqhfSW6u0aEVkr45fDChUqmPz3Ll/2cpNOFWdnZ5Qkub9LHSFfNuU+LZ0kixcvVj/lvmL8Aiz3bLmvyJdXubdIPIg0NOQ+IFNkH3jgAdXJ9sUXX2Dy5MnqPiiMP7Pz9PRU9c+aNWswbdq0HL9bvXq1Glk1NlLyU3/JZ8hUtKlTp6qyGRt10lllinwxlvpGOoXknirTgiQOYsKECeqcck9rlTpLpj2//PLLKFeunDrHJ598EqGhoaozzBS5HnI/limo3bp1U1/w79Xp06dVR5yUdciQISquTxoC0gEnX+CF1EFy3lL3SMOwZcuW6v8xGTELDw/X5TWibAxkFWJiYqRZbujbt+9tv7t+/bohKioqa0tISMj63bRp09TrTG3e3t5Zx507d07tmzlzZp5lqFmzpqFXr14mf7d//371+m+//faO55GUlGRIT0/PsU8+29HR0fDOO+9k7fvmm2/U+3322We3vUdGRob6Kecqx8g55mY8b6OQkBD1fM6cOTmOe/nllw1ly5bNcc2yPxYpKSmGpk2bGh588MEc+52dnQ1Dhgy57bPlGshnyXmJyMhIg4ODg+Hhhx/Oce5z585Vx8m5GnXq1EntW7p0ada+5ORkg5eXl+HJJ5803I28vkmTJjn27dixQ72n/MzO+DfP/jeT85F92f8WokWLFgY/P7+s53/88Yc67pVXXsnz76PXa0RkqYz/rn7//Xd1fwwLCzP88MMPBnd3d3WPlefZGf8tmdpGjBhx2/vKfd6Uu9Ufsj/7v/e85L73iu+//169dteuXVn7Bg8ebLCxsTFZHuP9Z+3atSbve8bzls1o0aJF6tgjR47kOK5x48Y57vv5rb/uVB/K/VDqUqMNGzaoY997770cxz311FOGUqVKGU6fPp21T46T+2T2fYcOHTJZt5kix/n7+9+xrszrHi2k3Ln/FnLvlvN/7bXXsvZNnTpVHbdu3bo8/z56vUZkMHAqlJWIjY1VP00FYMuQrvRYGzcZ6sztxx9/VL1B2TeZUlXSZFjSGAMiPWUyxC3nJMOaQUFBOcor84LHjBlz23vcSxpZmU4lvRzSA2Ukny89XL1791a9c0bZH0vPeExMjOpRyV6+gpAefxn5GDduXI74l5deekkNpf/yyy85jpfrkX3Os8wHlt456S0rKSNHjszxXM4/++fL30f+Drl7+O7172OO14hIr2TUT+oCmTYivcsy8iC9xTIdJTeZcpK7bpBN/i2WtOz3XpkKK73c7dq1U8+N91+ZgimjBnLfNsaRFPb+IyOfMoKfvX6QkXEZhZDR7oLWXwWxefNmNSoiIyzZybQf+Z7866+/3va3zT7yK6P0co8sqXufZKA0jjAI+f9Mzj93/SCJZWQ2QVH8fcztGpk7ToWyEjKcZxxizE2GYmWakAzL5hWEJ0OhJRG8fbebhnFevsyll+xV2eftZx+ilLnAcrMqygBsqSBkWFyGTmVOvszzlGC27BWHkClP7733nkqDl5ycnO9zy8uFCxfUTzmf7OTLsMxPNf7eSCr/3J8lUxhypxIuLsZ4idyfL42s7H8fGfZ3c3Mrks80t2tEpGfSuSSdKdIpIlNVdu3alWcGNml0yBexknC3e6hM2ZXpkDL9VO7N2cm5iKioKNXRJvPni4rUjQ899JCaDvXuu++qfdLIkPrHOM22IPVXQci9Te6lxjreyDhtK/e9r0aNGre9R+77c3HKz+dL/SBTj4qKuV0jc8cRCyshmTskeFl6UXKTufBSMXTs2LHYv3AmJiaa/J3MazUec7f0o+PHj1cNneXLl6t5ktI7JnMzizu9nDQgpHdj7dq16rlUInJds+fylhgBmcsp5yGVh/SUSPmeffbZ24LEikte2ZLu9fPzqsxzB2Pf7fP1pKivEZElkdE7qRPky52MVMiXcLmHmeqYKgrG+35h6weJl5JUozJiKnPkJS2sMei2uOsHibM7efJk1roKUj9IYyN7h5yW9Zfe6wc93XvNoYx6xoaFFZHAYQmc2rdvnyafL2lu5cZrigTaGo+5E5l6JFk6vv76a3Ujf/jhh1UFmDvzjwxjynumpqbm+V4FHUGQYDupcKUnSgKNpeKSzEfZe/JkCFcqP6kwJOhMAoHz6s3L7+cbr4nxGhnJ1B9Ta44UNWPAZu5rnLuXpyDk7yMBlKaSApjjNSKyVPIlS5I5yL9XY3ajoiYjnJKdLfe/XyPZL7+/06i59CZLsg5Zf0hGLWQajQQay4hl7s+SaS2mOtkKUz9IXSAjpFI/SONC6rrsST0KUn8V5LPl3iZ/G5l1kJ2sTWX8vTnWD0X599H6GlkbNiysyBtvvKFuzvKFV6Y9lXRr/JFHHlEZHXKvpCzThb766it4eHio7A93q+Ryl1NGEHKvCis9bTK/1lRFaHy9XAtTN8S7jVpIdhGZHiDvn3salJRPbnjZe2sko4qp1aNlCkF+PlsqHqmwJDNF9nOXykmG942ZpoqL3HTlvGQ6RHYyInOv5O8j52IqRWP2czSXa0RkySQOTzpVZs+erWIXiprcX+RL9s8//6wy72Qnz2W//P5Oo6HG3+WuH6TM2UmMgzQC5D1NrQRufL0xo1V+6wfJkCjZ8GSkQqZiyf0o98Ke+a2/CvLZUq9KfZO7rpNMR1IXFXeWO2MsQvb6wZgmvDD1g6TgXb9+fZH8fbS+RtaGMRZWpH79+li5ciUGDBig5qIbV96Wf6jSqyu/k5uuqQA96WkxFfgtPUKSbs9IeoxMVTxyg5W0cPKFXFLtSeOmRYsWKnhNenikd0JyWd9t4SFJIStp5iSdoKSWO3LkCFasWHFbr5Skw5P3k2FnGaGRYDG52UmQr6SQk9VDJdBPAsnk82U+scz3lyH/O829laF2yUMumxyfezRCvsB+9tlnanqUTB2Qeb4yX1nS/Oaevy/p9aQ8crzM/5QREVOpgKWHbdKkSepLuLyvTLWSHjz5Yi+5vIt7cSqZ7iV/szlz5qibsFQkEkeSew5zQUivnaQulIbAqVOn1HnJVACZSia/kzzi5nSNiCydpOaU+4CkiM6enEEa7jKtx5Tc/+7k/m9qPQBJBy7ThCTQWjqXpK6QoHDplJF0sXLfkd/fiYxCyBQjWY9GRqolDk6mQkndlpu8l/xO0rzKZ8lc+8uXL6sv+ZJqVBoJkqxDGgKyMKCco4xMP/jgg6oDLC/S0STnLPcdaWTkTsee3/pL7rHy2oULF6q4APkSLfc9uf/lJkHocs9866231PWSOl3ObePGjSqA/k4puouCNPgkJkHSuMr/I3LN5O8s9+TcjcT8kveR7xzG7wpSD8jotkzLk2si52hO18jqaJ2WikqepFEbNWqUoV69egYnJydD6dKlDQ0bNjSMHDnScPDgwRzH3indbPZUfMZ0gXlty5Yty0pt++qrrxpq165tsLe3N7i4uBi6dOli+PXXX/NVdknXJ2npKleurMrdsWNHw549e25L/2dMPfjWW29lfZakE5X0cmfOnMk6Zvfu3SoNqqSXy556Nq8UekI+U343bNgwk7//+uuvDfXr11cp9OS6Sjo8U+934sQJwwMPPKDOQ35nTKtqKk2fMXWqvJ+ci6enp/obyvW8W7pYU+n38pLX6yX1pKRiLVOmjKFChQoqlWRwcLDJdLOSIjY3U+eflpam0kjKOcn1l5SWPXv2NAQGBur6GhFZqjulhZU0qXXr1lWb/Nu9W7rZ7P/eje+b12ZMY3v8+HFD//79DR4eHgY7Ozv185lnnlH78yM8PNzw+OOPG8qXL29wdXU19OvXz3Dp0iWTacUvXLig0s4aU+nWqVNHpVKV1NNGX375pdpva2ubo74zVd+I2NjYrHvV8uXLC1V/bdy4UaWrleuQ/T5r6j4VFxen6tUqVaqoe5/UP3JvzZ66O690sULez1Ra79zyer3cs9u2bavu4zVq1FBp3vNKN2sq5byp87969aph9OjRhqpVq6r3rVatmipjdHS0rq8RGQyl5D9aN26IiIiIiMi8McaCiIiIiIgKjQ0LIiIiIiIqNDYsiIiIiIio0NiwICIiIiKiQmPDgoiIiIiICo0NCyIiIiIiKjSrWyBPFuGSpd1lQZWCLAlPRGTJJPN4XFycWohQFsq0VqwjiIjuvX6wuoaFVBjVq1fXuhhERLoUFhaGatWqwVqxjiAiuvf6weoaFtILZbw4Li4uWheHiEgXYmNj1Rdq4z3SWrGOICK69/rB6hoWxqFtqTBYaRAR5WTt039YRxAR3Xv9YL0TaYmIiIiIqMiwYUFERERERObdsFiwYAGaN2+eNeTcvn17/Prrr3d8zdq1a9GwYUM4OTmhWbNm2Lx5c4mVl4iISgbrByIi86Npw0Iiyz/88EMEBgYiICAADz74IPr06YOjR4+aPH737t0YMGAAhg4digMHDqBv375qCw4OLvGyExFR8WH9QERkfkoZJDmtjri5uWHmzJmqcsitf//+iI+Px6ZNm7L2tWvXDr6+vli4cGG+I9tdXV0RExPDwDwiIjO6NxZ3/WAu14GIqCQV5L6omxiL9PR0rFq1SlUMMuRtyp49e9C1a9cc+7p376725yU5OVldkOwbEZGlSU3PwJQNRxB2LQGWprjqByIia/HnySh8ueusWuyuOGmebvbIkSOqokhKSkLZsmWxfv16NG7c2OSxV65cgaenZ4598lz252XGjBmYPn16kZebiEhPpv98FMv3hqrKY/v4znCw002/kW7rB2Pnk2xG7HwiIktz4Wo8xqwMQmxSGlxK26F/6xrF9lma1zze3t44ePAg/v33X4waNQpDhgzBsWPHiuz9J02apIZujJssekREZEmW7jmvGhWSYnzqo00solFREvWDsfNJhviNG1fdJiJLEp+chuFLA1Wjwrd6efRtUbVYP0/z2sfBwQH16tWDn5+fusH7+Pjg888/N3msl5cXIiIicuyT57I/L46OjllZRbjgERFZmr9ORWH6z5lftt/o3hDdGufstTdnxV0/CHY+EZGlMhgMeOPHwwiJiEOlso5YONAPjna2lt2wyC0jIyPHsHR2MiS+ffv2HPu2bduW55xbIiJLdjbqJvxXBCE9w4AnWlbFyE51YMmKo35g5xMRWapFu87il8OXYWdTCgsGtoSXq1Oxf6amMRbSU9SzZ0/UqFEDcXFxWLlyJXbu3InffvtN/X7w4MGoWrWq6qkSY8eORadOnfDpp5+iV69eKphP0hAuXrxYy9MgIipxMQmpGPZdgBreblmjPD54vBlKyVwoC8H6gYjo3u06GYWPt5xQj6c91gSta7mhJGjasIiMjFSVw+XLl9XcVlkMSSqNbt26qd+HhobCxua/QZUOHTqoymXKlCmYPHky6tevjw0bNqBp06YangURUclKS8/A6O+DcDY6HlVcnbBoUCs42Rfv8HZJY/1ARHRvQq8mYMz3B5BhAPr5VcPAtsUXrK37dSyKG3OUE5G5e/uno1iy+zxK29vih1Ht0aSKa6Hfk/fGTLwORGTOElLS8MT83ThxJQ4+1ctj9fB2he54Mst1LIiI6O5W/huqGhViVn+fImlUEBGR+TMYDHjzxyOqUVGprAMWDmxZ4qPZbFgQEZmJPWeuYurGYPX4tW4N0KNpZa2LREREOvHVX+fw86FLKlh7/nN+qOxausTLwIYFEZGZzJkdtSIQaRkG9PapgtEP1tO6SEREpBN/n4rGjF+Pq8f/e7Qx2tQumWDt3NiwICLSubikVAxbuh83ElLRvJorZj7V3KIyQBER0b0LuybB2kEqWPspv2oY3L4mtMKGBRGRjskaFeNWHcTJiJvwdHHEl4MtLwMUERHdm8SUdIxYFojrtzqe3uvbVNOOJzYsiIh0bOZvIdh+IhKOdjZYPKgVPF2Kf4EjIiIyj2DtSesO49jlWFR0lmBtP807ntiwICLSqXVB4Vj45xn1+OOnmqvUgUREROKbf85jw8FLsLUphXnPtUSV8iUfrJ0bGxZERDp0IPQ6Jq47oh77d6mLPr5VtS4SERHpxO4z0fhgc2aw9pRejdCuTkXoARsWREQ6czkmEcOXBSIlLQPdGnvitW7eWheJiIh0Ivx6AkavPKBi8J5oWRXPd6gFvWDDgohIR5JS0zF8aSCi4pLR0KscZvf3hY0NM0ARERFUHTFyeSCuxaegaVUXfPB4M11lCWTDgohIR4F4E344jCMXY+Dm7KAyQDk72mldLCIi0kkdMXn9EQRfjFV1xKJB+ssSyIYFEZFOzN95JtuqqS1R3a2M1kUiIiKdWLL7PNYFXVTB2nOfbYGqOgjWzo0NCyIiHdh2LAKfbA1Rj6f3aaKbQDwiItLe3rNX8d4vmcHakx9phA51K0GP2LAgItJYyJU4jFt1AAYD1Iqpz7XVbtVUIiLSl4s3EuG/IkgFa/f1rYIXO+onWDs3NiyIiDR0PT4Fw5buR3xKOtrXqYj/PdpY6yIREZGOgrVHLQ/E1fgUNKnighlPNNdVsHZubFgQEWkkNT0DL68IQti1RFR3K63iKuxteVsmIiKoYO231gfjcHgMKpSxVytrl3bQV7B2bqzBiIg08t6mY9hz9iqcHWzx1eDWqODsoHWRiIhIJ5btvYAfg8IhGcfnPmseCT3YsCAi0sD3+0Lx3Z4L6vGs/r7w9iqndZGIiEgn/j17Fe/8fEw9ntSzETrW02ewtq4aFjNmzEDr1q1Rrlw5eHh4oG/fvggJycyKkpclS5aouWXZNycnpxIrMxFRYe0/fw1TNwarx68/3AAPN/HSukhERKQTl2MS4b8yCGkZBjzmUwXD7q8Nc6Fpw+LPP/+Ev78/9u7di23btiE1NRUPP/ww4uPj7/g6FxcXXL58OWu7cCGz14+IyByye4xcFojUdAN6Na8M/y71tC4SERHpamXtIETfTEGjyi746El9B2vrqmGxZcsWPP/882jSpAl8fHzUaERoaCgCAwPv+Dq5wF5eXlmbp6dniZWZiOheJaakY8SyAJXdo3FlF8x8yrwqjJLEEW0issZg7akbg3Eo7AbKl7HH4kH6D9bWdYxFTEyM+unm5nbH427evImaNWuievXq6NOnD44ePVpCJSQiuvcK480fDyP4YizcnB2weLAfyjjYaV0s3eKINhFZm+X/hmJNQGaw9pwBLcwiWDs33dRqGRkZGDduHDp27IimTZvmeZy3tze++eYbNG/eXDVEPvnkE3To0EE1LqpVq3bb8cnJyWozio2NLbZzICLKy6JdZ/HToUuwsyml0spWq2B+FUZJj2jnHo2QkQsZ0X7ggQfuOqJNRGRusXfTf8rsKH+zR0PcX98d5kg3IxbSMxUcHIxVq1bd8bj27dtj8ODB8PX1RadOnbBu3Tq4u7tj0aJFeQ6nu7q6Zm0yykFEVJJ2hETioy0n1ONpvRujXZ2KWhfJ7HBEm4gs1ZWYJIxanhmsLbF3wx+oA3Oli4bF6NGjsWnTJuzYscPkqMOd2Nvbo0WLFjh9+rTJ30+aNElVSMYtLCysiEpNRHR3Z6Nu4pXvD8BgAAa0qYGB7WpqXSSzU9AR7Y0bN2L58uXqdTKiHR4enudrZERbRrKzb0REJSU5LR2jVgQi+mYyGnqVM/vYOzut5xyPGTMG69evx86dO1G7dsHTaaWnp+PIkSN45JFHTP7e0dFRbUREJS0uKRUvLQ1AXFIaWtWsgOmPNTHrCkPrEe2///77riPashlJo6JRo0ZqRPvdd9/Nc1R7+vTpRV5mIqL8ePunozgQegOupe2xaJD5x97ZaF1ZSK/SypUrVeaPK1euqC0xMTHrGJn2JKMORu+88w62bt2Ks2fPIigoCAMHDlTBecOGDdPoLIiIbpeRYcCrqw/iTFQ8Krs6YcFAPzjY6WKQ2KwU54i24Kg2EWll5b+h+H5fGKS/6YsBLVCzojPMnabNogULFqifnTt3zrH/22+/VWlohaSftbH5rzK+fv06XnrpJdUAqVChAvz8/LB79240bty4hEtPRJS3Wb+fxO/HI+FoZ6N6odzLceRUbyPagqPaRKSFwAvXMO2nzIVSJ3T3RqcG5hmsrbupUHcjFUp2s2bNUhsRkV79euQy5vyR2Us+44lmaF6tvNZFMjsyoi2j2RIvYRzRFpKEo3Tp0lkj2lWrVlXTmYwj2u3atUO9evVw48YNzJw5kyPaRKQ7EbFJahE8WSj1kWZeGNWpLiyFeU/kIiLSmRNXYvHa2kPq8dD7auOJlgWbvkOZOKJNRBYbrL08EFFxyWjgWRYzn/KxqNg7NiyIiIrIjYQUDF8aiISUdHSoWxGTejbUukhmiyPaRGSJpv98DEGhN+DiZIfFg1rB2dGyvoozkpCIqAikZxgw5vsDCL2WgGoVSmPusy1hZ8tbLBERZfp+X6gK2JYBis+faYFalcw/WDs31npEREVg5m8h+OtUNJzsbVQvlJuzg9ZFIiIinQgKvY5pGzMX7HytWwN0aegBS8SGBRFRIW06fAkL/zyjHst82cZVXLQuEhER6URknKysHYiU9Az0aOIF/y71YKnYsCAiKoTjl2MxYe1h9XhEpzro7VNF6yIREZFOpKRl4OXlQYiITUY9j7L45GnLCtbOjQ0LIqJCBGuPWBaIxNR03F+/Et7ozmBtIiL6z7ubjiHgwnWUc5RgbT+UtbBg7dzYsCAiusdg7VdWHVTB2tXdSmPOgBawtbHcXigiIiqYNfvDsGzvhcxg7QG+qONeFpaODQsionvw6dYQ7DoZpYK1Fw1shfJlGKxNRESZDobdwJQNmStrv9q1AR5s6AlrwIYFEdE9rKw9f2dmsPZHTzZnsDYREWWRxe9GLssM1n64sSdGW3Cwdm5sWBARFcCpiDi8fmtl7WH31UYf36paF4mIiHQiNT0D/iuCcCU2CXXdnfHp0z6wsaJpsmxYEBHlU2xSqgrWjr+1svZErqxNRETZvP/Lcew7fy0zWHtwK5Rzsoc1YcOCiCgfMjIMGL/6EM5Gx6Nq+cxgba6sTURERj8GhmPJ7vPq8az+vqhrBcHaubFWJCLKh7k7TuP34xFwsLPBgoEtUbGso9ZFIiIinTgcfgOT1h9Rj8d1rY+uja0jWDs3NiyIiO5ix4lIzPr9pHr8Xt+maF6tvNZFIiIinYi+eStYOy0DXRt54pUH68NasWFBRHQHF67GY+yqAzAYgOfa1sDTraprXSQiItJZsPalmCTUcXfGZ/2tK1g7NzYsiIjykJiSjpHLgxCblIYWNcpjau/GWheJiIh05IPNx/HvuWtqRe3Fg1rBxcqCtXNjw4KIyASDwYDJ64/g+OVYVCrrgAXP+cHRzlbrYhERkU6sCwrHt/9kBmtLWtl6HtYXrJ0bGxZERCYs3XMB6w9chK1NKcx9tiW8XJ20LhIREelE8MUYTFqXGaz9yoP10L2Jl9ZF0gVNGxYzZsxA69atUa5cOXh4eKBv374ICQm56+vWrl2Lhg0bwsnJCc2aNcPmzZtLpLxEZB0CL1zDu5uOqceTejZEuzoVtS4SERHpxNWbyWpNo+S0DDzY0APjujbQuki6oWnD4s8//4S/vz/27t2Lbdu2ITU1FQ8//DDi4+PzfM3u3bsxYMAADB06FAcOHFCNEdmCg4NLtOxEZJki45Lw8oogpGUY0Kt5ZQy9r7bWRSIiIp1IS8/A6JUHcPFGImpXclbrVVhzsHZupQwykVgnoqKi1MiFNDgeeOABk8f0799fNTw2bdqUta9du3bw9fXFwoUL7/oZsbGxcHV1RUxMDFxcXIq0/ERk/tk9nvvqX+w7dw31Pcpig39HODvawRrw3piJ14GI7uS9Tcfw1d/n4Oxgq+qI+p7lYOliC3Bf1FWMhRRYuLm55XnMnj170LVr1xz7unfvrvabkpycrC5I9o2IyJSPt5xQjQrJ7rFgoJ/VNCr0iFNliUhvNh68qBoVxmBta2hUFJRuGhYZGRkYN24cOnbsiKZNm+Z53JUrV+DpmXM1Q3ku+/OqnKSVZdyqV2cOeiK63S+HL+PLvzIrjE/6NWd2D41xqiwR6S1Y+80fD6vHo7vUQ4+mlbUuki7ppmEhFYjc/FetWlWk7ztp0iQ1EmLcwsLCivT9icj8nY68iTd+OKQej3igDisMHdiyZQuef/55NGnSBD4+PliyZAlCQ0MRGBiY52s+//xz9OjRAxMmTECjRo3w7rvvomXLlpg7d26Jlp2ILMu1+BQVrJ2UmoHO3u54tRuDtXXdsBg9erSKmdixYweqVat2x2O9vLwQERGRY588l/2mODo6qvlg2TciIqP45DSMWh6I+JR0tK3thgndvbUuEpXQVFkiovwEa4/5PkgFa9esWAaf92+h0pCTDhsWEjcujYr169fjjz/+QO3ad8++0r59e2zfvj3HPhkml/1ERAW9B01cdwSnIm/Co5wj5jzbAna2uuhvoRKYKisYh0dEd/LxbyH45/RVlHGwVStru5ax7pW178ZG6+lPy5cvx8qVK1WAntz8ZUtMTMw6ZvDgwWo6k9HYsWPVEPmnn36KEydO4O2330ZAQIBqoBARFcR3u8/j50OXYGdTCvOfawmPclwET4+Ka6qsYBweEeXlp0OXsHjXWfX4k34+8PZisLauGxYLFixQw9udO3dG5cqVs7bVq1dnHSNzai9fvpz1vEOHDqohsnjxYjXv9ocffsCGDRvu2ItFRJRbUOh1vL/5uHo86ZFGaFUr7yk2ZJlTZQXj8IjIlGOXYrNi70Z1rotHmjH2Lj80zaWYnyU0du7cedu+fv36qY2I6F5XTfVfEYTUdAN6NauMFzvW0rpIZKJ+GDNmjJoqK/VAQabKyrSp/E6VlTg82YiIjK5LsPbyABWs/UADd7z+MGPv8otJ2onIqqRnGDBu9UFcjklCHXdnfPhkM5QqxUA8PU5/ktHpjRs3Zk2VFTJdqXTp0llTZatWraqmMxmnynbq1ElNle3Vq5eaOiVTZWWEm4gov8Har6w6gLBriajhVgZfPOPLYO0CYJQiEVmVz7efwl+nolHa3hYLB/qhnBMD8fSIU2WJSAszt4Zk1RGLBvmhfBkHrYtk+SMW586dw19//YULFy4gISEB7u7uaNGihRpultVOiYj0aGdIJOb8cUo9/uCJpmjAVVN1i1NliaikbTp8CYv+zAzW/vip5mhUmUsUFGvDYsWKFWoBIhlalhR+VapUUUPS165dw5kzZ1Sj4rnnnsObb76JmjVrFrgwRETFRXKQyxQo+b76XNsaeLzFnQOBiYjIepy4EosJaw9nLZTa26eK1kWy7IaFjEg4ODiolVB//PHH21LySS5wWYRI5rS2atUK8+fPZ68REelCSloGXl4RhBsJqWhezRVTezfWukgWTeqDf//997ZR7fwEYBMRlbQbCSkYvjQQianpuL9+JbzRo6HWRbL8hsWHH36oVjDNi2TVkLmwsr3//vs4f/58UZWRiKhQPth8HIfCbsC1tD3mPdsSjna2WhfJIv3zzz9qVPvnn39GampqVqC1jGpLY6NOnToYPnw4Ro4cqQKyiYj0kNDjlVUHEXotAdXdSuOLZ7iydokEb9+pUZFbxYoV4efnd69lIiIqMr8cvowluzM7Oj572gfV3cpoXSSL9Nhjj6F///6oVasWtm7diri4OFy9ehXh4eFq1OLUqVOYMmWKSgfboEEDlQaWiEhrn24Nwa6TUXCyt8Giga1QwZnB2iWeFWrJkiUm96elpeVYJZuISEtno27izR8PZy1w9FAjT62LZLEkvask9vj4449x//33Z6WENZLRiiFDhmDLli2qcWFjw6SERKStzUcuY/7OM+rxx0/5oHEVBmsX1j3d2V955RUVP3H9+vWsfSEhIWjbti2+//77QheKiKiwElPSVVzFzeQ0tKnthte6NdC6SBZtxIgRsLfPX+rexo0b46GHHir2MhER5SXkShxeX5u5svbwB+rgMQZra9ewOHDggBrebtasmRrOnjdvHlq2bImGDRvi0KHMPxIRkZam/RSME1fiUKmsA+YOaAE7W/aQl5QdO3bk+btFixaVaFmIiHKLSUjF8GUBSEhJR8d6FfFGd66sXVTuqaatW7euCtJ74okn0KNHD7z66qv46quvVDpaCdYjItLS2oAwrAkIh8TfSSCehwvX1ylJUi9MmDBBBXAbRUdHo3fv3pg4caKmZSMi6ybB2mNXH8CFqwmoWr405gxoyY6nInTPV/KXX35RqWVlUbzy5cvj66+/xqVLl4qybERE9zS8/b+Nwerxq10boEO9SloXySpHLNavX4/WrVvj2LFjqr6Q1a9jY2Nx8OBBrYtHRFZs1raT2BlyK1h7kB/cGKytfcNC5tJKjIUshCcrcB8+fFitcSFTo9asWVO0JSQiyqf45DSMWhGIpNQMPNDAHf5d6mldJKvUoUMH1YCQxoRMk3388cfVyLaslM3FU4lIK1uCL2PujtPq8YdPNEfTqpxlo4uGhUyDksWPXnvtNZQqVQpeXl7YvHkz3nnnHbz44otFXkgiorsxGAyYvP4IzkbFw8vFCbP7+8KGucg1c/LkSQQEBKBatWqws7NTCT4k7SwRkRZORcThtTWZccAvdqyNvi2qal0ki3RPDYvAwED4+Pjctt/f31/9joiopH2/LwwbD15SCxvNfbYFh7c1JAuqyjTZbt26ITg4GPv27VNJP5o3b449e/ZoXTwisjIxiRKsHYj4lHS0q+OGyY9wZW1dNSxkle28eHszsp6ISlbwxRi8/fNR9Viye7Sq5aZ1kayarL69YcMGzJkzB05OTmpKlDQuJOFH586dtS4eEVmRjAwDXl19EOei41HF1QnznmWwdnGyKUiWj7179971OFlt9aOPPlIpaImIiltcUipGrwxCSloGHmrogZfur6N1kazekSNH0LNnzxz7ZI2LmTNnqlW5iYhKyuztp/DHiUg42EmwditULJt35zgVnl1+D5Rg7SeffFKlk5WUga1atUKVKlVUb5QslCeZP/7++28VayErsEoFQkRU3HEVE9cdwflbaQM/fdqHcRU6UKlS3pm4OnXqVKJlISLr9dvRK/hi+yn1eMbjzdCsGoO1dTNiMXToUJw9exaTJ09WjYjhw4fj/vvvV+kEu3fvji+//BI1atTA/v37sXr1avX4bnbt2qUaKdJAkSBwGTq/E8koIsfl3q5cuZLf0yAiC7J87wX8cvgy7GxKYc6zLVC+DOMqtDJy5Ei1cGp+SB0h6x4RERWX05H/BWs/36EWnvSrpnWRrEK+RyyMsRUDBw5Um4iJiUFiYiIqVqyohrkLKj4+XgWBSyYpmXubX5JdxMXFJeu5h4dHgT+biMzbkfAYvLvpuHo8sWdDtKxRQesiWTV3d3c0adIEHTt2vOOotqx/JPsXL16sdZGJyELFJmUGa99MTkPb2m54q1cjrYtkNQrUsMhNpkUVZqVtmYObex5ufkhDQhblIyLrrTT8Ja4iPQPdGnti6H21tS6S1Xv33XcxevRofPXVV5g/f75qSGRXrlw5dO3aVTUoJGaPiKi4grXHrz6oUo9XlmDt51rCnsHa+mxYfPHFFyb3S+OiQYMGKr1gSfD19UVycrLKNPL222+rHrK8yHGyGcnKr0Rk5nEVPx5G6LXMuIpPnvJRUyJJe56ennjrrbfUJqMUoaGhalRbYi7q1q3LvxMRFbsv/jiF349nBmsvHOiHSgzW1m/DYtasWSb337hxQ02LktVWf/rpJ7i5FU+qx8qVK2PhwoVqiF0aC9IzJqkLZbE+Wd3VlBkzZmD69OnFUh4iKnnL9l7A5iNXYG9bSvVEuZYp+DRMKn4VKlRQGxFRSfn9WARm/54ZrP1+36bwqc7ZLSWtlEG6/4qABHZL7IWMJsgweIELUqoU1q9fj759+xbodZJhRALFly1blu8Ri+rVq6uGUPY4DSIyj/Uqnpi/W02BmtKrEYYxtWyRkXujjD4X5t4oHUt3GtWWzqGCkAQfkmFQFl69fPnyXesISfDRpUuX2/bLa728vErsOhBRyTsTdRN95/6DuOQ0DG5fE+/0aap1kSxGQe6LhYqxyK5OnTpqtVUJxC5Jbdq0UQGBdwo4v9OCfkRkfnEVXRsxrkKP7vSlXzqPnnnmGZVBsEyZMvl6Pyb4IKL8rmc0fGmAalS0qeWG/z3aWOsiWa0ia1gIGTko6dSvBw8eLHAvGBGZFxlYnfTjEVy4tV7FJ/2ac76+DmVkZJjcL71cMurg7++P9957Dx988EG+3o8JPogoP8Haklb2TFQ8vFwYrK01m6JebbVmzZr5Pv7mzZuqYSCbOHfunHosAX9i0qRJGDx4cNbxs2fPxsaNG3H69GkEBwdj3Lhx+OOPP1RlRUSWa/m/ofjlSOZ6FXO5XoXZkSH0Bx98UMXprVu3rtg/T6bkSodTt27d8M8//xT75xGRdubtOI2txyLgYGuDBQNbwr0cZ6mYzYhFXhmVjL1Rr732GoYMGZLv9wsICMgxH3b8+PHqp7zHkiVL1LxYYyNDpKSkqM+4ePGiGkpv3rw5fv/9d5NzaonIcuIq3v05M3Xpmz0aogXXqzBbDRs2zPcieiWV4IOZA4nM1x8nIvDZ7yfV4/f6NmX9YG7B2zY2NnlOP5D9w4YNUylpHRz025vIwDwi85o323vO3zh/NQEPNfTAV0NacQqUGd8bZYRZVug+eTLzi4AeEnxIynJTmQNZRxDp27noeDw292/EJaVhYLsaeK9vM62LZLGKLXh7x44dJvfLh9SvX1+tsBoZGalWVSUiKgzp85i8Plg1Kqq4OuGTflyvwpzJNNfXX38dvXr10lWCD5lyaxwtz545kIj0S1bUVsHaSWloVbMCpj7aROsi0b00LKTn504OHTqkhpvT09ML8rZERLf5fl8Yfj50CbY2pTDn2Rao4KzfkVDKJOtWmGr8SXantLQ0FfMgIwR6SvDBzIFE5tfp9PqaQzgVeROeLo6YP7ClWgyPLDArFBFRUTh+ORbTfz6qHk/o7g2/msWz6CYVLUmwkdeotre3Nxo3LlgKSEnwIck6jIwJPmQRVpneJKMNEnO3dOnSrM+vXbs2mjRpgqSkJBVjIdOvtm7dWsgzIyK9mL/zDLYczVwkdcFAP3iUc9K6SJQNGxZEpCvxyWlqvYrktAx09nbHcC6CZzbulrzj8OHDKrBaEnHkBxN8EFF2O0Ii8cnWEPVYFsBryWBt3WHDgoh0NcQ9ZUMwzt7KR/7Z076wsWFchSX9fQsyVVYyOt0pv4g0LrJ744031EZElud8dDzGfn8Ackt4tm0NDGhTQ+siUWEbFtLbdLfVTomI7tXagHCsP3BRxVV8MaAF3BhXQURk9WQke8SyQMQmpaFljfKY1psra1tEw0IWHZLAPFM9SMb9zNpCRPfiZEQcpv4UrB6P79YAbWozroKIyNrJd8sJPxxCSEScWvxO4ioc7Wy1LhYVRcNCAueIiIpaQkoa/FcEISk1A/fXr4RRnepqXSS6B3dbXC4uLq7EykJElmHhn2ex+citYO3nWsLThcHaFtOwqFmzZvGVhIis1rSNR1XqQI9yjpjVn3EV5qp8+fJ3HLXmqDYRFcSfJ6Pw8W8n1OO3H2uCVrU4km1RDYuPP/4YY8aMQenSpdXzf/75R2X4MOYAl96oN998E/Pnzy+e0hKRxfkxMBxrA8MhbYnPn2mBSmW5poC5ymsRVSKigrpwNR6v3ArWfqZ1dTzLYG2zUMpwp5Qbudja2qr0fh4eHlm5ySWneJ06mekgIyIi1Krbel4gryDLkhNR8TodGYfec/5BYmq6iqt45aH6WhfJavHemInXgUgf02OfmL8bJ67Ewbd6eawe0Y5xFWZyXyzQUoW52yAFaJMQEeWQmJIO/xUHVKOiY72K8O9ST+siUSGtWbMmxxoV4eHhyMjIyHqekJCgRr6JiPIi3y3f+OGwalRUKuuABQNbslFhRrgGOhFp4u2fjqosHzL1aXb/FirFLJm3AQMG4MaNG1nPZaXt8+fPZz2X6bKyWjYRUV6+/OssNh2+DDubUpj/nB8qu2ZOvyfzwIYFEZW4dUHhWB0QBonj/eIZX5VCkMwfR7WJqDD+PhWND3/NDNaWtSqYdtwKVt7+6quvULZsWfU4LS1NrXxaqVIl9ZypBIkoP3EVb63PXK9i7EP10aFe5v2DiIisV9i1BIz+PggZBqCfXzUMbMdMpBbfsKhRowa+/PLLrOdeXl5YtmzZbccQEd0trqJD3YoY8yCDtYmIrJ3UDcOXBeJGQip8qrni3b5NmZraGhoW2efKEhEV1LSfgv+Lq3jGl3EVFui3335T2UOEBG5v374dwcGZI1TZ4y+IiIxTJieuO4zjl2NvBWv7wcmewdpW0bBISkrC77//jkcffVQ9lyC85OTk/97Mzg7vvPMOnJy4KiIR3b5exZqAzPUqJK7CoxzvE5ZoyJAhOZ6PGDFCs7IQkf59/fc5bDx4SQVrz3u2JaqUZ7C21QRvSzzFokWLsp7PnTsXu3fvxoEDB9Qm06IKsjjerl270Lt3b7X2hQx5bdiw4a6v2blzJ1q2bKkW5atXr54qExHp26mIOEzZYIyraMC4CgslIxR3227evKl1MYlIJ3afjsYHm4+rx1N6NULbOhW1LhKVZMNixYoVGD58eI59K1euVKutyjZz5kysXbs23+8XHx8PHx8fzJs3L1/Hnzt3Dr169UKXLl3Uwnzjxo3DsGHD1NA7Eel3oaOXVwSpuIr76lXC6Ae5XoU1ktHtzz77LGtBVSKybhKs7b8yM1j7yZbVMKRDLa2LRCU9Fer06dNo1qxZ1nOZ8mRj81/bpE2bNvD398/3+/Xs2VNt+bVw4ULUrl0bn376qXreqFEj/P3335g1axa6d++e7/chopKbOysjFacib6qUsrP6M67C0hsPb7/9NrZt2wYHBwe88cYb6Nu3L7755htMmTIFtra2ePXVV7UuJhHpIFh7xLJAXE9IRbOqrnj/cQZrW2XDQgLvssdUREVF5fi9DHNn/31R27NnD7p27ZpjnzQoZOSCiPRnbUA41gVdVHEVcwa04HoVFm7q1Klquqzcp2WabL9+/fDCCy9g7969arRCnkvjgoisu8Np0rrDOHY5FhWdHbBwEIO1rbZhUa1aNZXdw9vb2+TvDx8+rI4pLleuXIGnp2eOffI8NjYWiYmJKF369oAfaehkb+zIsURU/CTDx/82ZsZVvPawN9px7qzFk6mwS5cuxWOPPabqiubNm6v1jg4dOsTeSCJSvvnnPDYcvKRGr+c+2xJVGaxtvTEWjzzyiOqRkuxQuckX++nTp6sYCD2ZMWOGSn1o3KpXr651kYgs3s3kNPivCEJyWgY6e7tjVKe6WheJSkB4eDj8/PzU46ZNm6okGzL1iY0KIhK7z/wXrP3WI43Qvi47nKx6xGLy5MlYs2aNGrEYPXo0GjRooPaHhISoDFHSMyXHFBdZkC8iIiLHPnnu4uJicrTCmBJ3/PjxOUYs2LggKt5h7jd/PIyz0fGo7OqEWU/7woZxFVYhPT1dxVZkT0FetmxZTctERPpw8UYiRq88gPQMAx5vURUvdGSwNqy9YSHTjmTe7KhRozBx4kT1BUJIb1S3bt1UqtncU5WKUvv27bF58+Yc+yRIUPbnRXrMZCOikvHd7vP45fDlzJzkz7VEBef/vmiSZZM64fnnn8+658ro9siRI+Hs7JzjuHXr1uU7JblkGwwMDMTly5exfv16FQx+t5Tk0pl09OhR1YkkQeNSJiLSTlKqBGsH4Fp8CppUccGMJ5pxJNNCFahhISQr05YtW3Dt2jWVJUrIehJubm4F/nDJZ258D2M6WUkjK+9Vo0YNNdpw8eJFNWdXSAUlIyOSaeTFF1/EH3/8oUZQfvnllwJ/NhEVvaDQ63j/1jD35EcaoWWNCloXiTRcHG/gwIGFej9jSnK53z/xxBP5TkkudYWkR5dVvyUleeXKlZk5kEjDDofJ648g+GIsKpSxxyIGa1u0AjcsjOTLv6SXLYyAgAC1JoWRccqSVE6y8J30UIWGhuZo1EgjQubsfv755ypQ/KuvvmKFQaQD0hM1ekUQUtMNeKSZF4e5rdC3335bpO/HlOREljGKbcwOKCtrV6tQRusikR4bFkWhc+fOWdOpTDG1qra8Rlb5JiL9kDmzY1cdwKWYJNSu5IyPnmzOYW4qcfeSkpyZA4mKz79nr+LdX/4bxe5Qr5LWRSI9ZYUiIjLli+2n8NepaDjZ22DBwJYo52SvdZHICt0tJbkpzBxIVDwu3UjEyyuCVMdTH98qGHpfba2LRCWADQsiKpSdIZH44o9T6rEE5DX0ctG6SET5JrF8MTExWVtYWJjWRSKyiGDtkcsDcTU+BY0qu+DDJziKbS00nQpFROYt/HoCxq0+CJnROLBdDTzeovgWyCQqjpTkzBxIVLRkivuUDcE4HB6D8mXssXiQH0o7MFjbWnDEgojuuUdq1PIg3EhIhU81V/zv0cZaF4msnKQel0xQBUlJTkRFa9neC/ghMFwFa88d0BLV3RisbU3YsCCie+qRmroxGEcuxsDN2QHzB/rB0Y49UlS0JCW5pCCXLXtKcmO2QJnGNHjw4KzjJc3s2bNnVUryEydOqLWVJCW5ZBIkouK379w1vPPzMfV4Ys+GuK8+g7WtDRsWRFRgq/aHYU1AZo/UnAEtULW86WkmRIVNSd6iRQu1GVOSy+OpU6eq53mlJJdRCln/QtLOMiU5Ucm4HCPB2oFIyzCgt08VvHR/Ha2LRBpgjAURFciB0OuYtvGoevx6d290ZPpAKiZMSU5kHpLTJFg7CNE3U9DQqxw+epIra1srjlgQUb5FxiWpuIqU9Ax0b+KJUZ3qal0kIiLSkDT+pbPpUNgNuJaWYO1WKOPAfmtrxYYFEeVLSloG/FcE4UpsEuq6O+OTfj7skSIisnIr/g1V02ONU2NrVGSwtjVjw4KI8uX9X45h//nrKOtoh8WDW3ERPCIiKxdw/hqm/5w5NfaNHg3xQAN3rYtEGmPDgojuak1AGL7bc0E9ntXfF3Xdy2pdJCIi0lBEbBJGrQhCaroBvZpVxogHGKxNbFgQ0V0EhV7HlPXB6vHYh+qjW2NPrYtERESaB2sHIiouGd6e5fDxU1xZmzKxYUFEd+yRGrksUAVrP9zYUzUsiIjIur390zEcCL0BFyc7LBrkB2dHBmtTJjYsiCjPlbWHLwtEZFwyGniWxWf9fWEj0XlERGS1Vv4biu/3hUIGKL4Y0AK1KjlrXSTSETYsiMhk+sBJ645kpQ/8cnArFbRNRETWK/DCdUz7KXNq7OsPe6Ozt4fWRSKdYcOCiG6z4M8zWH/gImxtSmH+cy1RsyJ7pIiIrFmkBGsvD1TB2o8088LLnbmOEd2ODQsiymHr0SuY+VuIevx278ZcWZuIyMrJOkaSAco4NXbmU1zHiExjw4KIshy7FItxqw/CYAAGtquBQe1raV0kIiLS2DubjqppUBKsLStrM1ib8sKGBRFlZYAa+t1+JKSko0PdipjWu4nWRSIiIo2t2R+G5Xszg7U/f4bB2mQGDYt58+ahVq1acHJyQtu2bbFv3748j12yZIkafsu+yeuI6N4lpKRh2HcBuByThLruzljwnB/sbXVxeyAiIo0ckHWMNmQGa7/WrQG6NGSwNt2Z5t8cVq9ejfHjx2PatGkICgqCj48PunfvjsjIyDxf4+LigsuXL2dtFy5krghMRAWXkWHAq6sP4sjFGLg5O+Cb51vDtYy91sUiIiINRcZJsHaQWseoRxMv+Hepp3WRyAxo3rD47LPP8NJLL+GFF15A48aNsXDhQpQpUwbffPNNnq+RUQovL6+szdOTKwET3av3Nx/Hb0cj4GBrg8WD/JgBiojIykmwtv+KIFyJTUI9j7L45GkGa5MZNCxSUlIQGBiIrl27/lcgGxv1fM+ePXm+7ubNm6hZsyaqV6+OPn364OjRo3kem5ycjNjY2BwbEWX65u9z+Prvc+rxzH7N0aqWm9ZFIiIijb33yzHsP38d5RwlWNuP6xiReTQsoqOjkZ6eftuIgzy/cuWKydd4e3ur0YyNGzdi+fLlyMjIQIcOHRAeHm7y+BkzZsDV1TVrk8YIEQFbgi/j3V+OqccTezZEH9+qWheJiIg0tiYgDEv3ZE4xn9XfF3Xcy2pdJDIjmk+FKqj27dtj8ODB8PX1RadOnbBu3Tq4u7tj0aJFJo+fNGkSYmJisrawsLASLzOR3uw/fw1jV/2XVnbEA3W0LhIREWnsYNgNTFmfGaz9atcG6NqYU82pYDQd26pUqRJsbW0RERGRY788l9iJ/LC3t0eLFi1w+vRpk793dHRUGxFlCrkSh6FL9iM5LQNdG3ng7d5NOHeWiMjKRcUlY+SyQBWs3a2xJ8Y8yGBtMrMRCwcHB/j5+WH79u1Z+2RqkzyXkYn8kKlUR44cQeXKlYuxpESWIfx6AgZ/8y9ik9LgV7MC5gxoCTumlSUismqp6RnwX5kZrF3H3RmfPe0DGxt2OFHBaf6NQlLNfvnll/juu+9w/PhxjBo1CvHx8SpLlJBpTzKdyeidd97B1q1bcfbsWZWeduDAgSrd7LBhwzQ8CyL9u3ozGYO/2YeI2GTU9yiLr4e0QmkHW62LRXRHXOeIqPi9/8tx7Dt3TQVpy8ra5ZyYcpzujeZh/v3790dUVBSmTp2qArYldmLLli1ZAd2hoaEqU5TR9evXVXpaObZChQpqxGP37t0qVS0RmRablKoaFWej4lHF1QlLh7ZB+TIOWheLKF/rHEkacmlUzJ49W61zFBISAg8PjzzXOZLfG3GaH9Gd/RgYjiW7z2cFa0t6WaJ7VcpgkPBN6yHpZiU7lARySwVEZOkSU9LV9CdJHVjR2QFrRrZHXWb5IDO4N0pjonXr1pg7d27WVFnJ7DdmzBhMnDjR5IjFuHHjcOPGDYu6DkTF5Uh4DJ5cuFutWzH2ofp4tVsDrYtEOlSQ+6LmU6GIqPgkp6VjxPLAzHzkTnZqpIKNCjIHJbHOkeBaR2TN02NHLAtQjQpJ5CENC6LCYsOCyEJJZfHy8iDsOhmF0va2WPJCazSp4qp1sYh0s86R4FpHZM3B2pdiklCnkjM+6+/LYG0qEmxYEFlopTF6ZRC2n4iEo52NCtT2q8lVtcmyFXSdI8G1jsgazdh8AnvPXoOzgy0WD/aDC4O1yVKCt4mo6BsVY1cdwNZjEXCws8GXg1uhQ71KWheLSHfrHAmudUTWZv2BcHzzzzn1+NOnJVi7nNZFIgvCEQsiC5v+JCMVm49cgYOtDRYN8sMDDdy1LhZRgXGdI6KiF3wxBhN/PKIej+5SDz2a5q+RTpRfHLEgshBJqel4eUUQ/jgRqUYqFg5siS7eplNyEpkDSTU7ZMgQtGrVCm3atFHpZnOvc1S1alUVJ2Fc56hdu3aoV6+eygw1c+ZMrnNEdMu1+BSMWBaI5LQMdPF2ZwYoKhZsWBBZgISUNFVh/HUqGk72NmqBI45UkLnjOkdERSPtVtzdxRuJqFWxDGY/0wK2DNamYsB1LIjM3I2EFLy4ZD+CQm+gjIMtvh7SGu3rVtS6WGRmeG/MxOtAluj9X47hy7/OqWDt9f4d0cCTcRVUPPdFjlgQmbGI2CQM/nofQiLi4OJkh29faM3sT0RElGXjwYuqUSE+6efDRgUVKzYsiMzUmaibeP7bfQi7lgiPco5YNrQtvL1YYRARUaajl2Lw5o+H1WP/LnXRsxkTGVDxYsOCyAztP38NLy0NwI2EVNSsWAbLh7ZFdbcyWheLiIh04vqtYO2k1Ax09nbH+G7eWheJrAAbFkRmZtPhSxi/5pBKLetbvTy+GtIKlcoyDz8REf0XrD3m+wMIv56oOp8+789gbSoZbFgQmYmMDAM+335KbaJ7E0/M7t8CpR1stS4aERHpyMe/heDv09EqoYesZ+RahitrU8lgw4LIDMQnp+G1NYew5egV9fzFjrXxVq9G7IEiIqIcfjp0CYt3nVWPZz7lg4ZezG5GJYcNCyKdOx8dj5HLA3HiShzsbUvh/b7N8HTr6loXi4iIdOb45Vi88cMh9Xhkp7ro1ZzB2lSy2LAg0rEtwZcxYe1hxCWnqTiKRYNaMp0sERGZDNYevixABWvfX78SJnRnsDaVPDYsiHQoOS0dH28Jwdd/Z+Yeb12rAuYMaAkvVyeti0ZERDqTnmHAK6sOqPTj1d1KY84ABmuTNtiwINKZ05FxeOX7gzh2OVY9H/5AHdXzZG9ro3XRiIhIh2b+FoK/TkWjtL0tFg9qhfJlHLQuElkpXXxTmTdvHmrVqgUnJye0bdsW+/btu+Pxa9euRcOGDdXxzZo1w+bNm0usrETFmfVp6Z7z6PXF36pRUaGMPRYP8sPkRxqxUUFERHmmIF/45xn1+KOnmqNRZQZrk3Y0/7ayevVqjB8/HtOmTUNQUBB8fHzQvXt3REZGmjx+9+7dGDBgAIYOHYoDBw6gb9++agsODi7xshMVZYD2gC/3YurGo0hOy5wf+9u4B/BwEy+ti0ZERDp14kqsisMzjm4/5lNF6yKRlStlMBgMWhZARihat26NuXPnqucZGRmoXr06xowZg4kTJ952fP/+/REfH49NmzZl7WvXrh18fX2xcOHCu35ebGwsXF1dERMTAxcXtupJW6npGfj2n3P4bNtJFXAnw9hv9PDGkPa1YMP5sVSCeG/MxOtA5uJGQgoem/sPQq8l4L56lbDkhdaw4+g2aXxf1DTGIiUlBYGBgZg0aVLWPhsbG3Tt2hV79uwx+RrZLyMc2ckIx4YNG0wen5ycrLbsF+derT8QjvQMwM6mlAqKMv6UaSrGn5IONPOnDRzsMh872NnAwdYGjva2cLSzUa8rVYpfGq3d7tPRmPbTUZyKvKmed6xXETMeb44aFctoXTQiItJ5sPbYVQdVo6JahcxgbTYqSA80bVhER0cjPT0dnp6eOfbL8xMnTph8zZUrV0weL/tNmTFjBqZPn14k5ZVpKnFJaYV+H+mIdrrVyJCf0kstqyfLCpmlHezgrB7bwdnRFmUd5acdyjnd2hzt4VJaNju4lrZXm7yeDRXzmvYkgXa/HLmsnrs5O2Bij4bo16oa/45ERHRXn24NwZ8no+Bkb6NW1q7gzGBt0geLzwoloyHZRzhkxEKmWt2LB+q742ZymuopkC0tIwNptx6nphuQlp6hprbI48yfGUhJy0DKrX1GGQYgISVdbUBqoc9RRkTKl7ZHhTIOqOBsj4rOjurLasWysjnCvawD3MvJTyd4uDiqxgyVvKi4ZHyx/RS+3xeq/r+RBuagdjUxvps3XMvYa108IiIyA5uPXMb8nbeCtZ9sjiZVXLUuEpE+GhaVKlWCra0tIiIicuyX515epoNWZX9Bjnd0dFRbUZj3XMtCZfyRBkZyaoZao0Dm0yepn+lIlEZGajqSUtIRnyLP09TP+OQ01ZCRnzJSkrmlqp8xialqky+o0niJjEtWW364ONmp9RA8XZxQ2dUJXq6lUcXVCVXKl1Zb1fKl1QgKFY3LMYn4ctc51aBITJXGJNCpgTve7NEQjatwDjcREeXPyYg4vL42c2XtYffVRh/fqloXiUg/DQsHBwf4+flh+/btKrOTMXhbno8ePdrka9q3b69+P27cuKx927ZtU/v1TAJxnWxsb40WFE3vtMTdy6jH9YQU3EhIVT+vxf+3Rd+ULRlXbyYj6mYyImOTVcah2KQ0xCbdxMmIzLn9plQq64CqFcqouZs13MqgeoUy6mfNimVU44ML79zd8cuxWPLPeaw7EJ41YuVbvbxqULSvW1Hr4hERkRmRzsThSwNUvd+hbkVM7NlQ6yIR6W8qlExTGjJkCFq1aoU2bdpg9uzZKuvTCy+8oH4/ePBgVK1aVcVKiLFjx6JTp0749NNP0atXL6xatQoBAQFYvHgxrI3Mx5f4C9mqVchfQyQuOQ2RsUm4EpOMK+pnIi7FyM8kXLqRiIvXE9UxmY2SFBwKu3Hb+0iAujQ0pJFRq5IzamfbqriWtupsRjICte1YBJbtvYB9565l7W9b2w2jH6ynMncwjoKIiAo66+HV1Qdx/mqCmlUw99mWDNYmXdK8YSHpY6OiojB16lQVgC1pY7ds2ZIVoB0aGqoyRRl16NABK1euxJQpUzB58mTUr19fZYRq2rSphmdhHuQLrYuTvdrqeZS7Y69I+PUEhF1LvPUzAReuJajsE+HXEtWUrrPR8WpDSNRt8R61KzqjjvutrVJZ1PUoqx7L51oiibEJCr2O9QcuYtOhS2pESMioTo8mXnjxvlrwq+mmdTGJiMhMzfr9JP44EamSvkiwtsRREumR5utYlDTmKC/8l2gZ6bgQHY9zV+NVhqNz0Qk4F31TNTyyB6nnVqmso2pg1L3V4JARDnle3a2M2a0sLXEv/567qkYnth2LVFPOjCRu5Sm/aniubU0Vy0JkDvR6b5w3bx5mzpypOp5kAdU5c+ao0e28rF27Fv/73/9w/vx51fH00Ucf4ZFHHjH760DWa0vwFYxcHqgez+rvg8dbVNO6SGRlYs1lHQsyP9ILL8OwsnWoVynH7yQr1sUbiTgbFY8zUTczRzXkZ1S8CiyXL9+yZZ8iZHzP6hVKq2lVtSo6qylWstVwc1YxHnrIYiUxKwfDruNA6A3sPXtV/ZTAeSNJBdytsSeealkN7epUtOrpYERFZfXq1Wq6rCx+KoupylRZWbcoJCQEHh4etx2/e/duDBgwQE2dffTRR9XotsTvBQUFcVSbzNKxS7F4bc1B9fjFjrXZqCDd44gFlQjJZnVONTRuNTZuPZZ9xkxJefEo54iqFTIbMxI47uUimayc1H4ZBalUzlGt/VHY2AVJDyyxJmHXExB+PVE1jk5FSJB7nHqemwSzP9CgEro38ULb2hXVNDAic6XHe6M0Jlq3bo25c+dmJfeQdOFjxozBxIkTTU6tlRi9TZs2Ze1r166dmmIrjRNzvQ5kfSR75IKdZzB/xxk1/bhdHTcsH9qWcRWkCY5YkO6Uc7JH82rl1ZadtGsjYpNxNvomLlxNwPlb06tCryUi9Gq8SrtrTKUrowR5kS/1sligrOchoweywKAsOCirnRtXSDcGwKUbDCrIWjJryJSmG4mpuHozRcWW3IlM4fKtXgGtalVAx7qVuEI2UTFKSUlBYGCgWovISOLtunbtij179ph8jezPvm6RkBEOicPLS3JystqyV6D34q9TUdh0KHPRS6LC2n/hmup8M6Ynn9Xfl40KMgtsWJCmZJRBRh9k61AXtzU6ZArSxVvZquTnpRtJiJBsVrFJiIxLQnRcihrxkLU8ZAE62QpDGijVZKqXTM2q6IwGnmVR37McGnm5cBE7ohIUHR2N9PT0rEQeRvL8xIkTJl8jcRimjpf9eZFpU9OnTy90eSV99+qAsEK/D5GRjMhP690YjzavzGyCZDbYsCDdkhuprBwuW+6Rjuxk1EHW8FCLBiakqnS5suhgfEqaanAYV0YX0uFjU6qUittwdrRVIxuSrcq9nINasVxGPRgfQWQ9ZEQk+yiHjFjIdKuCalWzAiZ09y7i0pG1Kutoh76+VdmhRWaHDQsyewVZy4OIzEOlSpVga2uLiIiIHPvluZeXl8nXyP6CHC8cHR3VVlg+1curjYjImnHCHhER6Y6DgwP8/Pywffv2rH0SvC3P27dvb/I1sj/78WLbtm15Hk9EREWLIxZERKRLMkVpyJAhaNWqlVq7QtLNStanF154Qf1+8ODBqFq1qoqTEGPHjkWnTp3w6aefolevXli1ahUCAgKwePFijc+EiMg6sGFBRES6JOljo6KiMHXqVBWALWljt2zZkhWgHRoaqjJFGXXo0EGtXTFlyhRMnjxZLZAnGaG4hgURUcngOhZERMR74y28DkRE935fZIwFEREREREVGhsWRERERERUaFYXY2Gc+XWvq6sSEVki4z3RymbH3oZ1BBHRvdcPVtewiIuLUz/vZQEkIiJruEfKXFprxTqCiOje6werC96WPOiXLl1CuXLl1MrOBWFckTUsLMysg/os4Tx4DvphCedhCedQ2POQqkAqjSpVquTItGRtrL2O4DnohyWchyWcg6WcR2wJ1Q9WN2IhF6RatWqFeg/5g5jr/1iWdh48B/2whPOwhHMozHlY80iFEeuITDwH/bCE87CEc7CU83Ap5vrBeruliIiIiIioyLBhQUREREREhcaGRQE4Ojpi2rRp6qc5s4Tz4DnohyWchyWcgyWdh7myhOvPc9APSzgPSzgHSzkPxxI6B6sL3iYiIiIioqLHEQsiIiIiIio0NiyIiIiIiKjQ2LAgIiIiIqJCY8PiHj322GOoUaMGnJycULlyZQwaNEgtqmROzp8/j6FDh6J27dooXbo06tatqwJ7UlJSYE7ef/99dOjQAWXKlEH58uVhLubNm4datWqp/4fatm2Lffv2wZzs2rULvXv3VgvmyEJiGzZsgLmZMWMGWrdurRZD8/DwQN++fRESEgJzsmDBAjRv3jwrN3n79u3x66+/al0sq2fudYSl1A/mWkewftCeJdQPWtQRbFjcoy5dumDNmjXqf7Iff/wRZ86cwVNPPQVzcuLECbXK7KJFi3D06FHMmjULCxcuxOTJk2FOpKLr168fRo0aBXOxevVqjB8/XlXUQUFB8PHxQffu3REZGQlzER8fr8otFaC5+vPPP+Hv74+9e/di27ZtSE1NxcMPP6zOzVzIYm4ffvghAgMDERAQgAcffBB9+vRR/6ZJO+ZeR1hK/WCOdQTrB32whPpBkzpCskJR4W3cuNFQqlQpQ0pKisGcffzxx4batWsbzNG3335rcHV1NZiDNm3aGPz9/bOep6enG6pUqWKYMWOGwRzJrWT9+vUGcxcZGanO5c8//zSYswoVKhi++uorrYtBFlZHmHP9YE51BOsHfbKU+qG46wiOWBSBa9euYcWKFWqo1d7eHuYsJiYGbm5uWhfDoknvmfQcdO3aNWufjY2Ner5nzx5Ny2bt5P9/Ya7/BtLT07Fq1SrVoybD3aQPllJHsH4ofqwf9Mvc64eSqiPYsCiEN998E87OzqhYsSJCQ0OxceNGmLPTp09jzpw5GDFihNZFsWjR0dHqH7enp2eO/fL8ypUrmpXL2sm0j3HjxqFjx45o2rQpzMmRI0dQtmxZtfDRyJEjsX79ejRu3FjrYlk9S6ojWD+UDNYP+mTO9UNJ1xFsWGQzceJEFWR0p03mnRpNmDABBw4cwNatW2Fra4vBgwfL1DKY23mIixcvokePHmoe6ksvvQRzPAeiwpC5tMHBwao3x9x4e3vj4MGD+Pfff9U88iFDhuDYsWNaF8viWEIdYQn1g2AdQSXJnOuHkq4juPJ2NlFRUbh69eodj6lTpw4cHBxu2x8eHo7q1atj9+7dmk9BKOh5SKaSzp07o127dliyZIkadjXHv4WUXXoUbty4Ab0PdUt2kh9++EFlmTCSf+hSdnPs1ZRKXHpAsp+PORk9erS67pLJRLLgmDuZNiFZfCTwloqOJdQRllA/WHIdwfpBfyytfijuOsKuyN/RjLm7u6vtXofJRHJyMszpPKQnSrKX+Pn54dtvv9VNpVGYv4XeSUUn13v79u1ZN1r5/0eeyw2MSo70q4wZM0ZVejt37rSYSkP+f9LDvcjSWEIdYQn1gyXXEawf9MNS64firiPYsLgHMpS0f/9+3HfffahQoYJKI/i///1Ptf60Hq0oCKk0pCeqZs2a+OSTT1QPkJGXlxfMhcxdluBI+SlzU2W4T9SrV0/NKdQjSSUoPVCtWrVCmzZtMHv2bBVM9cILL8Bc3Lx5U827Njp37py69hLYJvn7zWV4e+XKlao3SnKVG+cwu7q6qtz95mDSpEno2bOnuuZxcXHqfKQS/O2337QumtWyhDrCUuoHc6wjWD/ogyXUD5rUEcWSa8rCHT582NClSxeDm5ubwdHR0VCrVi3DyJEjDeHh4QZzS70n/wuY2szJkCFDTJ7Djh07DHo2Z84cQ40aNQwODg4qveDevXsN5kSur6nrLn8Pc5HX///yb8NcvPjii4aaNWuq/4/c3d0NDz30kGHr1q1aF8uqWUIdYSn1g7nWEawftGcJ9YMWdQRjLIiIiIiIqND0M2GSiIiIiIjMFhsWRERERERUaGxYEBERERFRobFhQUREREREhcaGBRERERERFRobFkREREREVGhsWBARERERUaGxYUFERERERIXGhgURERERERUaGxZERERERFRobFgQEREREVGhsWFBVMKioqLg5eWFDz74IGvf7t274eDggO3bt2taNiIi0g7rBzJ3pQwGg0HrQhBZm82bN6Nv376qwvD29oavry/69OmDzz77TOuiERGRhlg/kDljw4JII/7+/vj999/RqlUrHDlyBPv374ejo6PWxSIiIo2xfiBzxYYFkUYSExPRtGlThIWFITAwEM2aNdO6SEREpAOsH8hcMcaCSCNnzpzBpUuXkJGRgfPnz2tdHCIi0gnWD2SuOGJBpIGUlBS0adNGzZ2VObSzZ89Ww90eHh5aF42IiDTE+oHMGRsWRBqYMGECfvjhBxw6dAhly5ZFp06d4Orqik2bNmldNCIi0hDrBzJnnApFVMJ27typeqCWLVsGFxcX2NjYqMd//fUXFixYoHXxiIhII6wfyNxxxIKIiIiIiAqNIxZERERERFRobFgQEREREVGhsWFBRERERESFxoYFEREREREVGhsWRERERERUaGxYEBERERFRobFhQUREREREhcaGBRERERERFRobFkREREREVGhsWBARERERUaGxYUFERERERIXGhgUREREREaGw/g/9IuZUXGMWHgAAAABJRU5ErkJggg==",
                  "text/plain": [
                     "<Figure size 800x300 with 2 Axes>"
                  ]
               },
               "metadata": {},
               "output_type": "display_data"
            }
         ],
         "source": [
            "# GELU compared to RELU\n",
            "import matplotlib.pyplot as plt\n",
            "\n",
            "gelu, relu = GELU(), torch.nn.ReLU()\n",
            "\n",
            "x = torch.linspace(-3, 3, 100)\n",
            "\n",
            "gelu_out = gelu(x)\n",
            "relu_out = relu(x)\n",
            "\n",
            "plt.figure(figsize=(8, 3))\n",
            "for i, (y, label) in enumerate(zip([gelu_out, relu_out], [\"GELU\",\"RELU\"]), 1):\n",
            "    plt.subplot(1, 2, i)\n",
            "    plt.plot(x, y)\n",
            "    plt.title(f\"{label} activation function\")\n",
            "    plt.xlabel(\"x\")\n",
            "    plt.ylabel(f\"{label}(x)\")\n",
            "plt.tight_layout()\n",
            "plt.show()\n",
            "\n",
            "# GELU is a smooth, non-linear function that is differentiable at almost any negative value (except ~ -0.75). \n",
            "# Better optimisation due to:\n",
            "# 1. smoothness\n",
            "# 2. differential for negative values so these can contribute to the gradients/optimisation process"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 68,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "{'vocab_size': 50257, 'context_length': 1024, 'emb_dim': 768, 'n_heads': 12, 'n_layers': 12, 'drop_rate': 0.1, 'qkv_bias': False}\n"
               ]
            }
         ],
         "source": [
            "# Feedforward with GELU\n",
            "# Feedforward layers enable richer representations through expansion to higher dimensions \n",
            "\n",
            "class FeedForward(torch.nn.Module):\n",
            "    def __init__(self, cfg):\n",
            "        super().__init__()\n",
            "        self.layers = torch.nn.Sequential(torch.nn.Linear(cfg['emb_dim'], 4*cfg['emb_dim']), GELU(), torch.nn.Linear(4 * cfg['emb_dim'], cfg['emb_dim']))\n",
            "    \n",
            "    def forward(self, x):\n",
            "        out = self.layers(x)\n",
            "        return out\n",
            "\n",
            "print(GPT_CONFIG_124M)\n",
            "\n",
            "ffn = FeedForward(GPT_CONFIG_124M)\n",
            "x = torch.randn((2, 3, 768))\n",
            "out = ffn(x)\n",
            "\n",
            "\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "no skip\n",
                  "layers.0.0.weight has gradient mean of 0.00020173587836325169\n",
                  "layers.1.0.weight has gradient mean of 0.0001201116101583466\n",
                  "layers.2.0.weight has gradient mean of 0.0007152041071094573\n",
                  "layers.3.0.weight has gradient mean of 0.0013988735154271126\n",
                  "layers.4.0.weight has gradient mean of 0.005049645435065031\n",
                  "skip\n",
                  "layers.0.0.weight has gradient mean of 0.0014432291500270367\n",
                  "layers.1.0.weight has gradient mean of 0.004846951924264431\n",
                  "layers.2.0.weight has gradient mean of 0.004138893447816372\n",
                  "layers.3.0.weight has gradient mean of 0.005915115587413311\n",
                  "layers.4.0.weight has gradient mean of 0.032659437507390976\n"
               ]
            }
         ],
         "source": [
            "# Skip connections\n",
            "# Create an alternative path for the gradient flow by adding the output from layer to the output of a later layer \n",
            "# First applied in \"residual\" networks in computer vision\n",
            "# Help with optimisation process as the addition of the input (e.g x) to the output from a layer increase the magnitude of the values\n",
            "# Thus helps with the vanishing gradient problem\n",
            "\n",
            "class DeepNeuralNetwork(torch.nn.Module):\n",
            "    def __init__(self, layer_sizes, use_skip):\n",
            "        super().__init__()\n",
            "        self.use_skip = use_skip\n",
            "        self.layers = torch.nn.ModuleList([torch.nn.Sequential(torch.nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()), torch.nn.Sequential(torch.nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()), torch.nn.Sequential(torch.nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()), torch.nn.Sequential(torch.nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()), torch.nn.Sequential(torch.nn.Linear(layer_sizes[4], layer_sizes[5]), GELU())])\n",
            "    \n",
            "    def forward(self, x):\n",
            "        for layer in self.layers:\n",
            "            out = layer(x)\n",
            "            if self.use_skip and x.shape == out.shape:\n",
            "                x = out + x\n",
            "            else:\n",
            "                x = out\n",
            "        return x\n",
            "                \n",
            "layer_sizes = [3, 3, 3, 3, 3, 1]\n",
            "sample_input = torch.tensor([[1.0, 0., -1]])\n",
            "torch.manual_seed(123)\n",
            "model_no_skip = DeepNeuralNetwork(layer_sizes, False)\n",
            "\n",
            "def print_gradients(model, x):\n",
            "    output = model(x)\n",
            "    target = torch.tensor([[0.]])\n",
            "    loss = torch.nn.MSELoss()\n",
            "    loss = loss(output, target)\n",
            "    loss.backward()\n",
            "    for name, param in model.named_parameters():\n",
            "        if 'weight' in name:\n",
            "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")\n",
            "\n",
            "\n",
            "print(\"no skip\")\n",
            "print_gradients(model_no_skip, sample_input)\n",
            "\n",
            "print(\"skip\")\n",
            "model_skip = DeepNeuralNetwork(layer_sizes, True)\n",
            "print_gradients(model_skip, sample_input)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 83,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Input shape: torch.Size([2, 4, 768])\n",
                  "Output shape: torch.Size([2, 4, 768])\n"
               ]
            }
         ],
         "source": [
            "# Transformer implementation - combines all of the above\n",
            "\n",
            "class TransformerBlock(torch.nn.Module):\n",
            "    def __init__(self, cfg):\n",
            "        super().__init__()\n",
            "        self.att = MultiHeadAttention(\n",
            "            d_in=cfg[\"emb_dim\"],\n",
            "            d_out=cfg[\"emb_dim\"],\n",
            "            context_length=cfg[\"context_length\"],\n",
            "            num_heads=cfg[\"n_heads\"],\n",
            "            dropout=cfg[\"drop_rate\"],\n",
            "            qkv_bias=cfg[\"qkv_bias\"])\n",
            "        self.ff = FeedForward(cfg)\n",
            "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
            "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
            "        self.drop_shortcut = torch.nn.Dropout(cfg[\"drop_rate\"])\n",
            "    def forward(self, x):\n",
            "        shortcut = x\n",
            "        # layer norm\n",
            "        x = self.norm1(x)\n",
            "        # multihead attn\n",
            "        x = self.att(x)\n",
            "        # dropout\n",
            "        x = self.drop_shortcut(x)\n",
            "        # skip connection\n",
            "        x = x + shortcut\n",
            "        shortcut = x\n",
            "        # layer norm\n",
            "        x = self.norm2(x)\n",
            "        # feedforward\n",
            "        x = self.ff(x)\n",
            "        # dropout\n",
            "        x = self.drop_shortcut(x)\n",
            "        # skip connection\n",
            "        x = x + shortcut\n",
            "        return x\n",
            "\n",
            "\n",
            "torch.manual_seed(123)\n",
            "x = torch.rand(2, 4, 768)\n",
            "block = TransformerBlock(GPT_CONFIG_124M)\n",
            "output = block(x)\n",
            "print(\"Input shape:\", x.shape)\n",
            "print(\"Output shape:\", output.shape)\n",
            "\n",
            "# The preservation of shape throughout the transformer block\n",
            "# architecture is not incidental but a crucial aspect of its\n",
            "# design. This design enables its effective application across a\n",
            "# wide range of sequence-to-sequence tasks, where each\n",
            "# output vector directly corresponds to an input vector,\n",
            "# maintaining a one-to-one relationship"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 88,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "torch.Size([2, 1024])\n",
                  "Input batch:\n",
                  " tensor([[   40,   367,  2885,  ...,   691, 12226,   318],\n",
                  "        [  367,  2885,  1464,  ..., 12226,   318,   284]])\n",
                  "\n",
                  "Output shape: torch.Size([2, 1024, 50257])\n",
                  "tensor([[[ 0.1187, -0.1796, -0.6110,  ...,  0.0130, -1.3194,  0.1855],\n",
                  "         [ 0.3433, -0.4382,  0.3009,  ..., -0.6556, -1.0232, -0.4446],\n",
                  "         [ 0.0872,  0.0704, -1.1122,  ..., -0.0253, -0.9942, -0.1097],\n",
                  "         ...,\n",
                  "         [ 0.0912,  1.0045,  0.0662,  ...,  0.1016, -0.2378,  0.0536],\n",
                  "         [ 0.0238,  0.8239, -0.7458,  ...,  0.1565, -0.5105, -0.4867],\n",
                  "         [-0.2985, -0.6460, -0.3411,  ..., -0.4631, -0.3258, -0.0331]],\n",
                  "\n",
                  "        [[-0.0576, -0.7426, -0.7723,  ...,  0.1837, -0.7695, -0.3656],\n",
                  "         [ 0.6787, -0.1122,  0.3471,  ..., -0.5583, -1.1185, -0.3215],\n",
                  "         [ 0.9600, -0.2068, -1.1007,  ...,  0.2360, -0.9795, -0.9828],\n",
                  "         ...,\n",
                  "         [-0.4394,  0.0363, -0.4366,  ..., -0.2182, -1.5601, -0.4550],\n",
                  "         [ 0.2571,  1.3750, -0.5499,  ..., -0.2535,  0.1757, -0.4047],\n",
                  "         [-0.6850,  0.3276, -0.8184,  ...,  0.4380, -0.2520, -0.1804]]],\n",
                  "       grad_fn=<UnsafeViewBackward0>)\n"
               ]
            }
         ],
         "source": [
            "# GPT Model\n",
            "from torch import nn\n",
            "\n",
            "class GPTModel(nn.Module):\n",
            "    def __init__(self, cfg):\n",
            "        super().__init__()\n",
            "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
            "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
            "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
            "        self.trf_blocks = nn.Sequential(*[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
            "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
            "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
            "\n",
            "    def forward(self, in_idx):\n",
            "        batch_size, seq_len = in_idx.shape\n",
            "        tok_embeds = self.tok_emb(in_idx)\n",
            "        #1\n",
            "        pos_embeds = self.pos_emb(\n",
            "        torch.arange(seq_len, device=in_idx.device)\n",
            "        )\n",
            "        x = tok_embeds + pos_embeds\n",
            "        x = self.drop_emb(x)\n",
            "        x = self.trf_blocks(x)\n",
            "        x = self.final_norm(x)\n",
            "        logits = self.out_head(x)\n",
            "        return logits\n",
            "\n",
            "torch.manual_seed(123)\n",
            "model = GPTModel(GPT_CONFIG_124M)\n",
            "print(first_batch[0].shape)\n",
            "out = model(first_batch[0])\n",
            "print(\"Input batch:\\n\", first_batch[0])\n",
            "print(\"\\nOutput shape:\", out.shape)\n",
            "print(out)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 92,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Total number of parameters: 163,037,184\n",
                  "Token embedding layer shape: torch.Size([50257, 768])\n",
                  "Output layer shape: torch.Size([50257, 768])\n",
                  "Number of trainable parameters considering weight tying: 124,439,808\n"
               ]
            }
         ],
         "source": [
            "total_params = sum(p.numel() for p in model.parameters())\n",
            "print(f\"Total number of parameters: {total_params:,}\")\n",
            "print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n",
            "print(\"Output layer shape:\", model.out_head.weight.shape)\n",
            "\n",
            "total_params_gpt2 = (\n",
            "total_params - sum(p.numel()\n",
            "for p in model.out_head.parameters())\n",
            ")\n",
            "print(f\"Number of trainable parameters \"\n",
            "f\"considering weight tying: {total_params_gpt2:,}\"\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# exercise\n",
            "# Calculate and compare the number of parameters that are\n",
            "# contained in the feed forward module and those that are\n",
            "# contained in the multi-head attention module."
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "stanford",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.10.16"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
