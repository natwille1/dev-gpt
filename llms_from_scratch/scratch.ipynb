{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20479\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\") as f:\n",
    "    lines = f.read()\n",
    "\n",
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', lines)\n",
    "preprocessed[:10]\n",
    "preprocessed = [item for item in preprocessed if item.strip()]\n",
    "print(len(preprocessed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['would', 'wouldn', 'year', 'years', 'yellow', 'yet', 'you', 'younger', 'your', 'yourself']\n",
      "1130\n",
      "! 0\n",
      "\" 1\n",
      "' 2\n",
      "( 3\n",
      ") 4\n",
      ", 5\n",
      "-- 6\n",
      ". 7\n",
      ": 8\n",
      "; 9\n",
      "? 10\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "print(all_words[-10:])\n",
    "vocab_size = len(all_words)\n",
    "print(vocab_size)\n",
    "vocab = {token: idx for idx, token in enumerate(all_words)}\n",
    "i = 10\n",
    "for i, word in enumerate(vocab):\n",
    "    if i > 10:\n",
    "        break\n",
    "    print(word, vocab[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[989, 1077, 7, 7]\n",
      "their was..\n"
     ]
    }
   ],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.token_to_int = vocab\n",
    "        self.int_to_token = {idx: token for token, idx in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item for item in preprocessed if item.strip()]\n",
    "        return [self.token_to_int[token] for token in preprocessed]\n",
    "    \n",
    "    def decode(self, integers):\n",
    "        text = \" \".join([self.int_to_token[idx] for idx in integers])\n",
    "        # replaces spaces before punctuation marks for format sentences correctly\n",
    "        text = re.sub(r'\\s+([,.:;?_!\"()\\'])', r'\\1', text)\n",
    "        return text\n",
    "    \n",
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "test = \"their  was..\"\n",
    "print(tokenizer.encode(test))\n",
    "print(tokenizer.decode(tokenizer.encode(test)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|unk|>', 1130)\n",
      "('<|endoftext|>', 1131)\n",
      "Hi, do you like tea? <|endoftext|> In the sunlit terraces\n"
     ]
    }
   ],
   "source": [
    "all_words.extend([\"<|unk|>\", \"<|endoftext|>\"])\n",
    "all_words\n",
    "vocab = {token: idx for idx, token in enumerate(all_words)}\n",
    "max = 10\n",
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    if i > max:\n",
    "        break\n",
    "    print(item)\n",
    "text1 = \"Hi, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1130, 5, 355, 1126, 628, 975, 10, 1131, 55, 988, 956, 984]\n",
      "<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces\n",
      "<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces\n"
     ]
    }
   ],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.token_to_int = vocab\n",
    "        self.int_to_token = {idx: token for  token, idx in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [item if item in self.token_to_int else \"<|unk|>\" for item in preprocessed]\n",
    "        return [self.token_to_int[token] for token in preprocessed]\n",
    "\n",
    "    def decode(self, integers):\n",
    "        text = \" \".join([self.int_to_token[idx] for idx in integers])\n",
    "        # replaces spaces before punctuation marks for format sentences correctly\n",
    "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
    "        return text\n",
    "\n",
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "encoded = tokenizer.encode(text)\n",
    "# tokenizer.int_to_token.keys()\n",
    "print(encoded)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(decoded)\n",
    "print(tokenizer.decode(tokenizer.encode(text)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n",
      "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "bpe_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "text = (\n",
    "\"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "\"of someunknownPlace.\"\n",
    ")\n",
    "encoded = bpe_tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(encoded)\n",
    "strings = bpe_tokenizer.decode(encoded)\n",
    "print(strings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33901, 86, 343, 86, 220, 959]\n",
      "Akwirw ier\n"
     ]
    }
   ],
   "source": [
    "new_word = \"Akwirw ier\"\n",
    "\n",
    "encoded = bpe_tokenizer.encode(new_word)\n",
    "print(encoded)\n",
    "decoded = bpe_tokenizer.decode(encoded)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20479\n",
      "['year', 'years', 'yellow', 'yet', 'you', 'younger', 'your', 'yourself', '<|unk|>', '<|endoftext|>']\n",
      "[40, 367, 2885, 1464, 1807, 3619, 402, 271, 10899, 2138]\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\") as f:\n",
    "    lines = f.read()\n",
    "\n",
    "print(len(lines))\n",
    "all_words = sorted(set(preprocessed))\n",
    "all_words.extend([\"<|unk|>\", \"<|endoftext|>\"])\n",
    "print(all_words[-10:])\n",
    "vocab_size = len(all_words)\n",
    "enc_text = bpe_tokenizer.encode(lines)\n",
    "vocab_size = len(enc_text)\n",
    "print(enc_text[:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290, 4920]\n",
      "[4920, 2241]\n",
      " and established -->  established himself\n"
     ]
    }
   ],
   "source": [
    "block_size = 2\n",
    "enc_sample = enc_text[50:]\n",
    "for i in range(block_size):\n",
    "    x = enc_sample[i:block_size]\n",
    "    y = enc_sample[i+1:i+block_size+1]\n",
    "    print(x)\n",
    "    print(y)\n",
    "    print(bpe_tokenizer.decode(x), \"-->\", bpe_tokenizer.decode(y))\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 13,
=======
   "execution_count": 12,
>>>>>>> 10223bb67ea2becec745bc13cfd1227e198426e4
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  44,  149, 1003,   57]), tensor([ 149, 1003,   57,   38]))"
      ]
     },
<<<<<<< HEAD
     "execution_count": 13,
=======
     "execution_count": 12,
>>>>>>> 10223bb67ea2becec745bc13cfd1227e198426e4
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, text, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        self.encoded = tokenizer.encode(text)\n",
    "        #max_length = block_size or context_length, so need to substract max_length from range as that will be size of the sliced array \n",
    "        for i in range(0, len(self.encoded) - max_length, stride):\n",
    "            input_chunk = self.encoded[i:i+max_length]\n",
    "            targets = self.encoded[i+1:i+max_length+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(targets))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.input_ids[idx], self.target_ids[idx])\n",
    "\n",
    "\n",
    "dataset = GPTDatasetV1(lines, tokenizer, 4, 1)\n",
    "dataset[1]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 14,
=======
   "execution_count": 13,
>>>>>>> 10223bb67ea2becec745bc13cfd1227e198426e4
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[  40,  367, 2885, 1464],\n",
       "         [ 367, 2885, 1464, 1807],\n",
       "         [2885, 1464, 1807, 3619],\n",
       "         [1464, 1807, 3619,  402]]),\n",
       " tensor([[ 367, 2885, 1464, 1807],\n",
       "         [2885, 1464, 1807, 3619],\n",
       "         [1464, 1807, 3619,  402],\n",
       "         [1807, 3619,  402,  271]])]"
      ]
     },
<<<<<<< HEAD
     "execution_count": 14,
=======
     "execution_count": 13,
>>>>>>> 10223bb67ea2becec745bc13cfd1227e198426e4
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def create_dataloader_v1(text, batch_size, max_length, stride, shuffle=True, drop_last=True, num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(text, tokenizer=tokenizer, max_length=max_length, stride=stride)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers, drop_last=drop_last, shuffle=shuffle)\n",
    "    return dataloader\n",
    "\n",
    "dataloader = create_dataloader_v1(lines, batch_size=4, max_length=4, stride=1, shuffle=False)\n",
    "\n",
    "first_batch = next(iter(dataloader))\n",
    "\n",
    "first_batch"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 16,
=======
   "execution_count": 14,
>>>>>>> 10223bb67ea2becec745bc13cfd1227e198426e4
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[   40,   367,  2885,  1464],\n",
       "         [ 1464,  1807,  3619,   402],\n",
       "         [  402,   271, 10899,  2138],\n",
       "         [ 2138,   257,  7026, 15632]]),\n",
       " tensor([[  367,  2885,  1464,  1807],\n",
       "         [ 1807,  3619,   402,   271],\n",
       "         [  271, 10899,  2138,   257],\n",
       "         [  257,  7026, 15632,   438]])]"
      ]
     },
<<<<<<< HEAD
     "execution_count": 16,
=======
     "execution_count": 14,
>>>>>>> 10223bb67ea2becec745bc13cfd1227e198426e4
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dataloader = create_dataloader_v1(lines, batch_size=4, max_length=4, stride=3, shuffle=False)\n",
    "\n",
    "first_batch = next(iter(dataloader))\n",
    "\n",
    "first_batch"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 3\n",
    "Prior to transformers, RNNs were a popular architecture for NLP tasks such as machine translation. These networks consist of an encoder and decoder network. The encoder processes a sequence of tokens, where the first token is passed to the first hidden layer, the second is passed to the second layer state AS WELL AS the hidden state of the first token. The last hidden state of the encoder is passed to the decoder network, therefore it relies solely on the encapsulated context from the last hidden state of the encoder. It does not have direct access to previous tokens/hidden states (e.g words in a sentence), but only the last generated hidden state from the encoder. Although the the task of the encoder is to generate a sufficiently rich representation of the entire embedded sentence, this is remains limited to short sequence lengths and highlights the main limitation of the classical RNN architecture.\n",
    "\n",
    "#### Bahdanau attention (2014)\n",
    "\n",
    "Modified the encoder-decoder RNN so the decoder can selectively access different parts of the input sequence at each decoding step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaled dot product attention\n",
    "\n",
    "The following implements self-attention where the dot products are scaled by the dimension of the compute key matrix encoding representations of the tokens after apply the key weights matrix. The reason for normalised and scaling these values is to improve training performance. For large architectures that use a large embedding dimension (e.g 1000 for GPT models), large dot products can result in very small gradients during backpropagation due to the softmax function. If one value dominates output from the dot product, the softmax normalisation will accordingly scale all the remaining values to very small values. Scaling by the sqrt of the embedding dim prevents these very large values from dominating the normalisation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
=======
   "cell_type": "code",
   "execution_count": null,
>>>>>>> 10223bb67ea2becec745bc13cfd1227e198426e4
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "tensor([[ 0.5744, -0.4008, -0.6831],\n",
      "        [ 0.1049,  0.1748, -0.0828],\n",
      "        [-0.8691,  0.9307, -0.6233],\n",
      "        [ 0.3495,  0.2621,  1.5216]], grad_fn=<SelectBackward0>)\n",
      "torch.Size([4, 4, 3])\n",
      "torch.Size([2])\n",
      "torch.Size([4, 2])\n",
      "tensor(0.0044, grad_fn=<AddBackward0>)\n",
      "tensor(0.0048, grad_fn=<AddBackward0>)\n",
      "tensor(0.0048, grad_fn=<DotBackward0>)\n",
      "attention_scores_2 shape: torch.Size([4])\n",
      "values shape: torch.Size([4, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.3686,  0.1474], grad_fn=<SqueezeBackward4>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "vocab_size = tokenizer.n_vocab\n",
    "output_dim = 3  # for illustration\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "input_ids, target_ids = first_batch\n",
    "inputs = token_embedding_layer(input_ids)\n",
    "\n",
    "print(inputs[1])\n",
    "print(inputs.shape)\n",
    "\n",
    "x2 = inputs[1][1]\n",
    "d_in = x2.shape[0]\n",
    "d_out = 2\n",
    "\n",
    "torch.manual_seed(0)\n",
    "W_query = torch.nn.Parameter(torch.randn(d_in, d_out))\n",
    "K_query = torch.nn.Parameter(torch.randn(d_in, d_out))\n",
    "V_query = torch.nn.Parameter(torch.randn(d_in, d_out))\n",
    "\n",
    "\n",
    "x2_query = x2 @ W_query # query vector for 2nd token in the first example of the batch\n",
    "keys = inputs[1] @ K_query # key matrix for all tokens in the first example of the batch\n",
    "values = inputs[1] @ V_query # value matrix for all tokens in the first example of the batch\n",
    "\n",
    "print(x2_query.shape) # query vector\n",
    "print(keys.shape) # \n",
    "\n",
    "# attention score\n",
    "tmp_score = 0\n",
    "x2_key = keys[1]\n",
    "for i in range(2):\n",
    "    tmp_score += x2_query[i] * x2_key[i]\n",
    "    print(tmp_score)\n",
    "attention_score_22 = x2_query.dot(keys[1])\n",
    "print(attention_score_22)\n",
    "\n",
    "attention_scores_2 = x2_query @ keys.T\n",
    "# softmax normalisation\n",
    "d_k = keys.shape[1] # embedding dimension of the keys\n",
    "# SCALED DOT PRODUCT ATTENTION\n",
    "attention_weights_2 = torch.nn.functional.softmax(attention_scores_2 / (d_k ** 0.5), dim=-1)\n",
    "attention_weights_2\n",
    "\n",
    "# context vector calculation\n",
    "print(f\"attention_scores_2 shape: {attention_scores_2.shape}\")\n",
    "print(f\"values shape: {values.shape}\")\n",
    "context_vector_2 = attention_weights_2 @ values\n",
    "context_vector_2 # represents relative importance of all the input tokens wrto the 2nd token in the first example of the batch\n",
    "\n",
    "context_vector_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v1(torch.nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        self.W = torch.nn.Parameter(torch.randn(d_in, d_out))\n",
    "        self.K = torch.nn.Parameter(torch.randn(d_in, d_out))\n",
    "        self.V = torch.nn.Parameter(torch.randn(d_in, d_out))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        query = x @ self.W\n",
    "        keys = x @ self.K\n",
    "        values = x @ self.V\n",
    "        attention_scores = query @ keys.T\n",
    "        d_k = keys.shape[1]\n",
    "        attention_weights = torch.nn.functional.softmax(attention_scores / (d_k ** 0.5), dim=-1)\n",
    "        context_vector = attention_weights @ values\n",
    "        return context_vector\n",
    "\n",
    "class SelfAttention_v2(torch.nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias: bool = False):\n",
    "        self.W = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.K = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.V = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        query = self.W(x)\n",
    "        keys = self.K(x)\n",
    "        values = self.V(x)\n",
    "        attention_scores = query @ keys.T\n",
    "        d_k = keys.shape[1]\n",
    "        attention_weights = torch.nn.functional.softmax(attention_scores / (d_k ** 0.5), dim=-1)\n",
    "        context_vector = attention_weights @ values\n",
    "        return context_vector\n",
    "\n",
    "    \n",
=======
      "torch.Size([4, 4]) torch.Size([4, 4, 256])\n",
      "torch.Size([4, 256])\n",
      "torch.Size([4, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "# we know the bpe tokenizer using gpt-2 has vocab size of 50257\n",
    "vocab_size = 50257\n",
    "output_dim = 256 # embedding dim \n",
    "context_length = 4\n",
    "\n",
    "inputs, outputs = first_batch\n",
    "token_embedding_layer = nn.Embedding(vocab_size, output_dim)\n",
    "first_batch_embeddings = token_embedding_layer(inputs)\n",
    "print(inputs.shape, first_batch_embeddings.shape)\n",
    "\n",
    "pos_embedding_layer = nn.Embedding(context_length, output_dim)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(0, context_length))\n",
    "\n",
    "print(pos_embeddings.shape)\n",
    "\n",
    "#openai models use absolute positional embeddings that are learned during training - therefore same pos embedding vectors are used for all tokens\n",
    "\n",
    "input_embeddings = first_batch_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)\n",
>>>>>>> 10223bb67ea2becec745bc13cfd1227e198426e4
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stanford",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
