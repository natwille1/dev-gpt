{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": 1,
         "metadata": {},
         "outputs": [],
         "source": [
            "import tiktoken\n",
            "import re"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 2,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "20479\n"
               ]
            }
         ],
         "source": [
            "with open(\"the-verdict.txt\", \"r\") as f:\n",
            "    lines = f.read()\n",
            "\n",
            "print(len(lines))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 3,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "['I', ' ', 'HAD', ' ', 'always', ' ', 'thought', ' ', 'Jack', ' ']\n",
                  "4690\n"
               ]
            }
         ],
         "source": [
            "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', lines)\n",
            "print(preprocessed[:10])\n",
            "preprocessed = [item for item in preprocessed if item.strip()]\n",
            "print(len(preprocessed))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 4,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "['would', 'wouldn', 'year', 'years', 'yellow', 'yet', 'you', 'younger', 'your', 'yourself']\n",
                  "1130\n",
                  "! 0\n",
                  "\" 1\n",
                  "' 2\n",
                  "( 3\n",
                  ") 4\n",
                  ", 5\n",
                  "-- 6\n",
                  ". 7\n",
                  ": 8\n",
                  "; 9\n",
                  "? 10\n"
               ]
            }
         ],
         "source": [
            "all_words = sorted(set(preprocessed))\n",
            "print(all_words[-10:])\n",
            "vocab_size = len(all_words)\n",
            "print(vocab_size)\n",
            "vocab = {token: idx for idx, token in enumerate(all_words)}\n",
            "for i, word in enumerate(vocab):\n",
            "    if i > 10:\n",
            "        break\n",
            "    print(word, vocab[word])"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 5,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "[989, 1077, 7, 7]\n",
                  "their was..\n"
               ]
            }
         ],
         "source": [
            "class SimpleTokenizerV1:\n",
            "    def __init__(self, vocab):\n",
            "        self.token_to_int = vocab\n",
            "        self.int_to_token = {idx: token for token, idx in vocab.items()}\n",
            "    \n",
            "    def encode(self, text):\n",
            "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
            "        preprocessed = [item for item in preprocessed if item.strip()]\n",
            "        return [self.token_to_int[token] for token in preprocessed]\n",
            "    \n",
            "    def decode(self, integers):\n",
            "        text = \" \".join([self.int_to_token[idx] for idx in integers])\n",
            "        # replaces spaces before punctuation marks for format sentences correctly\n",
            "        text = re.sub(r'\\s+([,.:;?_!\"()\\'])', r'\\1', text)\n",
            "        return text\n",
            "    \n",
            "tokenizer = SimpleTokenizerV1(vocab)\n",
            "test = \"their  was..\"\n",
            "print(tokenizer.encode(test))\n",
            "print(tokenizer.decode(tokenizer.encode(test)))\n",
            "\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 6,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "('younger', 1127)\n",
                  "('your', 1128)\n",
                  "('yourself', 1129)\n",
                  "('<|unk|>', 1130)\n",
                  "('<|endoftext|>', 1131)\n",
                  "Hi, do you like tea? <|endoftext|> In the sunlit terraces\n"
               ]
            }
         ],
         "source": [
            "all_words.extend([\"<|unk|>\", \"<|endoftext|>\"])\n",
            "all_words\n",
            "vocab = {token: idx for idx, token in enumerate(all_words)}\n",
            "max = 10\n",
            "for i, item in enumerate(list(vocab.items())[-5:]):\n",
            "    if i > max:\n",
            "        break\n",
            "    print(item)\n",
            "text1 = \"Hi, do you like tea?\"\n",
            "text2 = \"In the sunlit terraces\"\n",
            "text = \" <|endoftext|> \".join((text1, text2))\n",
            "\n",
            "print(text)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 7,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "[1130, 5, 355, 1126, 628, 975, 10, 1131, 55, 988, 956, 984]\n",
                  "<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces\n",
                  "<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces\n"
               ]
            }
         ],
         "source": [
            "class SimpleTokenizerV2:\n",
            "    def __init__(self, vocab):\n",
            "        self.token_to_int = vocab\n",
            "        self.int_to_token = {idx: token for  token, idx in vocab.items()}\n",
            "    \n",
            "    def encode(self, text):\n",
            "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
            "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
            "        preprocessed = [item if item in self.token_to_int else \"<|unk|>\" for item in preprocessed]\n",
            "        return [self.token_to_int[token] for token in preprocessed]\n",
            "\n",
            "    def decode(self, integers):\n",
            "        text = \" \".join([self.int_to_token[idx] for idx in integers])\n",
            "        # replaces spaces before punctuation marks for format sentences correctly\n",
            "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
            "        return text\n",
            "\n",
            "tokenizer = SimpleTokenizerV2(vocab)\n",
            "encoded = tokenizer.encode(text)\n",
            "# tokenizer.int_to_token.keys()\n",
            "print(encoded)\n",
            "decoded = tokenizer.decode(encoded)\n",
            "print(decoded)\n",
            "print(tokenizer.decode(tokenizer.encode(text)))\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 8,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n",
                  "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
               ]
            }
         ],
         "source": [
            "bpe_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
            "text = (\n",
            "\"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
            "\"of someunknownPlace.\"\n",
            ")\n",
            "encoded = bpe_tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
            "print(encoded)\n",
            "strings = bpe_tokenizer.decode(encoded)\n",
            "print(strings)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 9,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "[33901, 86, 343, 86, 220, 959]\n",
                  "Akwirw ier\n"
               ]
            }
         ],
         "source": [
            "new_word = \"Akwirw ier\"\n",
            "\n",
            "encoded = bpe_tokenizer.encode(new_word)\n",
            "print(encoded)\n",
            "decoded = bpe_tokenizer.decode(encoded)\n",
            "print(decoded)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 10,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "['I', ' ', 'HAD', ' ', 'always', ' ', 'thought', ' ', 'Jack', ' ']\n",
                  "20479\n",
                  "['year', 'years', 'yellow', 'yet', 'you', 'younger', 'your', 'yourself', '<|unk|>', '<|endoftext|>']\n",
                  "[40, 367, 2885, 1464, 1807, 3619, 402, 271, 10899, 2138]\n"
               ]
            }
         ],
         "source": [
            "with open(\"the-verdict.txt\", \"r\") as f:\n",
            "    lines = f.read()\n",
            "\n",
            "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', lines)\n",
            "print(preprocessed[:10])\n",
            "preprocessed = [item for item in preprocessed if item.strip()]\n",
            "print(len(lines))\n",
            "all_words = sorted(set(preprocessed))\n",
            "all_words.extend([\"<|unk|>\", \"<|endoftext|>\"])\n",
            "print(all_words[-10:])\n",
            "vocab_size = len(all_words)\n",
            "enc_text = bpe_tokenizer.encode(lines)\n",
            "vocab_size = len(enc_text)\n",
            "print(enc_text[:10])\n",
            "\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 11,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "[290, 4920]\n",
                  "[4920, 2241]\n",
                  " and established -->  established himself\n"
               ]
            }
         ],
         "source": [
            "block_size = 2\n",
            "enc_sample = enc_text[50:]\n",
            "for i in range(block_size):\n",
            "    x = enc_sample[i:block_size]\n",
            "    y = enc_sample[i+1:i+block_size+1]\n",
            "    print(x)\n",
            "    print(y)\n",
            "    print(bpe_tokenizer.decode(x), \"-->\", bpe_tokenizer.decode(y))\n",
            "    break\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 12,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "(tensor([  44,  149, 1003,   57]), tensor([ 149, 1003,   57,   38]))"
                  ]
               },
               "execution_count": 12,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "import torch\n",
            "from torch.utils.data import DataLoader, Dataset\n",
            "\n",
            "class GPTDatasetV1(Dataset):\n",
            "    def __init__(self, text, tokenizer, max_length, stride):\n",
            "        self.input_ids = []\n",
            "        self.target_ids = []\n",
            "        self.encoded = tokenizer.encode(text)\n",
            "        #max_length = block_size or context_length, so need to substract max_length from range as that will be size of the sliced array \n",
            "        for i in range(0, len(self.encoded) - max_length, stride):\n",
            "            input_chunk = self.encoded[i:i+max_length]\n",
            "            targets = self.encoded[i+1:i+max_length+1]\n",
            "            self.input_ids.append(torch.tensor(input_chunk))\n",
            "            self.target_ids.append(torch.tensor(targets))\n",
            "    \n",
            "    def __len__(self):\n",
            "        return len(self.input_ids)\n",
            "    \n",
            "    def __getitem__(self, idx):\n",
            "        return (self.input_ids[idx], self.target_ids[idx])\n",
            "\n",
            "\n",
            "dataset = GPTDatasetV1(lines, tokenizer, 4, 1)\n",
            "dataset[1]\n",
            "\n",
            "\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 13,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "torch.Size([4, 4])"
                  ]
               },
               "execution_count": 13,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "\n",
            "def create_dataloader_v1(text, batch_size, max_length, stride, shuffle=True, drop_last=True, num_workers=0):\n",
            "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
            "    dataset = GPTDatasetV1(text, tokenizer=tokenizer, max_length=max_length, stride=stride)\n",
            "    dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers, drop_last=drop_last, shuffle=shuffle)\n",
            "    return dataloader\n",
            "\n",
            "dataloader = create_dataloader_v1(lines, batch_size=4, max_length=4, stride=1, shuffle=False)\n",
            "\n",
            "first_batch = next(iter(dataloader))\n",
            "\n",
            "first_batch[0].shape"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 14,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "[tensor([[   40,   367,  2885,  1464],\n",
                     "         [ 1464,  1807,  3619,   402],\n",
                     "         [  402,   271, 10899,  2138],\n",
                     "         [ 2138,   257,  7026, 15632]]),\n",
                     " tensor([[  367,  2885,  1464,  1807],\n",
                     "         [ 1807,  3619,   402,   271],\n",
                     "         [  271, 10899,  2138,   257],\n",
                     "         [  257,  7026, 15632,   438]])]"
                  ]
               },
               "execution_count": 14,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "\n",
            "dataloader = create_dataloader_v1(lines, batch_size=4, max_length=4, stride=3, shuffle=False)\n",
            "\n",
            "first_batch = next(iter(dataloader))\n",
            "\n",
            "first_batch"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Chapter 3\n",
            "Prior to transformers, RNNs were a popular architecture for NLP tasks such as machine translation. These networks consist of an encoder and decoder network. The encoder processes a sequence of tokens, where the first token is passed to the first hidden layer, the second is passed to the second layer state AS WELL AS the hidden state of the first token. The last hidden state of the encoder is passed to the decoder network, therefore it relies solely on the encapsulated context from the last hidden state of the encoder. It does not have direct access to previous tokens/hidden states (e.g words in a sentence), but only the last generated hidden state from the encoder. Although the the task of the encoder is to generate a sufficiently rich representation of the entire embedded sentence, this is remains limited to short sequence lengths and highlights the main limitation of the classical RNN architecture.\n",
            "\n",
            "#### Bahdanau attention (2014)\n",
            "\n",
            "Modified the encoder-decoder RNN so the decoder can selectively access different parts of the input sequence at each decoding step. "
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Scaled dot product attention\n",
            "\n",
            "The following implements self-attention where the dot products are scaled by the dimension of the compute key matrix encoding representations of the tokens after apply the key weights matrix. The reason for normalised and scaling these values is to improve training performance. For large architectures that use a large embedding dimension (e.g 1000 for GPT models), large dot products can result in very small gradients during backpropagation due to the softmax function. If one value dominates output from the dot product, the softmax normalisation will accordingly scale all the remaining values to very small values. Scaling by the sqrt of the embedding dim prevents these very large values from dominating the normalisation.\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 15,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "tensor([[-1.3780,  0.5325, -0.7355],\n",
                  "        [-0.3446, -0.1626, -1.7539],\n",
                  "        [-1.4151, -0.4194, -0.4316],\n",
                  "        [-0.0733,  0.0910,  1.3885]], grad_fn=<SelectBackward0>)\n",
                  "input shape torch.Size([4, 4, 3])\n",
                  "torch.Size([2])\n",
                  "torch.Size([4, 2])\n",
                  "tensor(1.7674, grad_fn=<AddBackward0>)\n",
                  "tensor(0.4321, grad_fn=<AddBackward0>)\n",
                  "tensor(0.4321, grad_fn=<DotBackward0>)\n",
                  "attention_scores_2 shape: torch.Size([4])\n",
                  "values shape: torch.Size([4, 2])\n"
               ]
            },
            {
               "data": {
                  "text/plain": [
                     "tensor([ 0.9945, -0.7966], grad_fn=<SqueezeBackward4>)"
                  ]
               },
               "execution_count": 15,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "\n",
            "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
            "vocab_size = tokenizer.n_vocab\n",
            "output_dim = 3  # for illustration\n",
            "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
            "input_ids, target_ids = first_batch\n",
            "inputs = token_embedding_layer(input_ids)\n",
            "\n",
            "print(inputs[1])\n",
            "print(\"input shape\", inputs.shape)\n",
            "\n",
            "x2 = inputs[1][1]\n",
            "d_in = x2.shape[0]\n",
            "d_out = 2\n",
            "\n",
            "torch.manual_seed(0)\n",
            "W_query = torch.nn.Parameter(torch.randn(d_in, d_out))\n",
            "K_query = torch.nn.Parameter(torch.randn(d_in, d_out))\n",
            "V_query = torch.nn.Parameter(torch.randn(d_in, d_out))\n",
            "\n",
            "\n",
            "x2_query = x2 @ W_query # query vector for 2nd token in the first example of the batch\n",
            "keys = inputs[1] @ K_query # key matrix for all tokens in the first example of the batch\n",
            "values = inputs[1] @ V_query # value matrix for all tokens in the first example of the batch\n",
            "\n",
            "print(x2_query.shape) # query vector\n",
            "print(keys.shape) # \n",
            "\n",
            "# attention score\n",
            "tmp_score = 0\n",
            "x2_key = keys[1]\n",
            "for i in range(2):\n",
            "    tmp_score += x2_query[i] * x2_key[i]\n",
            "    print(tmp_score)\n",
            "attention_score_22 = x2_query.dot(keys[1])\n",
            "print(attention_score_22)\n",
            "\n",
            "attention_scores_2 = x2_query @ keys.T\n",
            "# softmax normalisation\n",
            "d_k = keys.shape[1] # embedding dimension of the keys\n",
            "# SCALED DOT PRODUCT ATTENTION\n",
            "attention_weights_2 = torch.nn.functional.softmax(attention_scores_2 / (d_k ** 0.5), dim=-1)\n",
            "attention_weights_2\n",
            "\n",
            "# context vector calculation\n",
            "print(f\"attention_scores_2 shape: {attention_scores_2.shape}\")\n",
            "print(f\"values shape: {values.shape}\")\n",
            "context_vector_2 = attention_weights_2 @ values\n",
            "context_vector_2 # represents relative importance of all the input tokens wrto the 2nd token in the first example of the batch\n",
            "\n",
            "context_vector_2\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 16,
         "metadata": {},
         "outputs": [],
         "source": [
            "class SelfAttention_v1(torch.nn.Module):\n",
            "    def __init__(self, d_in, d_out):\n",
            "        super().__init__()\n",
            "        self.Q = torch.nn.Parameter(torch.randn(d_in, d_out))\n",
            "        self.K = torch.nn.Parameter(torch.randn(d_in, d_out))\n",
            "        self.V = torch.nn.Parameter(torch.randn(d_in, d_out))\n",
            "    \n",
            "    def forward(self, x):\n",
            "        query = x @ self.Q\n",
            "        keys = x @ self.K\n",
            "        values = x @ self.V\n",
            "        attention_scores = query @ keys.T\n",
            "        d_k = keys.shape[1]\n",
            "        # rescale the attention scores because dot products can be large when embedding dimension is large\n",
            "        # i.e the sum is taken over the embedding dimension, so many values result in a large dot product\n",
            "        # results in very small gradients\n",
            "        attention_weights = torch.nn.functional.softmax(attention_scores / (d_k ** 0.5), dim=-1)\n",
            "        context_vector = attention_weights @ values\n",
            "        return context_vector\n",
            "\n",
            "class SelfAttention_v2(torch.nn.Module):\n",
            "    def __init__(self, d_in, d_out, qkv_bias: bool = False):\n",
            "        super().__init__()\n",
            "        # proper initialization of weights in torch nn.Linear modules\n",
            "        self.Q = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
            "        self.K = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
            "        self.V = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
            "    \n",
            "    def forward(self, x):\n",
            "        query = self.Q(x)\n",
            "        keys = self.K(x)\n",
            "        values = self.V(x)\n",
            "        attention_scores = query @ keys.T\n",
            "        d_k = keys.shape[1]\n",
            "        attention_weights = torch.nn.functional.softmax(attention_scores / (d_k ** 0.5), dim=-1)\n",
            "        context_vector = attention_weights @ values\n",
            "        return context_vector\n",
            "\n",
            "    \n",
            "\n",
            "\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 17,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "torch.Size([4, 4, 3])"
                  ]
               },
               "execution_count": 17,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "inputs.shape"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 18,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "d_in 3, d_out 2\n",
                  "torch.Size([4, 3])\n",
                  "sa_v1\n",
                  "tensor([ 3.3697, -0.0059], grad_fn=<SelectBackward0>)\n",
                  "sa_v2\n",
                  "tensor([-0.0793,  0.2262], grad_fn=<SelectBackward0>)\n",
                  "torch.Size([3, 2])\n",
                  "torch.Size([2, 3])\n",
                  "updated sa_v1\n",
                  "tensor([-0.0793,  0.2262], grad_fn=<SelectBackward0>)\n"
               ]
            }
         ],
         "source": [
            "# compare SelfAttentionV1 and V2\n",
            "d_in = inputs.shape[-1] # embedding dim\n",
            "d_out = 2\n",
            "print(f\"d_in {d_in}, d_out {d_out}\")\n",
            "sa_v1 = SelfAttention_v1(d_in=d_in, d_out=d_out)\n",
            "sa_v2 = SelfAttention_v2(d_in=d_in, d_out=d_out)\n",
            "\n",
            "x2 = inputs[1]\n",
            "print(x2.shape)\n",
            "sa_v1_mat = sa_v1(inputs[1])\n",
            "sa_v2_mat = sa_v2(inputs[1])\n",
            "print(\"sa_v1\")\n",
            "print(sa_v1_mat[0])\n",
            "print(\"sa_v2\")\n",
            "print(sa_v2_mat[0])\n",
            "\n",
            "print(sa_v1.Q.shape)\n",
            "print(sa_v2.Q.weight.shape)\n",
            "\n",
            "sa_v1.Q = torch.nn.Parameter(sa_v2.Q.weight.T)\n",
            "sa_v1.V = torch.nn.Parameter(sa_v2.V.weight.T)\n",
            "sa_v1.K = torch.nn.Parameter(sa_v2.K.weight.T)\n",
            "print(\"updated sa_v1\")\n",
            "sa_v1_mat_up = sa_v1(inputs[1])\n",
            "print(sa_v1_mat_up[0])\n",
            "\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 19,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "torch.Size([4, 4, 2]) torch.Size([4, 4, 2])\n",
                  "context length 4\n",
                  "tensor([[0., 1., 1., 1.],\n",
                  "        [0., 0., 1., 1.],\n",
                  "        [0., 0., 0., 1.],\n",
                  "        [0., 0., 0., 0.]])\n",
                  "tensor([[False,  True,  True,  True],\n",
                  "        [False, False,  True,  True],\n",
                  "        [False, False, False,  True],\n",
                  "        [False, False, False, False]])\n",
                  "tensor([[[-0.1464,    -inf,    -inf,    -inf],\n",
                  "         [-0.1635, -0.1080,    -inf,    -inf],\n",
                  "         [-0.2915, -0.1780, -0.0812,    -inf],\n",
                  "         [ 0.0865,  0.1422,  0.3364, -0.0422]],\n",
                  "\n",
                  "        [[-0.0422,    -inf,    -inf,    -inf],\n",
                  "         [-0.1691, -0.4977,    -inf,    -inf],\n",
                  "         [-0.0688, -1.1478,  0.1376,    -inf],\n",
                  "         [ 0.1302,  0.1614,  0.7024,  0.0093]],\n",
                  "\n",
                  "        [[ 0.0093,    -inf,    -inf,    -inf],\n",
                  "         [ 0.2120,  0.5268,    -inf,    -inf],\n",
                  "         [ 0.1706,  0.1400, -0.0270,    -inf],\n",
                  "         [-0.4841, -0.7002,  0.0775,  0.8519]],\n",
                  "\n",
                  "        [[ 0.8519,    -inf,    -inf,    -inf],\n",
                  "         [ 1.9729,  2.6030,    -inf,    -inf],\n",
                  "         [ 0.5238,  0.5978,  0.1404,    -inf],\n",
                  "         [-0.1098, -0.1367, -0.0527,  0.0577]]], grad_fn=<MaskedFillBackward0>)\n"
               ]
            },
            {
               "data": {
                  "text/plain": [
                     "tensor([[[1.0000, 0.0000, 0.0000, 0.0000],\n",
                     "         [0.4902, 0.5098, 0.0000, 0.0000],\n",
                     "         [0.3083, 0.3340, 0.3577, 0.0000],\n",
                     "         [0.2412, 0.2509, 0.2878, 0.2202]],\n",
                     "\n",
                     "        [[1.0000, 0.0000, 0.0000, 0.0000],\n",
                     "         [0.5578, 0.4422, 0.0000, 0.0000],\n",
                     "         [0.3812, 0.1777, 0.4411, 0.0000],\n",
                     "         [0.2253, 0.2303, 0.3376, 0.2068]],\n",
                     "\n",
                     "        [[1.0000, 0.0000, 0.0000, 0.0000],\n",
                     "         [0.4446, 0.5554, 0.0000, 0.0000],\n",
                     "         [0.3511, 0.3436, 0.3053, 0.0000],\n",
                     "         [0.1690, 0.1450, 0.2514, 0.4346]],\n",
                     "\n",
                     "        [[1.0000, 0.0000, 0.0000, 0.0000],\n",
                     "         [0.3904, 0.6096, 0.0000, 0.0000],\n",
                     "         [0.3551, 0.3741, 0.2708, 0.0000],\n",
                     "         [0.2411, 0.2365, 0.2510, 0.2714]]], grad_fn=<SoftmaxBackward0>)"
                  ]
               },
               "execution_count": 19,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "# masked self-attention (causal self-attention)\n",
            "# ensures that no current position is influenced by future positions (e.g for generative tasks)\n",
            "queries = sa_v2.Q(inputs)\n",
            "keys = sa_v2.K(inputs)\n",
            "print(queries.shape, keys.shape)\n",
            "attention_scores = queries @ keys.transpose(1,2)\n",
            "# normalisation of scores to they form a probability distribution summing to 1 - this is applied along the embedding dim \n",
            "attention_weights = torch.nn.functional.softmax(attention_scores / keys.shape[-1] ** 0.5, dim=-1)\n",
            "\n",
            "context_length = inputs.shape[1]\n",
            "print(f\"context length {context_length}\")\n",
            "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
            "print(mask)\n",
            "print(mask.bool())\n",
            "# negative infinity approaches 0 in softmax function (because e-inf approaches 0)\n",
            "attention_scores_masked = attention_scores.masked_fill_(mask.bool(), -torch.inf)\n",
            "print(attention_scores_masked)\n",
            "# normalised weights\n",
            "attention_weights = torch.nn.functional.softmax((attention_scores_masked) / keys.shape[-1] ** 0.5, dim=-1)\n",
            "attention_weights"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 20,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "tensor([[2., 2., 0., 2.],\n",
                  "        [2., 0., 0., 0.],\n",
                  "        [0., 2., 0., 2.],\n",
                  "        [2., 2., 2., 2.]])\n"
               ]
            }
         ],
         "source": [
            "# Dropout\n",
            "# only applied during training, not inference\n",
            "torch.manual_seed(123)\n",
            "dropout = torch.nn.Dropout(0.5)\n",
            "example = torch.ones((context_length, context_length))\n",
            "# to compensate for the reduction in active units, the un-zero'd values are scaled by (original_val / dropout_prob (0.5))\n",
            "# 1 / 0.5 = 2\n",
            "# this ensures the overall influence of the weight is consistent at both training and inference time\n",
            "print(dropout(example))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 21,
         "metadata": {},
         "outputs": [],
         "source": [
            "class CausalAttention(torch.nn.Module):\n",
            "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias: bool= False):\n",
            "        super().__init__()\n",
            "        self.d_out = d_out\n",
            "        self.dropout = torch.nn.Dropout(dropout)\n",
            "        self.Q = torch.nn.Linear(in_features=d_in, out_features=d_out)\n",
            "        self.K = torch.nn.Linear(in_features=d_in, out_features=d_out)\n",
            "        self.V = torch.nn.Linear(in_features=d_in, out_features=d_out)\n",
            "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
            "    \n",
            "    def forward(self, x):\n",
            "        batch, num_token, d_in = x.shape\n",
            "        keys = self.K(x)\n",
            "        queries = self.Q(x)\n",
            "        values = self.V(x)\n",
            "        attn_scores = queries @ keys.transpose(1, 2)\n",
            "        attn_scores.masked_fill_(self.mask.bool()[:num_token, :num_token], -torch.inf)\n",
            "        attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim=-1)\n",
            "        attn_weights = self.dropout(attn_weights)\n",
            "        context_vec = attn_weights @ values\n",
            "        return context_vec\n",
            "        \n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 22,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "torch.Size([4, 4, 2])"
                  ]
               },
               "execution_count": 22,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "context_length = inputs.shape[1]\n",
            "ca=CausalAttention(d_in, d_out, context_length, 0.0)\n",
            "context_vecs = ca(inputs)\n",
            "context_vecs.shape\n",
            "\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 23,
         "metadata": {},
         "outputs": [],
         "source": [
            "# multihead attention\n",
            "# apply multiple query key and value matrices (one for each head) in parallel\n",
            "\n",
            "class MultiHeadAttention(torch.nn.Module):\n",
            "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
            "        super().__init__()\n",
            "        assert (d_out % num_heads == 0), f\"d_out {d_out} must be divisible by num heads {num_heads}\"\n",
            "        self.d_out = d_out\n",
            "        self.num_heads = num_heads\n",
            "        self.head_dim = d_out // num_heads\n",
            "        self.Q = torch.nn.Linear(in_features=d_in, out_features=d_out)\n",
            "        self.K = torch.nn.Linear(in_features=d_in, out_features=d_out)\n",
            "        self.V = torch.nn.Linear(in_features=d_in, out_features=d_out)\n",
            "        self.out_proj = torch.nn.Linear(in_features=d_out, out_features=d_out)\n",
            "        self.dropout = torch.nn.Dropout(dropout)\n",
            "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
            "    \n",
            "    def forward(self, x):\n",
            "        batch, num_token, d_in = x.shape\n",
            "        keys = self.K(x)\n",
            "        queries = self.Q(x)\n",
            "        values = self.V(x)\n",
            "        keys = keys.view(batch, num_token, self.num_heads, self.head_dim)\n",
            "        queries = queries.view(batch, num_token, self.num_heads, self.head_dim)\n",
            "        values = values.view(batch, num_token, self.num_heads, self.head_dim)\n",
            "        keys = keys.transpose(1,2)\n",
            "        queries = queries.transpose(1,2)\n",
            "        values = values.transpose(1,2)\n",
            "        attn_scores = queries @ keys.transpose(2, 3)\n",
            "        mask_bool = self.mask.bool()[:num_token, :num_token]\n",
            "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
            "        attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim=-1)\n",
            "        attn_weights = self.dropout(attn_weights)\n",
            "        context_vec = attn_weights @ values\n",
            "        # contiguous makes a copy of the tensor with the final memory layout specified by the tensor shape\n",
            "        context_vec = context_vec.contiguous().view(batch, num_token, self.d_out)\n",
            "        # project layer\n",
            "        context_vec = self.out_proj(context_vec)\n",
            "        return context_vec\n",
            "        \n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 24,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "torch.Size([4, 4, 2])"
                  ]
               },
               "execution_count": 24,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "torch.manual_seed(123)\n",
            "\n",
            "batch_size, context_length, d_in = inputs.shape\n",
            "d_out = 2\n",
            "mha = MultiHeadAttention(d_in, d_out, context_length, 0.5, num_heads=2)\n",
            "out = mha(inputs)\n",
            "out.shape"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 25,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "torch.Size([2, 1024])\n"
               ]
            },
            {
               "data": {
                  "text/plain": [
                     "torch.Size([2, 1024, 768])"
                  ]
               },
               "execution_count": 25,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "# gpt-2 sized MHA\n",
            "\n",
            "embedding_dim = 768\n",
            "num_heads = 12\n",
            "context_length = 1024\n",
            "\n",
            "dataloader = create_dataloader_v1(lines, batch_size=2, max_length=context_length, stride=1, shuffle=False)\n",
            "first_batch = next(iter(dataloader))\n",
            "print(first_batch[0].shape)\n",
            "\n",
            "vocab_size = tokenizer.n_vocab\n",
            "token_embedding_layer = torch.nn.Embedding(vocab_size, embedding_dim)\n",
            "input_ids, target_ids = first_batch\n",
            "inputs = token_embedding_layer(input_ids)\n",
            "\n",
            "inputs[0].shape\n",
            "\n",
            "mha = MultiHeadAttention(d_in=768, d_out=768, context_length=context_length, dropout=0.2, num_heads=12)\n",
            "\n",
            "out = mha(inputs)\n",
            "\n",
            "out.shape"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 26,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "(tensor([[-1.3039e-08, -9.3132e-09, -6.2088e-10,  ..., -3.1044e-09,\n",
                     "          -7.4506e-09, -4.9671e-09],\n",
                     "         [-9.3132e-09, -6.2088e-10, -1.4901e-08,  ..., -7.4506e-09,\n",
                     "          -4.9671e-09,  2.4835e-09]], grad_fn=<MeanBackward1>),\n",
                     " tensor([[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
                     "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
                     "        grad_fn=<VarBackward0>))"
                  ]
               },
               "execution_count": 26,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "# GPT-2 implementation\n",
            "# vocab size is determined by the BPE tokenizer\n",
            "GPT_CONFIG_124M = {\"vocab_size\": 50257, \"context_length\": 1024, \"emb_dim\": 768, \"n_heads\": 12, \"n_layers\": 12, \"drop_rate\": 0.1, \"qkv_bias\": False}\n",
            "\n",
            "class LayerNorm(torch.nn.Module):\n",
            "    def __init__(self, emb_dim):\n",
            "        super().__init__()\n",
            "        self.eps = 1e-5\n",
            "        # Parameter = trainable parameter in torch\n",
            "        self.scale = torch.nn.Parameter(torch.ones(emb_dim))\n",
            "        self.shift = torch.nn.Parameter(torch.zeros(emb_dim))\n",
            "\n",
            "    def forward(self, x):\n",
            "        # dim = -1  --> embedding dimension\n",
            "        mean = x.mean(dim=-1, keepdim=True)\n",
            "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
            "        norm_x = (x-mean) / torch.sqrt(var + self.eps)\n",
            "        return self.scale * norm_x + self.shift\n",
            "\n",
            "first_batch\n",
            "embedding_dim = 768 \n",
            "ln = LayerNorm(embedding_dim)\n",
            "out_ln = ln(inputs)\n",
            "# expect mean = 0 and var = 1\n",
            "out_ln.mean(dim=-1), out_ln.var(dim=-1, unbiased=False)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 27,
         "metadata": {},
         "outputs": [],
         "source": [
            "# GELU activation function\n",
            "\n",
            "class GELU(torch.nn.Module):\n",
            "    def __init__(self):\n",
            "        super().__init__()\n",
            "    \n",
            "    def forward(self, x):\n",
            "        value = 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2.0 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3))))\n",
            "        return value\n",
            "\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 28,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "image/png": "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",
                  "text/plain": [
                     "<Figure size 800x300 with 2 Axes>"
                  ]
               },
               "metadata": {},
               "output_type": "display_data"
            }
         ],
         "source": [
            "# GELU compared to RELU\n",
            "import matplotlib.pyplot as plt\n",
            "\n",
            "gelu, relu = GELU(), torch.nn.ReLU()\n",
            "\n",
            "x = torch.linspace(-3, 3, 100)\n",
            "\n",
            "gelu_out = gelu(x)\n",
            "relu_out = relu(x)\n",
            "\n",
            "plt.figure(figsize=(8, 3))\n",
            "for i, (y, label) in enumerate(zip([gelu_out, relu_out], [\"GELU\",\"RELU\"]), 1):\n",
            "    plt.subplot(1, 2, i)\n",
            "    plt.plot(x, y)\n",
            "    plt.title(f\"{label} activation function\")\n",
            "    plt.xlabel(\"x\")\n",
            "    plt.ylabel(f\"{label}(x)\")\n",
            "plt.tight_layout()\n",
            "plt.show()\n",
            "\n",
            "# GELU is a smooth, non-linear function that is differentiable at almost any negative value (except ~ -0.75). \n",
            "# Better optimisation due to:\n",
            "# 1. smoothness\n",
            "# 2. differential for negative values so these can contribute to the gradients/optimisation process"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 29,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "{'vocab_size': 50257, 'context_length': 1024, 'emb_dim': 768, 'n_heads': 12, 'n_layers': 12, 'drop_rate': 0.1, 'qkv_bias': False}\n"
               ]
            }
         ],
         "source": [
            "# Feedforward with GELU\n",
            "# Feedforward layers enable richer representations through expansion to higher dimensions \n",
            "\n",
            "class FeedForward(torch.nn.Module):\n",
            "    def __init__(self, cfg):\n",
            "        super().__init__()\n",
            "        self.layers = torch.nn.Sequential(torch.nn.Linear(cfg['emb_dim'], 4*cfg['emb_dim']), GELU(), torch.nn.Linear(4 * cfg['emb_dim'], cfg['emb_dim']))\n",
            "    \n",
            "    def forward(self, x):\n",
            "        out = self.layers(x)\n",
            "        return out\n",
            "\n",
            "print(GPT_CONFIG_124M)\n",
            "\n",
            "ffn = FeedForward(GPT_CONFIG_124M)\n",
            "x = torch.randn((2, 3, 768))\n",
            "out = ffn(x)\n",
            "\n",
            "\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 30,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "no skip\n",
                  "layers.0.0.weight has gradient mean of 0.00020173587836325169\n",
                  "layers.1.0.weight has gradient mean of 0.0001201116101583466\n",
                  "layers.2.0.weight has gradient mean of 0.0007152041071094573\n",
                  "layers.3.0.weight has gradient mean of 0.0013988735154271126\n",
                  "layers.4.0.weight has gradient mean of 0.005049645435065031\n",
                  "skip\n",
                  "layers.0.0.weight has gradient mean of 0.0014432291500270367\n",
                  "layers.1.0.weight has gradient mean of 0.004846951924264431\n",
                  "layers.2.0.weight has gradient mean of 0.004138893447816372\n",
                  "layers.3.0.weight has gradient mean of 0.005915115587413311\n",
                  "layers.4.0.weight has gradient mean of 0.032659437507390976\n"
               ]
            }
         ],
         "source": [
            "# Skip connections\n",
            "# Create an alternative path for the gradient flow by adding the output from layer to the output of a later layer \n",
            "# First applied in \"residual\" networks in computer vision\n",
            "# Help with optimisation process as the addition of the input (e.g x) to the output from a layer increase the magnitude of the values\n",
            "# Thus helps with the vanishing gradient problem\n",
            "\n",
            "class DeepNeuralNetwork(torch.nn.Module):\n",
            "    def __init__(self, layer_sizes, use_skip):\n",
            "        super().__init__()\n",
            "        self.use_skip = use_skip\n",
            "        self.layers = torch.nn.ModuleList([torch.nn.Sequential(torch.nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()), torch.nn.Sequential(torch.nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()), torch.nn.Sequential(torch.nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()), torch.nn.Sequential(torch.nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()), torch.nn.Sequential(torch.nn.Linear(layer_sizes[4], layer_sizes[5]), GELU())])\n",
            "    \n",
            "    def forward(self, x):\n",
            "        for layer in self.layers:\n",
            "            out = layer(x)\n",
            "            if self.use_skip and x.shape == out.shape:\n",
            "                x = out + x\n",
            "            else:\n",
            "                x = out\n",
            "        return x\n",
            "                \n",
            "layer_sizes = [3, 3, 3, 3, 3, 1]\n",
            "sample_input = torch.tensor([[1.0, 0., -1]])\n",
            "torch.manual_seed(123)\n",
            "model_no_skip = DeepNeuralNetwork(layer_sizes, False)\n",
            "\n",
            "def print_gradients(model, x):\n",
            "    output = model(x)\n",
            "    target = torch.tensor([[0.]])\n",
            "    loss = torch.nn.MSELoss()\n",
            "    loss = loss(output, target)\n",
            "    loss.backward()\n",
            "    for name, param in model.named_parameters():\n",
            "        if 'weight' in name:\n",
            "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")\n",
            "\n",
            "\n",
            "print(\"no skip\")\n",
            "print_gradients(model_no_skip, sample_input)\n",
            "\n",
            "print(\"skip\")\n",
            "model_skip = DeepNeuralNetwork(layer_sizes, True)\n",
            "print_gradients(model_skip, sample_input)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 31,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Input shape: torch.Size([2, 4, 768])\n",
                  "Output shape: torch.Size([2, 4, 768])\n"
               ]
            }
         ],
         "source": [
            "# Transformer implementation - combines all of the above\n",
            "\n",
            "class TransformerBlock(torch.nn.Module):\n",
            "    def __init__(self, cfg):\n",
            "        super().__init__()\n",
            "        self.att = MultiHeadAttention(\n",
            "            d_in=cfg[\"emb_dim\"],\n",
            "            d_out=cfg[\"emb_dim\"],\n",
            "            context_length=cfg[\"context_length\"],\n",
            "            num_heads=cfg[\"n_heads\"],\n",
            "            dropout=cfg[\"drop_rate\"],\n",
            "            qkv_bias=cfg[\"qkv_bias\"])\n",
            "        self.ff = FeedForward(cfg)\n",
            "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
            "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
            "        self.drop_shortcut = torch.nn.Dropout(cfg[\"drop_rate\"])\n",
            "    def forward(self, x):\n",
            "        shortcut = x\n",
            "        # layer norm\n",
            "        x = self.norm1(x)\n",
            "        # multihead attn\n",
            "        x = self.att(x)\n",
            "        # dropout\n",
            "        x = self.drop_shortcut(x)\n",
            "        # skip connection\n",
            "        x = x + shortcut\n",
            "        shortcut = x\n",
            "        # layer norm\n",
            "        x = self.norm2(x)\n",
            "        # feedforward\n",
            "        x = self.ff(x)\n",
            "        # dropout\n",
            "        x = self.drop_shortcut(x)\n",
            "        # skip connection\n",
            "        x = x + shortcut\n",
            "        return x\n",
            "\n",
            "\n",
            "torch.manual_seed(123)\n",
            "x = torch.rand(2, 4, 768)\n",
            "block = TransformerBlock(GPT_CONFIG_124M)\n",
            "output = block(x)\n",
            "print(\"Input shape:\", x.shape)\n",
            "print(\"Output shape:\", output.shape)\n",
            "\n",
            "# The preservation of shape throughout the transformer block\n",
            "# architecture is not incidental but a crucial aspect of its\n",
            "# design. This design enables its effective application across a\n",
            "# wide range of sequence-to-sequence tasks, where each\n",
            "# output vector directly corresponds to an input vector,\n",
            "# maintaining a one-to-one relationship"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 32,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "torch.Size([2, 1024])\n",
                  "Input batch:\n",
                  " tensor([[   40,   367,  2885,  ...,   691, 12226,   318],\n",
                  "        [  367,  2885,  1464,  ..., 12226,   318,   284]])\n",
                  "\n",
                  "Output shape: torch.Size([2, 1024, 50257])\n",
                  "tensor([[[ 0.1187, -0.1796, -0.6110,  ...,  0.0130, -1.3194,  0.1855],\n",
                  "         [ 0.3433, -0.4382,  0.3009,  ..., -0.6556, -1.0232, -0.4446],\n",
                  "         [ 0.0872,  0.0704, -1.1122,  ..., -0.0253, -0.9942, -0.1097],\n",
                  "         ...,\n",
                  "         [ 0.0912,  1.0045,  0.0662,  ...,  0.1016, -0.2378,  0.0536],\n",
                  "         [ 0.0238,  0.8239, -0.7458,  ...,  0.1565, -0.5105, -0.4867],\n",
                  "         [-0.2985, -0.6460, -0.3411,  ..., -0.4631, -0.3258, -0.0331]],\n",
                  "\n",
                  "        [[-0.0576, -0.7426, -0.7723,  ...,  0.1837, -0.7695, -0.3656],\n",
                  "         [ 0.6787, -0.1122,  0.3471,  ..., -0.5583, -1.1185, -0.3215],\n",
                  "         [ 0.9600, -0.2068, -1.1007,  ...,  0.2360, -0.9795, -0.9828],\n",
                  "         ...,\n",
                  "         [-0.4394,  0.0363, -0.4366,  ..., -0.2182, -1.5601, -0.4550],\n",
                  "         [ 0.2571,  1.3750, -0.5499,  ..., -0.2535,  0.1757, -0.4047],\n",
                  "         [-0.6850,  0.3276, -0.8184,  ...,  0.4380, -0.2520, -0.1804]]],\n",
                  "       grad_fn=<UnsafeViewBackward0>)\n"
               ]
            }
         ],
         "source": [
            "# GPT Model\n",
            "from torch import nn\n",
            "\n",
            "class GPTModel(nn.Module):\n",
            "    def __init__(self, cfg):\n",
            "        super().__init__()\n",
            "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
            "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
            "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
            "        self.trf_blocks = nn.Sequential(*[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
            "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
            "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
            "\n",
            "    def forward(self, in_idx):\n",
            "        batch_size, seq_len = in_idx.shape\n",
            "        tok_embeds = self.tok_emb(in_idx)\n",
            "        #1\n",
            "        pos_embeds = self.pos_emb(\n",
            "        torch.arange(seq_len, device=in_idx.device)\n",
            "        )\n",
            "        x = tok_embeds + pos_embeds\n",
            "        x = self.drop_emb(x)\n",
            "        x = self.trf_blocks(x)\n",
            "        x = self.final_norm(x)\n",
            "        logits = self.out_head(x)\n",
            "        return logits\n",
            "\n",
            "torch.manual_seed(123)\n",
            "model = GPTModel(GPT_CONFIG_124M)\n",
            "print(first_batch[0].shape)\n",
            "out = model(first_batch[0])\n",
            "print(\"Input batch:\\n\", first_batch[0])\n",
            "print(\"\\nOutput shape:\", out.shape)\n",
            "print(out)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 33,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Total number of parameters: 163,037,184\n",
                  "Token embedding layer shape: torch.Size([50257, 768])\n",
                  "Output layer shape: torch.Size([50257, 768])\n",
                  "Number of trainable parameters considering weight tying: 124,439,808\n"
               ]
            }
         ],
         "source": [
            "total_params = sum(p.numel() for p in model.parameters())\n",
            "print(f\"Total number of parameters: {total_params:,}\")\n",
            "print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n",
            "print(\"Output layer shape:\", model.out_head.weight.shape)\n",
            "\n",
            "total_params_gpt2 = (\n",
            "total_params - sum(p.numel()\n",
            "for p in model.out_head.parameters())\n",
            ")\n",
            "print(f\"Number of trainable parameters \"\n",
            "f\"considering weight tying: {total_params_gpt2:,}\"\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 34,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "feedfordward params 56669184, attn params 28348416\n"
               ]
            }
         ],
         "source": [
            "# exercise\n",
            "# Calculate and compare the number of parameters that are\n",
            "# contained in the feed forward module and those that are\n",
            "# contained in the multi-head attention module.\n",
            "\n",
            "feedforward_params = 0\n",
            "attn_params = 0\n",
            "for i in range(len(model.trf_blocks)):\n",
            "    feedforward_params+= sum(p.numel() for p in model.trf_blocks[i].ff.parameters())\n",
            "    attn_params += sum(p.numel() for p in model.trf_blocks[i].att.parameters())\n",
            "print(f\"feedfordward params {feedforward_params}, attn params {attn_params}\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 35,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Total size of the model: 621.94 MB\n"
               ]
            }
         ],
         "source": [
            "total_size_bytes = total_params * 4 # 4 bytes per float32 parameter\n",
            "total_size_mb = total_size_bytes / (1024 * 1024)\n",
            "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 36,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "large params 838359040\n",
                  "large model size 3198.09 MB\n"
               ]
            }
         ],
         "source": [
            "# exercise\n",
            "# implement GPT2-large and count params\n",
            "\n",
            "GPT_CONFIG_124M\n",
            "GPT_CONFIG_L = {\"vocab_size\": 50257, \"context_length\": 1024, \"emb_dim\": 1280, \"n_heads\": 20, \"n_layers\": 36, \"drop_rate\": 0.1, \"qkv_bias\": False}\n",
            "\n",
            "large_model = GPTModel(GPT_CONFIG_L)\n",
            "large_params = sum(p.numel() for p in large_model.parameters())\n",
            "large_size = (large_params * 4) / (1024*1024)\n",
            "print(f\"large params {large_params}\")\n",
            "print(f\"large model size {large_size:.2f} MB\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 37,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "[15496, 314, 716]\n"
               ]
            },
            {
               "data": {
                  "text/plain": [
                     "'Hello I am Basdxjected Roosevelt rateoby FaceALL disappearsarmac'"
                  ]
               },
               "execution_count": 37,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "# generate text\n",
            "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
            "    for _ in range(max_new_tokens):\n",
            "        idx_cond = idx[:, -context_size:]\n",
            "        with torch.no_grad():\n",
            "            logits = model(idx_cond)\n",
            "        # last \"timestep\"/token i.e model generated output vs context\n",
            "        logits = logits[:, -1, :]\n",
            "        probas = torch.softmax(logits, dim=-1)\n",
            "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)\n",
            "        # cat the predicted token to the context for the next iteration\n",
            "        idx = torch.cat((idx, idx_next), dim=1)\n",
            "    return idx\n",
            "\n",
            "start_context = \"Hello I am\"\n",
            "encoded = tokenizer.encode(start_context)\n",
            "print(encoded)\n",
            "enc_input = torch.tensor(encoded).unsqueeze(0)\n",
            "model.eval()\n",
            "next_iter = generate_text_simple(model, enc_input, 10, GPT_CONFIG_124M['context_length'])\n",
            "next_iter\n",
            "decoded_text = tokenizer.decode(next_iter.squeeze(0).tolist())\n",
            "decoded_text\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 38,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "GPTModel(\n",
                     "  (tok_emb): Embedding(50257, 768)\n",
                     "  (pos_emb): Embedding(256, 768)\n",
                     "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
                     "  (trf_blocks): Sequential(\n",
                     "    (0): TransformerBlock(\n",
                     "      (att): MultiHeadAttention(\n",
                     "        (Q): Linear(in_features=768, out_features=768, bias=True)\n",
                     "        (K): Linear(in_features=768, out_features=768, bias=True)\n",
                     "        (V): Linear(in_features=768, out_features=768, bias=True)\n",
                     "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
                     "        (dropout): Dropout(p=0.1, inplace=False)\n",
                     "      )\n",
                     "      (ff): FeedForward(\n",
                     "        (layers): Sequential(\n",
                     "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
                     "          (1): GELU()\n",
                     "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
                     "        )\n",
                     "      )\n",
                     "      (norm1): LayerNorm()\n",
                     "      (norm2): LayerNorm()\n",
                     "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
                     "    )\n",
                     "    (1): TransformerBlock(\n",
                     "      (att): MultiHeadAttention(\n",
                     "        (Q): Linear(in_features=768, out_features=768, bias=True)\n",
                     "        (K): Linear(in_features=768, out_features=768, bias=True)\n",
                     "        (V): Linear(in_features=768, out_features=768, bias=True)\n",
                     "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
                     "        (dropout): Dropout(p=0.1, inplace=False)\n",
                     "      )\n",
                     "      (ff): FeedForward(\n",
                     "        (layers): Sequential(\n",
                     "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
                     "          (1): GELU()\n",
                     "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
                     "        )\n",
                     "      )\n",
                     "      (norm1): LayerNorm()\n",
                     "      (norm2): LayerNorm()\n",
                     "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
                     "    )\n",
                     "    (2): TransformerBlock(\n",
                     "      (att): MultiHeadAttention(\n",
                     "        (Q): Linear(in_features=768, out_features=768, bias=True)\n",
                     "        (K): Linear(in_features=768, out_features=768, bias=True)\n",
                     "        (V): Linear(in_features=768, out_features=768, bias=True)\n",
                     "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
                     "        (dropout): Dropout(p=0.1, inplace=False)\n",
                     "      )\n",
                     "      (ff): FeedForward(\n",
                     "        (layers): Sequential(\n",
                     "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
                     "          (1): GELU()\n",
                     "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
                     "        )\n",
                     "      )\n",
                     "      (norm1): LayerNorm()\n",
                     "      (norm2): LayerNorm()\n",
                     "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
                     "    )\n",
                     "    (3): TransformerBlock(\n",
                     "      (att): MultiHeadAttention(\n",
                     "        (Q): Linear(in_features=768, out_features=768, bias=True)\n",
                     "        (K): Linear(in_features=768, out_features=768, bias=True)\n",
                     "        (V): Linear(in_features=768, out_features=768, bias=True)\n",
                     "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
                     "        (dropout): Dropout(p=0.1, inplace=False)\n",
                     "      )\n",
                     "      (ff): FeedForward(\n",
                     "        (layers): Sequential(\n",
                     "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
                     "          (1): GELU()\n",
                     "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
                     "        )\n",
                     "      )\n",
                     "      (norm1): LayerNorm()\n",
                     "      (norm2): LayerNorm()\n",
                     "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
                     "    )\n",
                     "    (4): TransformerBlock(\n",
                     "      (att): MultiHeadAttention(\n",
                     "        (Q): Linear(in_features=768, out_features=768, bias=True)\n",
                     "        (K): Linear(in_features=768, out_features=768, bias=True)\n",
                     "        (V): Linear(in_features=768, out_features=768, bias=True)\n",
                     "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
                     "        (dropout): Dropout(p=0.1, inplace=False)\n",
                     "      )\n",
                     "      (ff): FeedForward(\n",
                     "        (layers): Sequential(\n",
                     "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
                     "          (1): GELU()\n",
                     "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
                     "        )\n",
                     "      )\n",
                     "      (norm1): LayerNorm()\n",
                     "      (norm2): LayerNorm()\n",
                     "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
                     "    )\n",
                     "    (5): TransformerBlock(\n",
                     "      (att): MultiHeadAttention(\n",
                     "        (Q): Linear(in_features=768, out_features=768, bias=True)\n",
                     "        (K): Linear(in_features=768, out_features=768, bias=True)\n",
                     "        (V): Linear(in_features=768, out_features=768, bias=True)\n",
                     "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
                     "        (dropout): Dropout(p=0.1, inplace=False)\n",
                     "      )\n",
                     "      (ff): FeedForward(\n",
                     "        (layers): Sequential(\n",
                     "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
                     "          (1): GELU()\n",
                     "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
                     "        )\n",
                     "      )\n",
                     "      (norm1): LayerNorm()\n",
                     "      (norm2): LayerNorm()\n",
                     "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
                     "    )\n",
                     "    (6): TransformerBlock(\n",
                     "      (att): MultiHeadAttention(\n",
                     "        (Q): Linear(in_features=768, out_features=768, bias=True)\n",
                     "        (K): Linear(in_features=768, out_features=768, bias=True)\n",
                     "        (V): Linear(in_features=768, out_features=768, bias=True)\n",
                     "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
                     "        (dropout): Dropout(p=0.1, inplace=False)\n",
                     "      )\n",
                     "      (ff): FeedForward(\n",
                     "        (layers): Sequential(\n",
                     "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
                     "          (1): GELU()\n",
                     "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
                     "        )\n",
                     "      )\n",
                     "      (norm1): LayerNorm()\n",
                     "      (norm2): LayerNorm()\n",
                     "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
                     "    )\n",
                     "    (7): TransformerBlock(\n",
                     "      (att): MultiHeadAttention(\n",
                     "        (Q): Linear(in_features=768, out_features=768, bias=True)\n",
                     "        (K): Linear(in_features=768, out_features=768, bias=True)\n",
                     "        (V): Linear(in_features=768, out_features=768, bias=True)\n",
                     "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
                     "        (dropout): Dropout(p=0.1, inplace=False)\n",
                     "      )\n",
                     "      (ff): FeedForward(\n",
                     "        (layers): Sequential(\n",
                     "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
                     "          (1): GELU()\n",
                     "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
                     "        )\n",
                     "      )\n",
                     "      (norm1): LayerNorm()\n",
                     "      (norm2): LayerNorm()\n",
                     "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
                     "    )\n",
                     "    (8): TransformerBlock(\n",
                     "      (att): MultiHeadAttention(\n",
                     "        (Q): Linear(in_features=768, out_features=768, bias=True)\n",
                     "        (K): Linear(in_features=768, out_features=768, bias=True)\n",
                     "        (V): Linear(in_features=768, out_features=768, bias=True)\n",
                     "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
                     "        (dropout): Dropout(p=0.1, inplace=False)\n",
                     "      )\n",
                     "      (ff): FeedForward(\n",
                     "        (layers): Sequential(\n",
                     "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
                     "          (1): GELU()\n",
                     "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
                     "        )\n",
                     "      )\n",
                     "      (norm1): LayerNorm()\n",
                     "      (norm2): LayerNorm()\n",
                     "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
                     "    )\n",
                     "    (9): TransformerBlock(\n",
                     "      (att): MultiHeadAttention(\n",
                     "        (Q): Linear(in_features=768, out_features=768, bias=True)\n",
                     "        (K): Linear(in_features=768, out_features=768, bias=True)\n",
                     "        (V): Linear(in_features=768, out_features=768, bias=True)\n",
                     "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
                     "        (dropout): Dropout(p=0.1, inplace=False)\n",
                     "      )\n",
                     "      (ff): FeedForward(\n",
                     "        (layers): Sequential(\n",
                     "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
                     "          (1): GELU()\n",
                     "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
                     "        )\n",
                     "      )\n",
                     "      (norm1): LayerNorm()\n",
                     "      (norm2): LayerNorm()\n",
                     "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
                     "    )\n",
                     "    (10): TransformerBlock(\n",
                     "      (att): MultiHeadAttention(\n",
                     "        (Q): Linear(in_features=768, out_features=768, bias=True)\n",
                     "        (K): Linear(in_features=768, out_features=768, bias=True)\n",
                     "        (V): Linear(in_features=768, out_features=768, bias=True)\n",
                     "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
                     "        (dropout): Dropout(p=0.1, inplace=False)\n",
                     "      )\n",
                     "      (ff): FeedForward(\n",
                     "        (layers): Sequential(\n",
                     "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
                     "          (1): GELU()\n",
                     "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
                     "        )\n",
                     "      )\n",
                     "      (norm1): LayerNorm()\n",
                     "      (norm2): LayerNorm()\n",
                     "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
                     "    )\n",
                     "    (11): TransformerBlock(\n",
                     "      (att): MultiHeadAttention(\n",
                     "        (Q): Linear(in_features=768, out_features=768, bias=True)\n",
                     "        (K): Linear(in_features=768, out_features=768, bias=True)\n",
                     "        (V): Linear(in_features=768, out_features=768, bias=True)\n",
                     "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
                     "        (dropout): Dropout(p=0.1, inplace=False)\n",
                     "      )\n",
                     "      (ff): FeedForward(\n",
                     "        (layers): Sequential(\n",
                     "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
                     "          (1): GELU()\n",
                     "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
                     "        )\n",
                     "      )\n",
                     "      (norm1): LayerNorm()\n",
                     "      (norm2): LayerNorm()\n",
                     "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
                     "    )\n",
                     "  )\n",
                     "  (final_norm): LayerNorm()\n",
                     "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
                     ")"
                  ]
               },
               "execution_count": 38,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "# Chapter 5\n",
            "# Pretraining\n",
            "\n",
            "GPT_CONFIG_124M = {\n",
            "    \"vocab_size\": 50257,\n",
            "    \"context_length\": 256,\n",
            "    \"emb_dim\": 768,\n",
            "    \"n_heads\": 12,\n",
            "    \"n_layers\": 12,\n",
            "    \"drop_rate\": 0.1,\n",
            "    \"qkv_bias\": False\n",
            "}\n",
            "torch.manual_seed(123)\n",
            "model = GPTModel(GPT_CONFIG_124M)\n",
            "model.eval()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 39,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Output text:\n",
                  " Every effort moves you 107 Holo analystsaiden Behampamr raidinguro覚醒\n"
               ]
            }
         ],
         "source": [
            "def text_to_token_ids(text, tokenizer):\n",
            "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
            "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
            "    #1\n",
            "    return encoded_tensor\n",
            "def token_ids_to_text(token_ids, tokenizer):\n",
            "    flat = token_ids.squeeze(0)\n",
            "    return tokenizer.decode(flat.tolist())\n",
            "\n",
            "start_context = \"Every effort moves you\"\n",
            "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
            "\n",
            "token_ids = generate_text_simple(\n",
            "    model=model,\n",
            "    idx=text_to_token_ids(start_context, tokenizer),\n",
            "    max_new_tokens=10,\n",
            "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
            ")\n",
            "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 40,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Characters: 20479\n",
                  "Tokens: 5145\n"
               ]
            }
         ],
         "source": [
            "file_path = \"the-verdict.txt\"\n",
            "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
            "    text_data = file.read()\n",
            "total_characters = len(text_data)\n",
            "total_tokens = len(tokenizer.encode(text_data))\n",
            "print(\"Characters:\", total_characters)\n",
            "print(\"Tokens:\", total_tokens)\n",
            "\n",
            "train_ratio = 0.90\n",
            "split_idx = int(train_ratio * len(text_data))\n",
            "train_data = text_data[:split_idx]\n",
            "val_data = text_data[split_idx:]"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 41,
         "metadata": {},
         "outputs": [],
         "source": [
            "\n",
            "torch.manual_seed(123)\n",
            "train_loader = create_dataloader_v1(\n",
            "    train_data,\n",
            "    batch_size=2,\n",
            "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
            "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
            "    drop_last=True,\n",
            "    shuffle=True,\n",
            "    num_workers=0\n",
            ")\n",
            "val_loader = create_dataloader_v1(\n",
            "    val_data,\n",
            "    batch_size=2,\n",
            "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
            "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
            "    drop_last=False,\n",
            "    shuffle=False,\n",
            "    num_workers=0\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 42,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Train loader:\n",
                  "torch.Size([2, 256]) torch.Size([2, 256])\n",
                  "torch.Size([2, 256]) torch.Size([2, 256])\n",
                  "torch.Size([2, 256]) torch.Size([2, 256])\n",
                  "torch.Size([2, 256]) torch.Size([2, 256])\n",
                  "torch.Size([2, 256]) torch.Size([2, 256])\n",
                  "torch.Size([2, 256]) torch.Size([2, 256])\n",
                  "torch.Size([2, 256]) torch.Size([2, 256])\n",
                  "torch.Size([2, 256]) torch.Size([2, 256])\n",
                  "torch.Size([2, 256]) torch.Size([2, 256])\n",
                  "\n",
                  "Validation loader:\n",
                  "torch.Size([2, 256]) torch.Size([2, 256])\n"
               ]
            }
         ],
         "source": [
            "print(\"Train loader:\")\n",
            "for x, y in train_loader:\n",
            "    print(x.shape, y.shape)\n",
            "print(\"\\nValidation loader:\")\n",
            "\n",
            "for x, y in val_loader:\n",
            "    print(x.shape, y.shape)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 43,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "train loss 10.99\n",
                  "val loss 10.97\n"
               ]
            }
         ],
         "source": [
            "def calc_loss_batch(train_batch, target_batch, model, device):\n",
            "    train_batch = train_batch.to(device)\n",
            "    target_batch = target_batch.to(device)\n",
            "    preds = model(train_batch)\n",
            "    # flatten converts preds [9, 2, 256] --> [18, 256]; target_batch [9, 2] --> [18]\n",
            "    loss = torch.nn.functional.cross_entropy(preds.flatten(0, 1), target_batch.flatten(0))\n",
            "    return loss\n",
            "\n",
            "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
            "    total_loss = 0.\n",
            "    if len(data_loader) == 0:\n",
            "        return float(\"nan\")\n",
            "    elif num_batches is None:\n",
            "        num_batches = len(data_loader)\n",
            "    else:\n",
            "        num_batches = min(num_batches, len(data_loader)) \n",
            "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
            "        if i < num_batches:\n",
            "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
            "            total_loss += loss.item()\n",
            "        else:\n",
            "            break\n",
            "    return total_loss / num_batches\n",
            "\n",
            "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
            "model.to(device)\n",
            "with torch.no_grad():\n",
            "    train_loss = calc_loss_loader(train_loader, model, device, 2)\n",
            "    val_loss = calc_loss_loader(train_loader, model, device, 2)\n",
            "print(f\"train loss {train_loss:.2f}\")\n",
            "print(f\"val loss {val_loss:.2f}\")\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 44,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Ep 1 (Step 000000): Train loss 9.680, Val loss 10.028\n",
                  "Ep 1 (Step 000005): Train loss 7.938, Val loss 8.265\n",
                  "Every effort moves you, the, the, the of the of the of the of the of the of the of the, of the of the of the of the of the of the of the of the of the of the of the of the of the of the of\n",
                  "Ep 2 (Step 000010): Train loss 6.516, Val loss 7.051\n",
                  "Ep 2 (Step 000015): Train loss 5.785, Val loss 6.538\n",
                  "Every effort moves you. \", and, and, and he was, and, and in the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of\n",
                  "Ep 3 (Step 000020): Train loss 4.923, Val loss 6.539\n",
                  "Ep 3 (Step 000025): Train loss 4.758, Val loss 6.240\n",
                  "Every effort moves you he was, in a was, in a little--I had been. Gisburn's was, in the, I had been to see it was a little of my, as he was a little of the picture--I had been was a\n",
                  "Ep 4 (Step 000030): Train loss 3.695, Val loss 6.204\n",
                  "Ep 4 (Step 000035): Train loss 3.142, Val loss 6.177\n",
                  "Every effort moves you know, I had been the picture, the--and the picture, and in the picture--and a little the fact, and in the. \"I the picture, he had the picture.          \n",
                  "Ep 5 (Step 000040): Train loss 2.439, Val loss 6.181\n",
                  "Every effort moves you know, and my dear, and he was not the fact with the in the and it was no--that was the women had been his pictures--his--as he had been his pictures--the quality of the my dear, and he was his\n",
                  "Ep 6 (Step 000045): Train loss 1.799, Val loss 6.198\n",
                  "Ep 6 (Step 000050): Train loss 1.472, Val loss 6.260\n",
                  "Every effort moves you in the, my-rooms, as his pictures--and by the in a and it was his eyes and in theed it. Gisburn. Stroud.  \"I didn't know of Jack's \"strong he had been his\n",
                  "Ep 7 (Step 000055): Train loss 1.080, Val loss 6.335\n",
                  "Ep 7 (Step 000060): Train loss 0.828, Val loss 6.329\n",
                  "Every effort moves you in the inevitable garlanded to have been--I told Mrs.                                    \n",
                  "Ep 8 (Step 000065): Train loss 0.435, Val loss 6.432\n",
                  "Ep 8 (Step 000070): Train loss 0.272, Val loss 6.449\n",
                  "Every effort moves you in the inevitable garlanded to have been the the fact with the background of the house.\"  \" went on groping and muddling; and he _not_ interesting--his own of the donkey, and were amusing himself by holding\n",
                  "Ep 9 (Step 000075): Train loss 0.158, Val loss 6.485\n",
                  "Ep 9 (Step 000080): Train loss 0.094, Val loss 6.538\n",
                  "Every effort moves you know,\" was not that my hostess was \"interesting\": on the Sevres and silver was no great surprise to me to hear that, in the moment--as Jack himself, as once one had been hard in a fashionable painter--that I had\n",
                  "Ep 10 (Step 000085): Train loss 0.064, Val loss 6.556\n",
                  "Every effort moves you know,\" was not that my hostess was \"interesting\": on that point I could have disarming, and went on groping and muddling; then I looked at the donkey again. \"I must add--because he didn't want\n"
               ]
            }
         ],
         "source": [
            "\n",
            "\n",
            "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
            "    model.eval() \n",
            "    with torch.no_grad():\n",
            "        train_loss = calc_loss_loader(\n",
            "            train_loader, model, device, num_batches=eval_iter\n",
            "        )\n",
            "        val_loss = calc_loss_loader(\n",
            "            val_loader, model, device, num_batches=eval_iter\n",
            "        )\n",
            "    model.train()\n",
            "    return train_loss, val_loss\n",
            "\n",
            "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
            "    model.eval()\n",
            "    context_size = model.pos_emb.weight.shape[0]\n",
            "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
            "    with torch.no_grad():\n",
            "        token_ids = generate_text_simple(\n",
            "            model=model, idx=encoded,\n",
            "            max_new_tokens=50, context_size=context_size\n",
            "        )\n",
            "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
            "    print(decoded_text.replace(\"\\n\", \" \"))\n",
            "    model.train()\n",
            "\n",
            "\n",
            "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,eval_freq, eval_iter, start_context, tokenizer):\n",
            "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
            "    tokens_seen, global_step = 0, -1\n",
            "    for epoch in range(num_epochs):\n",
            "        model.train()\n",
            "        for input_batch, target_batch in train_loader:\n",
            "            optimizer.zero_grad()\n",
            "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
            "            loss.backward()\n",
            "            optimizer.step()\n",
            "            tokens_seen += input_batch.numel()\n",
            "            global_step += 1\n",
            "            if global_step % eval_freq == 0:\n",
            "                train_loss, val_loss = evaluate_model(model, train_loader, val_loader, device, eval_iter)\n",
            "                train_losses.append(train_loss)\n",
            "                val_losses.append(val_loss)\n",
            "                track_tokens_seen.append(tokens_seen)\n",
            "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
            "                        f\"Train loss {train_loss:.3f}, \"\n",
            "                        f\"Val loss {val_loss:.3f}\"\n",
            "                )\n",
            "        generate_and_print_sample(model, tokenizer, device, start_context)\n",
            "    return train_losses, val_losses, track_tokens_seen\n",
            "\n",
            "torch.manual_seed(123)\n",
            "model = GPTModel(GPT_CONFIG_124M)\n",
            "model.to(device)\n",
            "optimizer = torch.optim.AdamW(\n",
            "    model.parameters(),\n",
            "    lr=0.0004, weight_decay=0.1\n",
            ")\n",
            "num_epochs = 10\n",
            "train_losses, val_losses, tokens_seen = train_model_simple(\n",
            "    model, train_loader, val_loader, optimizer, device,\n",
            "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
            "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
            ")\n",
            "\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 46,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "image/png": "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",
                  "text/plain": [
                     "<Figure size 500x300 with 2 Axes>"
                  ]
               },
               "metadata": {},
               "output_type": "display_data"
            }
         ],
         "source": [
            "import matplotlib.pyplot as plt\n",
            "from matplotlib.ticker import MaxNLocator\n",
            "\n",
            "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
            "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
            "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
            "    ax1.plot(\n",
            "        epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\"\n",
            "    )\n",
            "    ax1.set_xlabel(\"Epochs\")\n",
            "    ax1.set_ylabel(\"Loss\")\n",
            "    ax1.legend(loc=\"upper right\")\n",
            "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
            "    ax2 = ax1.twiny()\n",
            "    #1\n",
            "    ax2.plot(tokens_seen, train_losses, alpha=0)\n",
            "    ax2.set_xlabel(\"Tokens seen\")\n",
            "    fig.tight_layout()\n",
            "    plt.show()\n",
            "\n",
            "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
            "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 48,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "'every effort moves you know,\" was not that my hostess was \"interesting\": on that point I could have disarming, and went on grop'"
                  ]
               },
               "execution_count": 48,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "model.to(\"cpu\")\n",
            "model.eval()\n",
            "\n",
            "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
            "token_ids = generate_text_simple(model, text_to_token_ids(\"every effort moves you\", tokenizer), max_new_tokens=25, context_size=GPT_CONFIG_124M['context_length'])\n",
            "text = token_ids_to_text(token_ids, tokenizer)\n",
            "text\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 65,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "forward\n",
                  "forward\n",
                  "73 x closer\n",
                  "0 x every\n",
                  "0 x effort\n",
                  "582 x forward\n",
                  "2 x inches\n",
                  "0 x moves\n",
                  "0 x pizza\n",
                  "343 x toward\n"
               ]
            }
         ],
         "source": [
            "# text generation methods to introduce variation in the outpt\n",
            "vocab = {\n",
            "\"closer\": 0,\n",
            "\"every\": 1,\n",
            "\"effort\": 2,\n",
            "\"forward\": 3,\n",
            "\"inches\": 4,\n",
            "\"moves\": 5,\n",
            "\"pizza\": 6,\n",
            "\"toward\": 7,\n",
            "\"you\": 8,\n",
            "}\n",
            "inverse_vocab = {v: k for k, v in vocab.items()}\n",
            "next_token_logits = torch.tensor(\n",
            "    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
            ")\n",
            "probas = torch.softmax(next_token_logits, dim=0)\n",
            "next_token_id = torch.argmax(probas).item()\n",
            "print(inverse_vocab[next_token_id])\n",
            "\n",
            "# probabilistic sampling process using multinomial function\n",
            "# multinomial function samples from the probas probability distribution num_samples times\n",
            "# multinomial = probs distribution for discrete events, where each event takes on a fixed set of discrete possible values \n",
            "# in this case, possible outcomes = vocab \n",
            "torch.manual_seed(123)\n",
            "next_token_id = torch.multinomial(probas, num_samples=1).item()\n",
            "print(inverse_vocab[next_token_id])\n",
            "\n",
            "def print_sampled_tokens(probas):\n",
            "    torch.manual_seed(123)\n",
            "    sample = [torch.multinomial(probas, num_samples=1).item() for i in range(1_000)]\n",
            "    sampled_ids = torch.bincount(torch.tensor(sample))\n",
            "    for i, freq in enumerate(sampled_ids):\n",
            "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
            "\n",
            "print_sampled_tokens(probas)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 68,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "image/png": "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",
                  "text/plain": [
                     "<Figure size 500x300 with 1 Axes>"
                  ]
               },
               "metadata": {},
               "output_type": "display_data"
            }
         ],
         "source": [
            "# temperature scaling = scale the softmax to control the width of the distribution\n",
            "# dividing the logits with the temp values > 1, result in a flatter more spreadout distribution\n",
            "# temp values < 1 result in a peakier distribution \n",
            "def softmax_with_temperature(logits, temperature):\n",
            "    scaled_logits = logits / temperature\n",
            "    return torch.softmax(scaled_logits, dim=0)\n",
            "\n",
            "temperatures = [1, 0.1, 5]\n",
            "scaled_probas = [softmax_with_temperature(next_token_logits, T) for T in temperatures]\n",
            "x = torch.arange(len(vocab))\n",
            "bar_width = 0.15\n",
            "fig, ax = plt.subplots(figsize=(5, 3))\n",
            "for i, T in enumerate(temperatures):\n",
            "    rects = ax.bar(x + i * bar_width, scaled_probas[i],\n",
            "    bar_width, label=f'Temperature = {T}')\n",
            "    ax.set_ylabel('Probability')\n",
            "    ax.set_xticks(x)\n",
            "    ax.set_xticklabels(vocab.keys(), rotation=90)\n",
            "ax.legend()\n",
            "plt.tight_layout()\n",
            "plt.show()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 70,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "temp :1\n",
                  "73 x closer\n",
                  "0 x every\n",
                  "0 x effort\n",
                  "582 x forward\n",
                  "2 x inches\n",
                  "0 x moves\n",
                  "0 x pizza\n",
                  "343 x toward\n",
                  "temp :0.1\n",
                  "0 x closer\n",
                  "0 x every\n",
                  "0 x effort\n",
                  "985 x forward\n",
                  "0 x inches\n",
                  "0 x moves\n",
                  "0 x pizza\n",
                  "15 x toward\n",
                  "temp :5\n",
                  "165 x closer\n",
                  "75 x every\n",
                  "42 x effort\n",
                  "239 x forward\n",
                  "71 x inches\n",
                  "46 x moves\n",
                  "32 x pizza\n",
                  "227 x toward\n",
                  "103 x you\n",
                  "temp :1\n",
                  "temp :0.1\n",
                  "temp :5\n",
                  "tensor(0.0430)\n"
               ]
            }
         ],
         "source": [
            "# exercise 5.1 \n",
            "#Use the print_sampled_tokens function to print the sampling\n",
            "#frequencies of the softmax probabilities scaled with the\n",
            "# temperatures shown in figure 5.14. How often is the word\n",
            "# pizza sampled in each case? Can you think of a faster and\n",
            "# more accurate way to determine how often the word pizza\n",
            "# is sampled?\n",
            "\n",
            "for T in temperatures:\n",
            "    print(f\"temp :{T}\")\n",
            "    scaled_probs = softmax_with_temperature(next_token_logits, T)\n",
            "    print_sampled_tokens(scaled_probs)\n",
            "\n",
            "# more efficient way\n",
            "for T in temperatures:\n",
            "    print(f\"temp :{T}\")\n",
            "    scaled_probs = softmax_with_temperature(next_token_logits, T)\n",
            "    sample = torch.multinomial(scaled_probs, num_samples=1000, replacement=True)#.item()\n",
            "\n",
            "# pizza_sample = torch.sum(sample[])\n",
            "pizza_sample = sum([x.item() for x in sample if x == 6])\n",
            "pizza_sample\n",
            "\n",
            "temp_scaled_probs = [softmax_with_temperature(next_token_logits, T) for T in temperatures]\n",
            "pizza_sample_temp_5 = temp_scaled_probs[2][6]\n",
            "print(pizza_sample_temp_5)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 74,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Top logits: tensor([6.7500, 6.2800, 4.5100])\n",
                  "Top positions: tensor([3, 7, 0])\n",
                  "tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])\n",
                  "tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])\n"
               ]
            }
         ],
         "source": [
            "# topk sampling\n",
            "# selects the top k scores from the logits and masks out the remaining possible token scores by replacing them with -inf \n",
            "# when applying softmax, those scores receive 0 probability and only the top k sampled words are considered for the output/generated token \n",
            "top_k = 3\n",
            "top_logits, top_pos = torch.topk(next_token_logits, top_k)\n",
            "print(\"Top logits:\", top_logits)\n",
            "print(\"Top positions:\", top_pos)\n",
            "\n",
            "# derive new logits using where condition, by masking all values less than the n=k probability (last element) with -inf\n",
            "new_logits = torch.where(\n",
            "    condition=next_token_logits < top_logits[-1],\n",
            "    input=torch.tensor(float('-inf')),\n",
            "    other=next_token_logits\n",
            ")\n",
            "\n",
            "print(new_logits)\n",
            "# get the probabilitis for the top k logits\n",
            "topk_probas = torch.softmax(new_logits, dim=0)\n",
            "print(topk_probas)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Output text:\n",
                  " Every effort moves you an object Gisburn rather a little to face watching me across the been\n"
               ]
            }
         ],
         "source": [
            "# combine topk sampling with mulitnomial sampling and temperature scaling to generate text\n",
            "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
            "    for _ in range(max_new_tokens):\n",
            "        idx_cond = idx[:, -context_size:]\n",
            "        with torch.no_grad():\n",
            "            logits = model(idx_cond)\n",
            "        logits = logits[:, -1, :]\n",
            "        if top_k is not None:\n",
            "            top_logits, _ = torch.topk(logits, top_k)\n",
            "        min_val = top_logits[:, -1]\n",
            "        logits = torch.where(logits < min_val, torch.tensor(float('-inf')).to(logits.device),logits)\n",
            "        if temperature > 0.0:\n",
            "            logits = logits / temperature\n",
            "            probs = torch.softmax(logits, dim=-1)\n",
            "            idx_next = torch.multinomial(probs, num_samples=1)\n",
            "        else:\n",
            "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
            "        if idx_next == eos_id:\n",
            "            break\n",
            "        idx = torch.cat((idx, idx_next), dim=1)\n",
            "    return idx\n",
            "\n",
            "\n",
            "torch.manual_seed(123)\n",
            "token_ids = generate(model=model,\n",
            "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
            "    max_new_tokens=15,\n",
            "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
            "    top_k=25,\n",
            "    temperature=1.4\n",
            ")\n",
            "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n",
            "\n",
            "\n",
            "\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "temp 1, topk 3, text Every effort moves you just began to face was! She's a with a deprecating in\n",
                  "temp 1, topk 10, text Every effort moves you?\"\n",
                  "\" on painting people, as my own sex for my such's\n",
                  "temp 1, topk 15, text Every effort moves you say to the house.\"ed to see the the't dab to me in\n",
                  "temp 0.1, topk 3, text Every effort moves you't _was Gisburn, as the deep years by little the background\n",
                  "temp 0.1, topk 10, text Every effort moves you been his own sittersforming, as it, he turned Mrs. St\n",
                  "temp 0.1, topk 15, text Every effort moves you half me into; his glory, he Gisburn--chairs forwardanim\n",
                  "temp 5, topk 3, text Every effort moves you in her inevitable the do the picture by the endinteresting\": on the point\n",
                  "temp 5, topk 10, text Every effort moves you thought Jack's go a little felt able to face the fact with equanim\n",
                  "temp 5, topk 15, text Every effort moves you know without such, at the picture for me into his painting. He says\n"
               ]
            }
         ],
         "source": [
            "\n",
            "# exercise 5.2\n",
            "# different topk and temp settings - when is that desired?\n",
            "\n",
            "temps = [1, 0.1, 5]\n",
            "topk = [3, 10, 15]\n",
            "settings_text = []\n",
            "for t in temps:\n",
            "    for k in topk:\n",
            "        settings_text.append({\"temp\": t, \"topk\": k, \"text\": []})\n",
            "# settings_text = [{\"temp\": t, \"topk\": k, \"text\": []} for t, k in zip(temps, topk)]\n",
            "\n",
            "for i, set_ in enumerate(settings_text):\n",
            "    token_ids = generate(model=model,\n",
            "        idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
            "        max_new_tokens=15,\n",
            "        context_size=GPT_CONFIG_124M[\"context_length\"],\n",
            "        top_k=25,\n",
            "        temperature=1.4\n",
            "    )\n",
            "    # print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n",
            "    gen_text = token_ids_to_text(token_ids, tokenizer)\n",
            "    settings_text[i]['text'] = gen_text\n",
            "\n",
            "for set_ in settings_text:\n",
            "    print(f\"temp {set_['temp']}, topk {set_['topk']}, text {set_['text']}\")\n",
            "\n",
            "# EXERCISE 5.3\n",
            "# What are the different combinations of settings for the\n",
            "# generate function to force deterministic behavior, that is,\n",
            "# disabling the random sampling such that it always\n",
            "# produces the same outputs similar to the generate_simple\n",
            "# function?\n",
            "# temp = 1, topk = 1\n"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "stanford",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.10.16"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
